{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4654ffb-a288-430f-b105-519fb0aa5de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import freetype\n",
    "import uharfbuzz as hb\n",
    "import cv2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3f35562-c5af-4905-b777-a00938d9deb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 13141222 training samples\n"
     ]
    }
   ],
   "source": [
    "# Load the NepaliText dataset\n",
    "dataset = load_dataset(\"Sakonii/nepalitext-language-model-dataset\")\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "print(f\"Loaded dataset with {len(train_texts)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "915610d9-89f8-4cab-ab4c-25aa7d3b062b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Nepali words from dataset...\n",
      "  Processed 10000 texts, found 49361 unique words\n",
      "  Processed 20000 texts, found 77417 unique words\n",
      "  Processed 30000 texts, found 100764 unique words\n",
      "  Processed 40000 texts, found 119936 unique words\n",
      "  Processed 50000 texts, found 137609 unique words\n",
      "  Processed 60000 texts, found 153427 unique words\n",
      "  Processed 70000 texts, found 167843 unique words\n",
      "  Processed 80000 texts, found 182163 unique words\n",
      "  Processed 90000 texts, found 195560 unique words\n",
      "  Processed 100000 texts, found 208380 unique words\n",
      "  Processed 110000 texts, found 220954 unique words\n",
      "  Processed 120000 texts, found 232162 unique words\n",
      "  Processed 130000 texts, found 243317 unique words\n",
      "  Processed 140000 texts, found 254115 unique words\n",
      "  Processed 150000 texts, found 264583 unique words\n",
      "  Processed 160000 texts, found 274815 unique words\n",
      "  Processed 170000 texts, found 285196 unique words\n",
      "  Processed 180000 texts, found 294661 unique words\n",
      "  Processed 190000 texts, found 305185 unique words\n",
      "  Processed 200000 texts, found 315117 unique words\n",
      "  Processed 210000 texts, found 324407 unique words\n",
      "  Processed 220000 texts, found 334305 unique words\n",
      "  Processed 230000 texts, found 342750 unique words\n",
      "  Processed 240000 texts, found 351895 unique words\n",
      "  Processed 250000 texts, found 360378 unique words\n",
      "  Processed 260000 texts, found 369115 unique words\n",
      "  Processed 270000 texts, found 376562 unique words\n",
      "  Processed 280000 texts, found 384812 unique words\n",
      "  Processed 290000 texts, found 392269 unique words\n",
      "  Processed 300000 texts, found 400027 unique words\n",
      "  Processed 310000 texts, found 407750 unique words\n",
      "  Processed 320000 texts, found 414849 unique words\n",
      "  Processed 330000 texts, found 422726 unique words\n",
      "  Processed 340000 texts, found 429970 unique words\n",
      "  Processed 350000 texts, found 437296 unique words\n",
      "  Processed 360000 texts, found 444784 unique words\n",
      "  Processed 370000 texts, found 451940 unique words\n",
      "  Processed 380000 texts, found 459094 unique words\n",
      "  Processed 390000 texts, found 466158 unique words\n",
      "  Processed 400000 texts, found 473037 unique words\n",
      "  Processed 410000 texts, found 479976 unique words\n",
      "  Processed 420000 texts, found 487022 unique words\n",
      "  Processed 430000 texts, found 493573 unique words\n",
      "  Processed 440000 texts, found 499924 unique words\n",
      "  Processed 450000 texts, found 506628 unique words\n",
      "  Processed 460000 texts, found 513059 unique words\n",
      "  Processed 470000 texts, found 519843 unique words\n",
      "  Processed 480000 texts, found 525985 unique words\n",
      "  Processed 490000 texts, found 531929 unique words\n",
      "  Processed 500000 texts, found 538250 unique words\n",
      "  Processed 510000 texts, found 544850 unique words\n",
      "  Processed 520000 texts, found 551323 unique words\n",
      "  Processed 530000 texts, found 557436 unique words\n",
      "  Processed 540000 texts, found 563269 unique words\n",
      "  Processed 550000 texts, found 568926 unique words\n",
      "  Processed 560000 texts, found 575316 unique words\n",
      "  Processed 570000 texts, found 581483 unique words\n",
      "  Processed 580000 texts, found 587334 unique words\n",
      "  Processed 590000 texts, found 593564 unique words\n",
      "  Processed 600000 texts, found 599570 unique words\n",
      "  Processed 610000 texts, found 605393 unique words\n",
      "  Processed 620000 texts, found 611548 unique words\n",
      "  Processed 630000 texts, found 617981 unique words\n",
      "  Processed 640000 texts, found 623429 unique words\n",
      "  Processed 650000 texts, found 628426 unique words\n",
      "  Processed 660000 texts, found 634189 unique words\n",
      "  Processed 670000 texts, found 639912 unique words\n",
      "  Processed 680000 texts, found 645167 unique words\n",
      "  Processed 690000 texts, found 650406 unique words\n",
      "  Processed 700000 texts, found 655981 unique words\n",
      "  Processed 710000 texts, found 660817 unique words\n",
      "  Processed 720000 texts, found 666461 unique words\n",
      "  Processed 730000 texts, found 671316 unique words\n",
      "  Processed 740000 texts, found 676845 unique words\n",
      "  Processed 750000 texts, found 682486 unique words\n",
      "  Processed 760000 texts, found 688019 unique words\n",
      "  Processed 770000 texts, found 692843 unique words\n",
      "  Processed 780000 texts, found 698188 unique words\n",
      "  Processed 790000 texts, found 703927 unique words\n",
      "  Processed 800000 texts, found 708964 unique words\n",
      "  Processed 810000 texts, found 713774 unique words\n",
      "  Processed 820000 texts, found 718921 unique words\n",
      "  Processed 830000 texts, found 723733 unique words\n",
      "  Processed 840000 texts, found 728917 unique words\n",
      "  Processed 850000 texts, found 733459 unique words\n",
      "  Processed 860000 texts, found 738594 unique words\n",
      "  Processed 870000 texts, found 743367 unique words\n",
      "  Processed 880000 texts, found 748149 unique words\n",
      "  Processed 890000 texts, found 752750 unique words\n",
      "  Processed 900000 texts, found 757083 unique words\n",
      "  Processed 910000 texts, found 761619 unique words\n",
      "  Processed 920000 texts, found 766351 unique words\n",
      "  Processed 930000 texts, found 770859 unique words\n",
      "  Processed 940000 texts, found 775271 unique words\n",
      "  Processed 950000 texts, found 779682 unique words\n",
      "  Processed 960000 texts, found 784543 unique words\n",
      "  Processed 970000 texts, found 789059 unique words\n",
      "  Processed 980000 texts, found 793922 unique words\n",
      "  Processed 990000 texts, found 798469 unique words\n",
      "  Processed 1000000 texts, found 802655 unique words\n",
      "  Processed 1010000 texts, found 807021 unique words\n",
      "  Processed 1020000 texts, found 811156 unique words\n",
      "  Processed 1030000 texts, found 815673 unique words\n",
      "  Processed 1040000 texts, found 820004 unique words\n",
      "  Processed 1050000 texts, found 824754 unique words\n",
      "  Processed 1060000 texts, found 828747 unique words\n",
      "  Processed 1070000 texts, found 833261 unique words\n",
      "  Processed 1080000 texts, found 837512 unique words\n",
      "  Processed 1090000 texts, found 841467 unique words\n",
      "  Processed 1100000 texts, found 845511 unique words\n",
      "  Processed 1110000 texts, found 849414 unique words\n",
      "  Processed 1120000 texts, found 853601 unique words\n",
      "  Processed 1130000 texts, found 857905 unique words\n",
      "  Processed 1140000 texts, found 861977 unique words\n",
      "  Processed 1150000 texts, found 865861 unique words\n",
      "  Processed 1160000 texts, found 869685 unique words\n",
      "  Processed 1170000 texts, found 873394 unique words\n",
      "  Processed 1180000 texts, found 877331 unique words\n",
      "  Processed 1190000 texts, found 881381 unique words\n",
      "  Processed 1200000 texts, found 885165 unique words\n",
      "  Processed 1210000 texts, found 890515 unique words\n",
      "  Processed 1220000 texts, found 895230 unique words\n",
      "  Processed 1230000 texts, found 899065 unique words\n",
      "  Processed 1240000 texts, found 903314 unique words\n",
      "  Processed 1250000 texts, found 907084 unique words\n",
      "  Processed 1260000 texts, found 910630 unique words\n",
      "  Processed 1270000 texts, found 914593 unique words\n",
      "  Processed 1280000 texts, found 918976 unique words\n",
      "  Processed 1290000 texts, found 922884 unique words\n",
      "  Processed 1300000 texts, found 926962 unique words\n",
      "  Processed 1310000 texts, found 930785 unique words\n",
      "  Processed 1320000 texts, found 934318 unique words\n",
      "  Processed 1330000 texts, found 938130 unique words\n",
      "  Processed 1340000 texts, found 942246 unique words\n",
      "  Processed 1350000 texts, found 945828 unique words\n",
      "  Processed 1360000 texts, found 949487 unique words\n",
      "  Processed 1370000 texts, found 953221 unique words\n",
      "  Processed 1380000 texts, found 956786 unique words\n",
      "  Processed 1390000 texts, found 960490 unique words\n",
      "  Processed 1400000 texts, found 964103 unique words\n",
      "  Processed 1410000 texts, found 967336 unique words\n",
      "  Processed 1420000 texts, found 971000 unique words\n",
      "  Processed 1430000 texts, found 975264 unique words\n",
      "  Processed 1440000 texts, found 978867 unique words\n",
      "  Processed 1450000 texts, found 982620 unique words\n",
      "  Processed 1460000 texts, found 986799 unique words\n",
      "  Processed 1470000 texts, found 991136 unique words\n",
      "  Processed 1480000 texts, found 994960 unique words\n",
      "  Processed 1490000 texts, found 998696 unique words\n",
      "  Processed 1500000 texts, found 1002078 unique words\n",
      "  Processed 1510000 texts, found 1006187 unique words\n",
      "  Processed 1520000 texts, found 1010106 unique words\n",
      "  Processed 1530000 texts, found 1013661 unique words\n",
      "  Processed 1540000 texts, found 1017397 unique words\n",
      "  Processed 1550000 texts, found 1020869 unique words\n",
      "  Processed 1560000 texts, found 1024671 unique words\n",
      "  Processed 1570000 texts, found 1028119 unique words\n",
      "  Processed 1580000 texts, found 1031216 unique words\n",
      "  Processed 1590000 texts, found 1034355 unique words\n",
      "  Processed 1600000 texts, found 1037809 unique words\n",
      "  Processed 1610000 texts, found 1041240 unique words\n",
      "  Processed 1620000 texts, found 1044465 unique words\n",
      "  Processed 1630000 texts, found 1047775 unique words\n",
      "  Processed 1640000 texts, found 1051131 unique words\n",
      "  Processed 1650000 texts, found 1054341 unique words\n",
      "  Processed 1660000 texts, found 1057619 unique words\n",
      "  Processed 1670000 texts, found 1061094 unique words\n",
      "  Processed 1680000 texts, found 1064556 unique words\n",
      "  Processed 1690000 texts, found 1068110 unique words\n",
      "  Processed 1700000 texts, found 1071475 unique words\n",
      "  Processed 1710000 texts, found 1074757 unique words\n",
      "  Processed 1720000 texts, found 1078343 unique words\n",
      "  Processed 1730000 texts, found 1081826 unique words\n",
      "  Processed 1740000 texts, found 1085011 unique words\n",
      "  Processed 1750000 texts, found 1087988 unique words\n",
      "  Processed 1760000 texts, found 1091210 unique words\n",
      "  Processed 1770000 texts, found 1094555 unique words\n",
      "  Processed 1780000 texts, found 1098137 unique words\n",
      "  Processed 1790000 texts, found 1101578 unique words\n",
      "  Processed 1800000 texts, found 1105075 unique words\n",
      "  Processed 1810000 texts, found 1108223 unique words\n",
      "  Processed 1820000 texts, found 1111353 unique words\n",
      "  Processed 1830000 texts, found 1114879 unique words\n",
      "  Processed 1840000 texts, found 1118088 unique words\n",
      "  Processed 1850000 texts, found 1121397 unique words\n",
      "  Processed 1860000 texts, found 1125032 unique words\n",
      "  Processed 1870000 texts, found 1127912 unique words\n",
      "  Processed 1880000 texts, found 1131363 unique words\n",
      "  Processed 1890000 texts, found 1134798 unique words\n",
      "  Processed 1900000 texts, found 1137868 unique words\n",
      "  Processed 1910000 texts, found 1140696 unique words\n",
      "  Processed 1920000 texts, found 1143988 unique words\n",
      "  Processed 1930000 texts, found 1147361 unique words\n",
      "  Processed 1940000 texts, found 1150288 unique words\n",
      "  Processed 1950000 texts, found 1153606 unique words\n",
      "  Processed 1960000 texts, found 1156530 unique words\n",
      "  Processed 1970000 texts, found 1159431 unique words\n",
      "  Processed 1980000 texts, found 1162423 unique words\n",
      "  Processed 1990000 texts, found 1165358 unique words\n",
      "  Processed 2000000 texts, found 1168498 unique words\n",
      "  Processed 2010000 texts, found 1171405 unique words\n",
      "  Processed 2020000 texts, found 1175071 unique words\n",
      "  Processed 2030000 texts, found 1177746 unique words\n",
      "  Processed 2040000 texts, found 1180871 unique words\n",
      "  Processed 2050000 texts, found 1183597 unique words\n",
      "  Processed 2060000 texts, found 1186517 unique words\n",
      "  Processed 2070000 texts, found 1190127 unique words\n",
      "  Processed 2080000 texts, found 1193170 unique words\n",
      "  Processed 2090000 texts, found 1196348 unique words\n",
      "  Processed 2100000 texts, found 1199626 unique words\n",
      "  Processed 2110000 texts, found 1202826 unique words\n",
      "  Processed 2120000 texts, found 1205956 unique words\n",
      "  Processed 2130000 texts, found 1209142 unique words\n",
      "  Processed 2140000 texts, found 1212090 unique words\n",
      "  Processed 2150000 texts, found 1215300 unique words\n",
      "  Processed 2160000 texts, found 1218475 unique words\n",
      "  Processed 2170000 texts, found 1221338 unique words\n",
      "  Processed 2180000 texts, found 1223991 unique words\n",
      "  Processed 2190000 texts, found 1226865 unique words\n",
      "  Processed 2200000 texts, found 1229755 unique words\n",
      "  Processed 2210000 texts, found 1232870 unique words\n",
      "  Processed 2220000 texts, found 1235561 unique words\n",
      "  Processed 2230000 texts, found 1239000 unique words\n",
      "  Processed 2240000 texts, found 1241607 unique words\n",
      "  Processed 2250000 texts, found 1244245 unique words\n",
      "  Processed 2260000 texts, found 1246748 unique words\n",
      "  Processed 2270000 texts, found 1249561 unique words\n",
      "  Processed 2280000 texts, found 1252326 unique words\n",
      "  Processed 2290000 texts, found 1254831 unique words\n",
      "  Processed 2300000 texts, found 1258122 unique words\n",
      "  Processed 2310000 texts, found 1260825 unique words\n",
      "  Processed 2320000 texts, found 1263636 unique words\n",
      "  Processed 2330000 texts, found 1266226 unique words\n",
      "  Processed 2340000 texts, found 1269007 unique words\n",
      "  Processed 2350000 texts, found 1271891 unique words\n",
      "  Processed 2360000 texts, found 1274802 unique words\n",
      "  Processed 2370000 texts, found 1277555 unique words\n",
      "  Processed 2380000 texts, found 1280496 unique words\n",
      "  Processed 2390000 texts, found 1282925 unique words\n",
      "  Processed 2400000 texts, found 1285951 unique words\n",
      "  Processed 2410000 texts, found 1288665 unique words\n",
      "  Processed 2420000 texts, found 1291706 unique words\n",
      "  Processed 2430000 texts, found 1294078 unique words\n",
      "  Processed 2440000 texts, found 1296677 unique words\n",
      "  Processed 2450000 texts, found 1299276 unique words\n",
      "  Processed 2460000 texts, found 1302129 unique words\n",
      "  Processed 2470000 texts, found 1305314 unique words\n",
      "  Processed 2480000 texts, found 1307863 unique words\n",
      "  Processed 2490000 texts, found 1310330 unique words\n",
      "  Processed 2500000 texts, found 1312889 unique words\n",
      "  Processed 2510000 texts, found 1315339 unique words\n",
      "  Processed 2520000 texts, found 1318670 unique words\n",
      "  Processed 2530000 texts, found 1321163 unique words\n",
      "  Processed 2540000 texts, found 1324041 unique words\n",
      "  Processed 2550000 texts, found 1326706 unique words\n",
      "  Processed 2560000 texts, found 1329229 unique words\n",
      "  Processed 2570000 texts, found 1331508 unique words\n",
      "  Processed 2580000 texts, found 1334124 unique words\n",
      "  Processed 2590000 texts, found 1336708 unique words\n",
      "  Processed 2600000 texts, found 1339308 unique words\n",
      "  Processed 2610000 texts, found 1341876 unique words\n",
      "  Processed 2620000 texts, found 1344355 unique words\n",
      "  Processed 2630000 texts, found 1346973 unique words\n",
      "  Processed 2640000 texts, found 1349687 unique words\n",
      "  Processed 2650000 texts, found 1352054 unique words\n",
      "  Processed 2660000 texts, found 1354613 unique words\n",
      "  Processed 2670000 texts, found 1357199 unique words\n",
      "  Processed 2680000 texts, found 1359679 unique words\n",
      "  Processed 2690000 texts, found 1362125 unique words\n",
      "  Processed 2700000 texts, found 1364723 unique words\n",
      "  Processed 2710000 texts, found 1367178 unique words\n",
      "  Processed 2720000 texts, found 1369619 unique words\n",
      "  Processed 2730000 texts, found 1372265 unique words\n",
      "  Processed 2740000 texts, found 1374711 unique words\n",
      "  Processed 2750000 texts, found 1377072 unique words\n",
      "  Processed 2760000 texts, found 1379377 unique words\n",
      "  Processed 2770000 texts, found 1381729 unique words\n",
      "  Processed 2780000 texts, found 1384370 unique words\n",
      "  Processed 2790000 texts, found 1386819 unique words\n",
      "  Processed 2800000 texts, found 1389281 unique words\n",
      "  Processed 2810000 texts, found 1391607 unique words\n",
      "  Processed 2820000 texts, found 1394314 unique words\n",
      "  Processed 2830000 texts, found 1397447 unique words\n",
      "  Processed 2840000 texts, found 1400027 unique words\n",
      "  Processed 2850000 texts, found 1402488 unique words\n",
      "  Processed 2860000 texts, found 1404759 unique words\n",
      "  Processed 2870000 texts, found 1407400 unique words\n",
      "  Processed 2880000 texts, found 1409757 unique words\n",
      "  Processed 2890000 texts, found 1412060 unique words\n",
      "  Processed 2900000 texts, found 1414192 unique words\n",
      "  Processed 2910000 texts, found 1416761 unique words\n",
      "  Processed 2920000 texts, found 1418933 unique words\n",
      "  Processed 2930000 texts, found 1421479 unique words\n",
      "  Processed 2940000 texts, found 1424071 unique words\n",
      "  Processed 2950000 texts, found 1426266 unique words\n",
      "  Processed 2960000 texts, found 1428553 unique words\n",
      "  Processed 2970000 texts, found 1430890 unique words\n",
      "  Processed 2980000 texts, found 1433577 unique words\n",
      "  Processed 2990000 texts, found 1436089 unique words\n",
      "  Processed 3000000 texts, found 1439823 unique words\n",
      "  Processed 3010000 texts, found 1442235 unique words\n",
      "  Processed 3020000 texts, found 1444500 unique words\n",
      "  Processed 3030000 texts, found 1446412 unique words\n",
      "  Processed 3040000 texts, found 1448602 unique words\n",
      "  Processed 3050000 texts, found 1450776 unique words\n",
      "  Processed 3060000 texts, found 1452861 unique words\n",
      "  Processed 3070000 texts, found 1455152 unique words\n",
      "  Processed 3080000 texts, found 1457244 unique words\n",
      "  Processed 3090000 texts, found 1459559 unique words\n",
      "  Processed 3100000 texts, found 1462543 unique words\n",
      "  Processed 3110000 texts, found 1464795 unique words\n",
      "  Processed 3120000 texts, found 1467341 unique words\n",
      "  Processed 3130000 texts, found 1469804 unique words\n",
      "  Processed 3140000 texts, found 1472384 unique words\n",
      "  Processed 3150000 texts, found 1474376 unique words\n",
      "  Processed 3160000 texts, found 1476648 unique words\n",
      "  Processed 3170000 texts, found 1479108 unique words\n",
      "  Processed 3180000 texts, found 1481280 unique words\n",
      "  Processed 3190000 texts, found 1483568 unique words\n",
      "  Processed 3200000 texts, found 1486161 unique words\n",
      "  Processed 3210000 texts, found 1488345 unique words\n",
      "  Processed 3220000 texts, found 1490477 unique words\n",
      "  Processed 3230000 texts, found 1492837 unique words\n",
      "  Processed 3240000 texts, found 1495155 unique words\n",
      "  Processed 3250000 texts, found 1497297 unique words\n",
      "  Processed 3260000 texts, found 1499569 unique words\n",
      "  Processed 3270000 texts, found 1501767 unique words\n",
      "  Processed 3280000 texts, found 1504155 unique words\n",
      "  Processed 3290000 texts, found 1506428 unique words\n",
      "  Processed 3300000 texts, found 1508591 unique words\n",
      "  Processed 3310000 texts, found 1510986 unique words\n",
      "  Processed 3320000 texts, found 1513519 unique words\n",
      "  Processed 3330000 texts, found 1515892 unique words\n",
      "  Processed 3340000 texts, found 1517964 unique words\n",
      "  Processed 3350000 texts, found 1519929 unique words\n",
      "  Processed 3360000 texts, found 1521888 unique words\n",
      "  Processed 3370000 texts, found 1523885 unique words\n",
      "  Processed 3380000 texts, found 1526151 unique words\n",
      "  Processed 3390000 texts, found 1528515 unique words\n",
      "  Processed 3400000 texts, found 1530869 unique words\n",
      "  Processed 3410000 texts, found 1533236 unique words\n",
      "  Processed 3420000 texts, found 1535295 unique words\n",
      "  Processed 3430000 texts, found 1537442 unique words\n",
      "  Processed 3440000 texts, found 1539579 unique words\n",
      "  Processed 3450000 texts, found 1541578 unique words\n",
      "  Processed 3460000 texts, found 1543564 unique words\n",
      "  Processed 3470000 texts, found 1545654 unique words\n",
      "  Processed 3480000 texts, found 1547824 unique words\n",
      "  Processed 3490000 texts, found 1549992 unique words\n",
      "  Processed 3500000 texts, found 1551971 unique words\n",
      "  Processed 3510000 texts, found 1554120 unique words\n",
      "  Processed 3520000 texts, found 1556209 unique words\n",
      "  Processed 3530000 texts, found 1558262 unique words\n",
      "  Processed 3540000 texts, found 1560206 unique words\n",
      "  Processed 3550000 texts, found 1562388 unique words\n",
      "  Processed 3560000 texts, found 1564492 unique words\n",
      "  Processed 3570000 texts, found 1566668 unique words\n",
      "  Processed 3580000 texts, found 1568819 unique words\n",
      "  Processed 3590000 texts, found 1571048 unique words\n",
      "  Processed 3600000 texts, found 1573279 unique words\n",
      "  Processed 3610000 texts, found 1575332 unique words\n",
      "  Processed 3620000 texts, found 1577347 unique words\n",
      "  Processed 3630000 texts, found 1579508 unique words\n",
      "  Processed 3640000 texts, found 1581849 unique words\n",
      "  Processed 3650000 texts, found 1583998 unique words\n",
      "  Processed 3660000 texts, found 1585967 unique words\n",
      "  Processed 3670000 texts, found 1587957 unique words\n",
      "  Processed 3680000 texts, found 1590040 unique words\n",
      "  Processed 3690000 texts, found 1592120 unique words\n",
      "  Processed 3700000 texts, found 1594218 unique words\n",
      "  Processed 3710000 texts, found 1596179 unique words\n",
      "  Processed 3720000 texts, found 1598341 unique words\n",
      "  Processed 3730000 texts, found 1600580 unique words\n",
      "  Processed 3740000 texts, found 1602675 unique words\n",
      "  Processed 3750000 texts, found 1604603 unique words\n",
      "  Processed 3760000 texts, found 1606835 unique words\n",
      "  Processed 3770000 texts, found 1609117 unique words\n",
      "  Processed 3780000 texts, found 1610910 unique words\n",
      "  Processed 3790000 texts, found 1612903 unique words\n",
      "  Processed 3800000 texts, found 1614724 unique words\n",
      "  Processed 3810000 texts, found 1616810 unique words\n",
      "  Processed 3820000 texts, found 1618761 unique words\n",
      "  Processed 3830000 texts, found 1620983 unique words\n",
      "  Processed 3840000 texts, found 1622973 unique words\n",
      "  Processed 3850000 texts, found 1624973 unique words\n",
      "  Processed 3860000 texts, found 1626835 unique words\n",
      "  Processed 3870000 texts, found 1628661 unique words\n",
      "  Processed 3880000 texts, found 1630710 unique words\n",
      "  Processed 3890000 texts, found 1632773 unique words\n",
      "  Processed 3900000 texts, found 1635162 unique words\n",
      "  Processed 3910000 texts, found 1637122 unique words\n",
      "  Processed 3920000 texts, found 1639427 unique words\n",
      "  Processed 3930000 texts, found 1641427 unique words\n",
      "  Processed 3940000 texts, found 1643647 unique words\n",
      "  Processed 3950000 texts, found 1645521 unique words\n",
      "  Processed 3960000 texts, found 1647176 unique words\n",
      "  Processed 3970000 texts, found 1649133 unique words\n",
      "  Processed 3980000 texts, found 1651191 unique words\n",
      "  Processed 3990000 texts, found 1653049 unique words\n",
      "  Processed 4000000 texts, found 1654798 unique words\n",
      "  Processed 4010000 texts, found 1656607 unique words\n",
      "  Processed 4020000 texts, found 1658792 unique words\n",
      "  Processed 4030000 texts, found 1660545 unique words\n",
      "  Processed 4040000 texts, found 1662287 unique words\n",
      "  Processed 4050000 texts, found 1664251 unique words\n",
      "  Processed 4060000 texts, found 1666112 unique words\n",
      "  Processed 4070000 texts, found 1667942 unique words\n",
      "  Processed 4080000 texts, found 1669830 unique words\n",
      "  Processed 4090000 texts, found 1671967 unique words\n",
      "  Processed 4100000 texts, found 1673695 unique words\n",
      "  Processed 4110000 texts, found 1675376 unique words\n",
      "  Processed 4120000 texts, found 1677303 unique words\n",
      "  Processed 4130000 texts, found 1679313 unique words\n",
      "  Processed 4140000 texts, found 1681340 unique words\n",
      "  Processed 4150000 texts, found 1683201 unique words\n",
      "  Processed 4160000 texts, found 1684897 unique words\n",
      "  Processed 4170000 texts, found 1686713 unique words\n",
      "  Processed 4180000 texts, found 1688599 unique words\n",
      "  Processed 4190000 texts, found 1690622 unique words\n",
      "  Processed 4200000 texts, found 1692546 unique words\n",
      "  Processed 4210000 texts, found 1694473 unique words\n",
      "  Processed 4220000 texts, found 1696235 unique words\n",
      "  Processed 4230000 texts, found 1698285 unique words\n",
      "  Processed 4240000 texts, found 1700022 unique words\n",
      "  Processed 4250000 texts, found 1702063 unique words\n",
      "  Processed 4260000 texts, found 1703906 unique words\n",
      "  Processed 4270000 texts, found 1706124 unique words\n",
      "  Processed 4280000 texts, found 1707951 unique words\n",
      "  Processed 4290000 texts, found 1709766 unique words\n",
      "  Processed 4300000 texts, found 1711644 unique words\n",
      "  Processed 4310000 texts, found 1713556 unique words\n",
      "  Processed 4320000 texts, found 1715416 unique words\n",
      "  Processed 4330000 texts, found 1717380 unique words\n",
      "  Processed 4340000 texts, found 1719118 unique words\n",
      "  Processed 4350000 texts, found 1720856 unique words\n",
      "  Processed 4360000 texts, found 1722868 unique words\n",
      "  Processed 4370000 texts, found 1724594 unique words\n",
      "  Processed 4380000 texts, found 1726555 unique words\n",
      "  Processed 4390000 texts, found 1728299 unique words\n",
      "  Processed 4400000 texts, found 1729902 unique words\n",
      "  Processed 4410000 texts, found 1731578 unique words\n",
      "  Processed 4420000 texts, found 1733258 unique words\n",
      "  Processed 4430000 texts, found 1734985 unique words\n",
      "  Processed 4440000 texts, found 1737001 unique words\n",
      "  Processed 4450000 texts, found 1738933 unique words\n",
      "  Processed 4460000 texts, found 1740470 unique words\n",
      "  Processed 4470000 texts, found 1742442 unique words\n",
      "  Processed 4480000 texts, found 1743878 unique words\n",
      "  Processed 4490000 texts, found 1745656 unique words\n",
      "  Processed 4500000 texts, found 1747191 unique words\n",
      "  Processed 4510000 texts, found 1749029 unique words\n",
      "  Processed 4520000 texts, found 1750556 unique words\n",
      "  Processed 4530000 texts, found 1752416 unique words\n",
      "  Processed 4540000 texts, found 1754188 unique words\n",
      "  Processed 4550000 texts, found 1756230 unique words\n",
      "  Processed 4560000 texts, found 1758006 unique words\n",
      "  Processed 4570000 texts, found 1760216 unique words\n",
      "  Processed 4580000 texts, found 1762072 unique words\n",
      "  Processed 4590000 texts, found 1763722 unique words\n",
      "  Processed 4600000 texts, found 1765466 unique words\n",
      "  Processed 4610000 texts, found 1767169 unique words\n",
      "  Processed 4620000 texts, found 1768928 unique words\n",
      "  Processed 4630000 texts, found 1770690 unique words\n",
      "  Processed 4640000 texts, found 1772434 unique words\n",
      "  Processed 4650000 texts, found 1774195 unique words\n",
      "  Processed 4660000 texts, found 1775779 unique words\n",
      "  Processed 4670000 texts, found 1777444 unique words\n",
      "  Processed 4680000 texts, found 1779757 unique words\n",
      "  Processed 4690000 texts, found 1781410 unique words\n",
      "  Processed 4700000 texts, found 1782959 unique words\n",
      "  Processed 4710000 texts, found 1784687 unique words\n",
      "  Processed 4720000 texts, found 1786540 unique words\n",
      "  Processed 4730000 texts, found 1788130 unique words\n",
      "  Processed 4740000 texts, found 1790153 unique words\n",
      "  Processed 4750000 texts, found 1792005 unique words\n",
      "  Processed 4760000 texts, found 1793706 unique words\n",
      "  Processed 4770000 texts, found 1795583 unique words\n",
      "  Processed 4780000 texts, found 1797174 unique words\n",
      "  Processed 4790000 texts, found 1798686 unique words\n",
      "  Processed 4800000 texts, found 1800272 unique words\n",
      "  Processed 4810000 texts, found 1801816 unique words\n",
      "  Processed 4820000 texts, found 1803582 unique words\n",
      "  Processed 4830000 texts, found 1805002 unique words\n",
      "  Processed 4840000 texts, found 1806599 unique words\n",
      "  Processed 4850000 texts, found 1808217 unique words\n",
      "  Processed 4860000 texts, found 1809655 unique words\n",
      "  Processed 4870000 texts, found 1811096 unique words\n",
      "  Processed 4880000 texts, found 1812869 unique words\n",
      "  Processed 4890000 texts, found 1814669 unique words\n",
      "  Processed 4900000 texts, found 1816345 unique words\n",
      "  Processed 4910000 texts, found 1818008 unique words\n",
      "  Processed 4920000 texts, found 1819584 unique words\n",
      "  Processed 4930000 texts, found 1821083 unique words\n",
      "  Processed 4940000 texts, found 1822753 unique words\n",
      "  Processed 4950000 texts, found 1824359 unique words\n",
      "  Processed 4960000 texts, found 1825983 unique words\n",
      "  Processed 4970000 texts, found 1827719 unique words\n",
      "  Processed 4980000 texts, found 1829303 unique words\n",
      "  Processed 4990000 texts, found 1831000 unique words\n",
      "  Processed 5000000 texts, found 1832445 unique words\n",
      "  Processed 5010000 texts, found 1833974 unique words\n",
      "  Processed 5020000 texts, found 1835588 unique words\n",
      "  Processed 5030000 texts, found 1837223 unique words\n",
      "  Processed 5040000 texts, found 1838759 unique words\n",
      "  Processed 5050000 texts, found 1840454 unique words\n",
      "  Processed 5060000 texts, found 1842044 unique words\n",
      "  Processed 5070000 texts, found 1843714 unique words\n",
      "  Processed 5080000 texts, found 1845512 unique words\n",
      "  Processed 5090000 texts, found 1847853 unique words\n",
      "  Processed 5100000 texts, found 1849347 unique words\n",
      "  Processed 5110000 texts, found 1851023 unique words\n",
      "  Processed 5120000 texts, found 1852556 unique words\n",
      "  Processed 5130000 texts, found 1854035 unique words\n",
      "  Processed 5140000 texts, found 1855649 unique words\n",
      "  Processed 5150000 texts, found 1857305 unique words\n",
      "  Processed 5160000 texts, found 1858959 unique words\n",
      "  Processed 5170000 texts, found 1860332 unique words\n",
      "  Processed 5180000 texts, found 1861913 unique words\n",
      "  Processed 5190000 texts, found 1863349 unique words\n",
      "  Processed 5200000 texts, found 1865136 unique words\n",
      "  Processed 5210000 texts, found 1866791 unique words\n",
      "  Processed 5220000 texts, found 1868230 unique words\n",
      "  Processed 5230000 texts, found 1869802 unique words\n",
      "  Processed 5240000 texts, found 1871352 unique words\n",
      "  Processed 5250000 texts, found 1872887 unique words\n",
      "  Processed 5260000 texts, found 1874449 unique words\n",
      "  Processed 5270000 texts, found 1876054 unique words\n",
      "  Processed 5280000 texts, found 1877562 unique words\n",
      "  Processed 5290000 texts, found 1879263 unique words\n",
      "  Processed 5300000 texts, found 1880640 unique words\n",
      "  Processed 5310000 texts, found 1882236 unique words\n",
      "  Processed 5320000 texts, found 1883902 unique words\n",
      "  Processed 5330000 texts, found 1885665 unique words\n",
      "  Processed 5340000 texts, found 1887062 unique words\n",
      "  Processed 5350000 texts, found 1888584 unique words\n",
      "  Processed 5360000 texts, found 1889997 unique words\n",
      "  Processed 5370000 texts, found 1891403 unique words\n",
      "  Processed 5380000 texts, found 1892820 unique words\n",
      "  Processed 5390000 texts, found 1894472 unique words\n",
      "  Processed 5400000 texts, found 1895953 unique words\n",
      "  Processed 5410000 texts, found 1897332 unique words\n",
      "  Processed 5420000 texts, found 1899223 unique words\n",
      "  Processed 5430000 texts, found 1900795 unique words\n",
      "  Processed 5440000 texts, found 1902233 unique words\n",
      "  Processed 5450000 texts, found 1903633 unique words\n",
      "  Processed 5460000 texts, found 1905088 unique words\n",
      "  Processed 5470000 texts, found 1906612 unique words\n",
      "  Processed 5480000 texts, found 1908002 unique words\n",
      "  Processed 5490000 texts, found 1909252 unique words\n",
      "  Processed 5500000 texts, found 1910779 unique words\n",
      "  Processed 5510000 texts, found 1912496 unique words\n",
      "  Processed 5520000 texts, found 1913993 unique words\n",
      "  Processed 5530000 texts, found 1915409 unique words\n",
      "  Processed 5540000 texts, found 1917237 unique words\n",
      "  Processed 5550000 texts, found 1918765 unique words\n",
      "  Processed 5560000 texts, found 1920333 unique words\n",
      "  Processed 5570000 texts, found 1921849 unique words\n",
      "  Processed 5580000 texts, found 1923381 unique words\n",
      "  Processed 5590000 texts, found 1925024 unique words\n",
      "  Processed 5600000 texts, found 1926465 unique words\n",
      "  Processed 5610000 texts, found 1928025 unique words\n",
      "  Processed 5620000 texts, found 1929427 unique words\n",
      "  Processed 5630000 texts, found 1930942 unique words\n",
      "  Processed 5640000 texts, found 1932610 unique words\n",
      "  Processed 5650000 texts, found 1933933 unique words\n",
      "  Processed 5660000 texts, found 1935360 unique words\n",
      "  Processed 5670000 texts, found 1936928 unique words\n",
      "  Processed 5680000 texts, found 1938505 unique words\n",
      "  Processed 5690000 texts, found 1940107 unique words\n",
      "  Processed 5700000 texts, found 1941481 unique words\n",
      "  Processed 5710000 texts, found 1942836 unique words\n",
      "  Processed 5720000 texts, found 1944214 unique words\n",
      "  Processed 5730000 texts, found 1945594 unique words\n",
      "  Processed 5740000 texts, found 1947094 unique words\n",
      "  Processed 5750000 texts, found 1948422 unique words\n",
      "  Processed 5760000 texts, found 1949784 unique words\n",
      "  Processed 5770000 texts, found 1951180 unique words\n",
      "  Processed 5780000 texts, found 1952658 unique words\n",
      "  Processed 5790000 texts, found 1954178 unique words\n",
      "  Processed 5800000 texts, found 1955439 unique words\n",
      "  Processed 5810000 texts, found 1956797 unique words\n",
      "  Processed 5820000 texts, found 1958136 unique words\n",
      "  Processed 5830000 texts, found 1959502 unique words\n",
      "  Processed 5840000 texts, found 1960920 unique words\n",
      "  Processed 5850000 texts, found 1962379 unique words\n",
      "  Processed 5860000 texts, found 1963951 unique words\n",
      "  Processed 5870000 texts, found 1965263 unique words\n",
      "  Processed 5880000 texts, found 1966678 unique words\n",
      "  Processed 5890000 texts, found 1967913 unique words\n",
      "  Processed 5900000 texts, found 1969114 unique words\n",
      "  Processed 5910000 texts, found 1970574 unique words\n",
      "  Processed 5920000 texts, found 1971984 unique words\n",
      "  Processed 5930000 texts, found 1973816 unique words\n",
      "  Processed 5940000 texts, found 1975376 unique words\n",
      "  Processed 5950000 texts, found 1976849 unique words\n",
      "  Processed 5960000 texts, found 1978198 unique words\n",
      "  Processed 5970000 texts, found 1979520 unique words\n",
      "  Processed 5980000 texts, found 1980795 unique words\n",
      "  Processed 5990000 texts, found 1982288 unique words\n",
      "  Processed 6000000 texts, found 1983699 unique words\n",
      "  Processed 6010000 texts, found 1984976 unique words\n",
      "  Processed 6020000 texts, found 1986319 unique words\n",
      "  Processed 6030000 texts, found 1987694 unique words\n",
      "  Processed 6040000 texts, found 1989142 unique words\n",
      "  Processed 6050000 texts, found 1990598 unique words\n",
      "  Processed 6060000 texts, found 1991951 unique words\n",
      "  Processed 6070000 texts, found 1993222 unique words\n",
      "  Processed 6080000 texts, found 1994650 unique words\n",
      "  Processed 6090000 texts, found 1996057 unique words\n",
      "  Processed 6100000 texts, found 1997351 unique words\n",
      "  Processed 6110000 texts, found 1998624 unique words\n",
      "  Processed 6120000 texts, found 1999887 unique words\n",
      "  Processed 6130000 texts, found 2001186 unique words\n",
      "  Processed 6140000 texts, found 2002543 unique words\n",
      "  Processed 6150000 texts, found 2003822 unique words\n",
      "  Processed 6160000 texts, found 2005050 unique words\n",
      "  Processed 6170000 texts, found 2006357 unique words\n",
      "  Processed 6180000 texts, found 2007641 unique words\n",
      "  Processed 6190000 texts, found 2008971 unique words\n",
      "  Processed 6200000 texts, found 2010331 unique words\n",
      "  Processed 6210000 texts, found 2011556 unique words\n",
      "  Processed 6220000 texts, found 2012936 unique words\n",
      "  Processed 6230000 texts, found 2014339 unique words\n",
      "  Processed 6240000 texts, found 2015941 unique words\n",
      "  Processed 6250000 texts, found 2017418 unique words\n",
      "  Processed 6260000 texts, found 2018690 unique words\n",
      "  Processed 6270000 texts, found 2019880 unique words\n",
      "  Processed 6280000 texts, found 2021245 unique words\n",
      "  Processed 6290000 texts, found 2022586 unique words\n",
      "  Processed 6300000 texts, found 2023879 unique words\n",
      "  Processed 6310000 texts, found 2025232 unique words\n",
      "  Processed 6320000 texts, found 2026506 unique words\n",
      "  Processed 6330000 texts, found 2027895 unique words\n",
      "  Processed 6340000 texts, found 2029119 unique words\n",
      "  Processed 6350000 texts, found 2030471 unique words\n",
      "  Processed 6360000 texts, found 2031940 unique words\n",
      "  Processed 6370000 texts, found 2033248 unique words\n",
      "  Processed 6380000 texts, found 2034545 unique words\n",
      "  Processed 6390000 texts, found 2035822 unique words\n",
      "  Processed 6400000 texts, found 2037138 unique words\n",
      "  Processed 6410000 texts, found 2038619 unique words\n",
      "  Processed 6420000 texts, found 2039927 unique words\n",
      "  Processed 6430000 texts, found 2041257 unique words\n",
      "  Processed 6440000 texts, found 2042556 unique words\n",
      "  Processed 6450000 texts, found 2044027 unique words\n",
      "  Processed 6460000 texts, found 2045256 unique words\n",
      "  Processed 6470000 texts, found 2046557 unique words\n",
      "  Processed 6480000 texts, found 2047970 unique words\n",
      "  Processed 6490000 texts, found 2049143 unique words\n",
      "  Processed 6500000 texts, found 2050537 unique words\n",
      "  Processed 6510000 texts, found 2051816 unique words\n",
      "  Processed 6520000 texts, found 2053122 unique words\n",
      "  Processed 6530000 texts, found 2054527 unique words\n",
      "  Processed 6540000 texts, found 2055739 unique words\n",
      "  Processed 6550000 texts, found 2056942 unique words\n",
      "  Processed 6560000 texts, found 2058180 unique words\n",
      "  Processed 6570000 texts, found 2059559 unique words\n",
      "  Processed 6580000 texts, found 2060808 unique words\n",
      "  Processed 6590000 texts, found 2062243 unique words\n",
      "  Processed 6600000 texts, found 2063603 unique words\n",
      "  Processed 6610000 texts, found 2064816 unique words\n",
      "  Processed 6620000 texts, found 2066066 unique words\n",
      "  Processed 6630000 texts, found 2067168 unique words\n",
      "  Processed 6640000 texts, found 2068652 unique words\n",
      "  Processed 6650000 texts, found 2069727 unique words\n",
      "  Processed 6660000 texts, found 2070866 unique words\n",
      "  Processed 6670000 texts, found 2072078 unique words\n",
      "  Processed 6680000 texts, found 2073750 unique words\n",
      "  Processed 6690000 texts, found 2075175 unique words\n",
      "  Processed 6700000 texts, found 2076319 unique words\n",
      "  Processed 6710000 texts, found 2077653 unique words\n",
      "  Processed 6720000 texts, found 2078948 unique words\n",
      "  Processed 6730000 texts, found 2080139 unique words\n",
      "  Processed 6740000 texts, found 2081370 unique words\n",
      "  Processed 6750000 texts, found 2082429 unique words\n",
      "  Processed 6760000 texts, found 2083672 unique words\n",
      "  Processed 6770000 texts, found 2084983 unique words\n",
      "  Processed 6780000 texts, found 2086232 unique words\n",
      "  Processed 6790000 texts, found 2087768 unique words\n",
      "  Processed 6800000 texts, found 2089059 unique words\n",
      "  Processed 6810000 texts, found 2090230 unique words\n",
      "  Processed 6820000 texts, found 2091356 unique words\n",
      "  Processed 6830000 texts, found 2092602 unique words\n",
      "  Processed 6840000 texts, found 2093860 unique words\n",
      "  Processed 6850000 texts, found 2096246 unique words\n",
      "  Processed 6860000 texts, found 2097490 unique words\n",
      "  Processed 6870000 texts, found 2098614 unique words\n",
      "  Processed 6880000 texts, found 2099894 unique words\n",
      "  Processed 6890000 texts, found 2101370 unique words\n",
      "  Processed 6900000 texts, found 2102701 unique words\n",
      "  Processed 6910000 texts, found 2103872 unique words\n",
      "  Processed 6920000 texts, found 2104953 unique words\n",
      "  Processed 6930000 texts, found 2106083 unique words\n",
      "  Processed 6940000 texts, found 2107448 unique words\n",
      "  Processed 6950000 texts, found 2108770 unique words\n",
      "  Processed 6960000 texts, found 2109979 unique words\n",
      "  Processed 6970000 texts, found 2111311 unique words\n",
      "  Processed 6980000 texts, found 2112667 unique words\n",
      "  Processed 6990000 texts, found 2113982 unique words\n",
      "  Processed 7000000 texts, found 2115265 unique words\n",
      "  Processed 7010000 texts, found 2116421 unique words\n",
      "  Processed 7020000 texts, found 2117584 unique words\n",
      "  Processed 7030000 texts, found 2118843 unique words\n",
      "  Processed 7040000 texts, found 2119944 unique words\n",
      "  Processed 7050000 texts, found 2121230 unique words\n",
      "  Processed 7060000 texts, found 2122592 unique words\n",
      "  Processed 7070000 texts, found 2123929 unique words\n",
      "  Processed 7080000 texts, found 2125083 unique words\n",
      "  Processed 7090000 texts, found 2126224 unique words\n",
      "  Processed 7100000 texts, found 2127403 unique words\n",
      "  Processed 7110000 texts, found 2128557 unique words\n",
      "  Processed 7120000 texts, found 2129678 unique words\n",
      "  Processed 7130000 texts, found 2131015 unique words\n",
      "  Processed 7140000 texts, found 2132345 unique words\n",
      "  Processed 7150000 texts, found 2133483 unique words\n",
      "  Processed 7160000 texts, found 2134732 unique words\n",
      "  Processed 7170000 texts, found 2135943 unique words\n",
      "  Processed 7180000 texts, found 2137023 unique words\n",
      "  Processed 7190000 texts, found 2138118 unique words\n",
      "  Processed 7200000 texts, found 2139372 unique words\n",
      "  Processed 7210000 texts, found 2140534 unique words\n",
      "  Processed 7220000 texts, found 2141768 unique words\n",
      "  Processed 7230000 texts, found 2143218 unique words\n",
      "  Processed 7240000 texts, found 2144267 unique words\n",
      "  Processed 7250000 texts, found 2145359 unique words\n",
      "  Processed 7260000 texts, found 2146419 unique words\n",
      "  Processed 7270000 texts, found 2147560 unique words\n",
      "  Processed 7280000 texts, found 2148735 unique words\n",
      "  Processed 7290000 texts, found 2149890 unique words\n",
      "  Processed 7300000 texts, found 2151006 unique words\n",
      "  Processed 7310000 texts, found 2152137 unique words\n",
      "  Processed 7320000 texts, found 2153426 unique words\n",
      "  Processed 7330000 texts, found 2154592 unique words\n",
      "  Processed 7340000 texts, found 2155772 unique words\n",
      "  Processed 7350000 texts, found 2156848 unique words\n",
      "  Processed 7360000 texts, found 2157939 unique words\n",
      "  Processed 7370000 texts, found 2159099 unique words\n",
      "  Processed 7380000 texts, found 2160285 unique words\n",
      "  Processed 7390000 texts, found 2161422 unique words\n",
      "  Processed 7400000 texts, found 2162400 unique words\n",
      "  Processed 7410000 texts, found 2163688 unique words\n",
      "  Processed 7420000 texts, found 2164763 unique words\n",
      "  Processed 7430000 texts, found 2166175 unique words\n",
      "  Processed 7440000 texts, found 2167427 unique words\n",
      "  Processed 7450000 texts, found 2168580 unique words\n",
      "  Processed 7460000 texts, found 2169635 unique words\n",
      "  Processed 7470000 texts, found 2170646 unique words\n",
      "  Processed 7480000 texts, found 2171807 unique words\n",
      "  Processed 7490000 texts, found 2172981 unique words\n",
      "  Processed 7500000 texts, found 2174247 unique words\n",
      "  Processed 7510000 texts, found 2175365 unique words\n",
      "  Processed 7520000 texts, found 2176647 unique words\n",
      "  Processed 7530000 texts, found 2177836 unique words\n",
      "  Processed 7540000 texts, found 2178994 unique words\n",
      "  Processed 7550000 texts, found 2180020 unique words\n",
      "  Processed 7560000 texts, found 2181109 unique words\n",
      "  Processed 7570000 texts, found 2182179 unique words\n",
      "  Processed 7580000 texts, found 2183398 unique words\n",
      "  Processed 7590000 texts, found 2184541 unique words\n",
      "  Processed 7600000 texts, found 2185730 unique words\n",
      "  Processed 7610000 texts, found 2186912 unique words\n",
      "  Processed 7620000 texts, found 2187882 unique words\n",
      "  Processed 7630000 texts, found 2188896 unique words\n",
      "  Processed 7640000 texts, found 2189827 unique words\n",
      "  Processed 7650000 texts, found 2191011 unique words\n",
      "  Processed 7660000 texts, found 2192056 unique words\n",
      "  Processed 7670000 texts, found 2193221 unique words\n",
      "  Processed 7680000 texts, found 2194417 unique words\n",
      "  Processed 7690000 texts, found 2195696 unique words\n",
      "  Processed 7700000 texts, found 2196845 unique words\n",
      "  Processed 7710000 texts, found 2197834 unique words\n",
      "  Processed 7720000 texts, found 2199006 unique words\n",
      "  Processed 7730000 texts, found 2200237 unique words\n",
      "  Processed 7740000 texts, found 2201324 unique words\n",
      "  Processed 7750000 texts, found 2202363 unique words\n",
      "  Processed 7760000 texts, found 2203416 unique words\n",
      "  Processed 7770000 texts, found 2204532 unique words\n",
      "  Processed 7780000 texts, found 2205690 unique words\n",
      "  Processed 7790000 texts, found 2206791 unique words\n",
      "  Processed 7800000 texts, found 2207848 unique words\n",
      "  Processed 7810000 texts, found 2208831 unique words\n",
      "  Processed 7820000 texts, found 2210123 unique words\n",
      "  Processed 7830000 texts, found 2211467 unique words\n",
      "  Processed 7840000 texts, found 2212529 unique words\n",
      "  Processed 7850000 texts, found 2213459 unique words\n",
      "  Processed 7860000 texts, found 2214528 unique words\n",
      "  Processed 7870000 texts, found 2215448 unique words\n",
      "  Processed 7880000 texts, found 2216523 unique words\n",
      "  Processed 7890000 texts, found 2217639 unique words\n",
      "  Processed 7900000 texts, found 2218786 unique words\n",
      "  Processed 7910000 texts, found 2219790 unique words\n",
      "  Processed 7920000 texts, found 2220750 unique words\n",
      "  Processed 7930000 texts, found 2221934 unique words\n",
      "  Processed 7940000 texts, found 2222991 unique words\n",
      "  Processed 7950000 texts, found 2223906 unique words\n",
      "  Processed 7960000 texts, found 2225000 unique words\n",
      "  Processed 7970000 texts, found 2226176 unique words\n",
      "  Processed 7980000 texts, found 2227162 unique words\n",
      "  Processed 7990000 texts, found 2228181 unique words\n",
      "  Processed 8000000 texts, found 2229351 unique words\n",
      "  Processed 8010000 texts, found 2230505 unique words\n",
      "  Processed 8020000 texts, found 2231535 unique words\n",
      "  Processed 8030000 texts, found 2232465 unique words\n",
      "  Processed 8040000 texts, found 2233526 unique words\n",
      "  Processed 8050000 texts, found 2234579 unique words\n",
      "  Processed 8060000 texts, found 2235650 unique words\n",
      "  Processed 8070000 texts, found 2236673 unique words\n",
      "  Processed 8080000 texts, found 2237655 unique words\n",
      "  Processed 8090000 texts, found 2238682 unique words\n",
      "  Processed 8100000 texts, found 2239693 unique words\n",
      "  Processed 8110000 texts, found 2240783 unique words\n",
      "  Processed 8120000 texts, found 2242187 unique words\n",
      "  Processed 8130000 texts, found 2243924 unique words\n",
      "  Processed 8140000 texts, found 2245114 unique words\n",
      "  Processed 8150000 texts, found 2246007 unique words\n",
      "  Processed 8160000 texts, found 2246925 unique words\n",
      "  Processed 8170000 texts, found 2247963 unique words\n",
      "  Processed 8180000 texts, found 2249097 unique words\n",
      "  Processed 8190000 texts, found 2250195 unique words\n",
      "  Processed 8200000 texts, found 2251138 unique words\n",
      "  Processed 8210000 texts, found 2252177 unique words\n",
      "  Processed 8220000 texts, found 2253176 unique words\n",
      "  Processed 8230000 texts, found 2254210 unique words\n",
      "  Processed 8240000 texts, found 2255277 unique words\n",
      "  Processed 8250000 texts, found 2256280 unique words\n",
      "  Processed 8260000 texts, found 2257285 unique words\n",
      "  Processed 8270000 texts, found 2258346 unique words\n",
      "  Processed 8280000 texts, found 2259401 unique words\n",
      "  Processed 8290000 texts, found 2260480 unique words\n",
      "  Processed 8300000 texts, found 2261530 unique words\n",
      "  Processed 8310000 texts, found 2262402 unique words\n",
      "  Processed 8320000 texts, found 2263408 unique words\n",
      "  Processed 8330000 texts, found 2264433 unique words\n",
      "  Processed 8340000 texts, found 2265419 unique words\n",
      "  Processed 8350000 texts, found 2266448 unique words\n",
      "  Processed 8360000 texts, found 2267399 unique words\n",
      "  Processed 8370000 texts, found 2268494 unique words\n",
      "  Processed 8380000 texts, found 2269540 unique words\n",
      "  Processed 8390000 texts, found 2270477 unique words\n",
      "  Processed 8400000 texts, found 2271401 unique words\n",
      "  Processed 8410000 texts, found 2272508 unique words\n",
      "  Processed 8420000 texts, found 2273482 unique words\n",
      "  Processed 8430000 texts, found 2274441 unique words\n",
      "  Processed 8440000 texts, found 2275324 unique words\n",
      "  Processed 8450000 texts, found 2276260 unique words\n",
      "  Processed 8460000 texts, found 2277329 unique words\n",
      "  Processed 8470000 texts, found 2278337 unique words\n",
      "  Processed 8480000 texts, found 2279238 unique words\n",
      "  Processed 8490000 texts, found 2280178 unique words\n",
      "  Processed 8500000 texts, found 2281070 unique words\n",
      "  Processed 8510000 texts, found 2282188 unique words\n",
      "  Processed 8520000 texts, found 2283173 unique words\n",
      "  Processed 8530000 texts, found 2284246 unique words\n",
      "  Processed 8540000 texts, found 2285311 unique words\n",
      "  Processed 8550000 texts, found 2286341 unique words\n",
      "  Processed 8560000 texts, found 2287385 unique words\n",
      "  Processed 8570000 texts, found 2288457 unique words\n",
      "  Processed 8580000 texts, found 2289535 unique words\n",
      "  Processed 8590000 texts, found 2290604 unique words\n",
      "  Processed 8600000 texts, found 2291584 unique words\n",
      "  Processed 8610000 texts, found 2292536 unique words\n",
      "  Processed 8620000 texts, found 2293416 unique words\n",
      "  Processed 8630000 texts, found 2294307 unique words\n",
      "  Processed 8640000 texts, found 2295287 unique words\n",
      "  Processed 8650000 texts, found 2296439 unique words\n",
      "  Processed 8660000 texts, found 2297419 unique words\n",
      "  Processed 8670000 texts, found 2298425 unique words\n",
      "  Processed 8680000 texts, found 2299557 unique words\n",
      "  Processed 8690000 texts, found 2300753 unique words\n",
      "  Processed 8700000 texts, found 2301761 unique words\n",
      "  Processed 8710000 texts, found 2302732 unique words\n",
      "  Processed 8720000 texts, found 2303564 unique words\n",
      "  Processed 8730000 texts, found 2304708 unique words\n",
      "  Processed 8740000 texts, found 2305555 unique words\n",
      "  Processed 8750000 texts, found 2306598 unique words\n",
      "  Processed 8760000 texts, found 2307551 unique words\n",
      "  Processed 8770000 texts, found 2308428 unique words\n",
      "  Processed 8780000 texts, found 2309331 unique words\n",
      "  Processed 8790000 texts, found 2310278 unique words\n",
      "  Processed 8800000 texts, found 2311164 unique words\n",
      "  Processed 8810000 texts, found 2312075 unique words\n",
      "  Processed 8820000 texts, found 2312959 unique words\n",
      "  Processed 8830000 texts, found 2313783 unique words\n",
      "  Processed 8840000 texts, found 2314800 unique words\n",
      "  Processed 8850000 texts, found 2315633 unique words\n",
      "  Processed 8860000 texts, found 2316507 unique words\n",
      "  Processed 8870000 texts, found 2317336 unique words\n",
      "  Processed 8880000 texts, found 2318191 unique words\n",
      "  Processed 8890000 texts, found 2319194 unique words\n",
      "  Processed 8900000 texts, found 2320159 unique words\n",
      "  Processed 8910000 texts, found 2321160 unique words\n",
      "  Processed 8920000 texts, found 2321977 unique words\n",
      "  Processed 8930000 texts, found 2323084 unique words\n",
      "  Processed 8940000 texts, found 2324058 unique words\n",
      "  Processed 8950000 texts, found 2324887 unique words\n",
      "  Processed 8960000 texts, found 2326056 unique words\n",
      "  Processed 8970000 texts, found 2326926 unique words\n",
      "  Processed 8980000 texts, found 2327881 unique words\n",
      "  Processed 8990000 texts, found 2328756 unique words\n",
      "  Processed 9000000 texts, found 2329708 unique words\n",
      "  Processed 9010000 texts, found 2330627 unique words\n",
      "  Processed 9020000 texts, found 2331507 unique words\n",
      "  Processed 9030000 texts, found 2332357 unique words\n",
      "  Processed 9040000 texts, found 2333198 unique words\n",
      "  Processed 9050000 texts, found 2333956 unique words\n",
      "  Processed 9060000 texts, found 2334982 unique words\n",
      "  Processed 9070000 texts, found 2335915 unique words\n",
      "  Processed 9080000 texts, found 2336882 unique words\n",
      "  Processed 9090000 texts, found 2337766 unique words\n",
      "  Processed 9100000 texts, found 2338697 unique words\n",
      "  Processed 9110000 texts, found 2339619 unique words\n",
      "  Processed 9120000 texts, found 2340473 unique words\n",
      "  Processed 9130000 texts, found 2341382 unique words\n",
      "  Processed 9140000 texts, found 2342307 unique words\n",
      "  Processed 9150000 texts, found 2343167 unique words\n",
      "  Processed 9160000 texts, found 2344248 unique words\n",
      "  Processed 9170000 texts, found 2345251 unique words\n",
      "  Processed 9180000 texts, found 2346140 unique words\n",
      "  Processed 9190000 texts, found 2347184 unique words\n",
      "  Processed 9200000 texts, found 2348204 unique words\n",
      "  Processed 9210000 texts, found 2349163 unique words\n",
      "  Processed 9220000 texts, found 2350145 unique words\n",
      "  Processed 9230000 texts, found 2350964 unique words\n",
      "  Processed 9240000 texts, found 2351854 unique words\n",
      "  Processed 9250000 texts, found 2352675 unique words\n",
      "  Processed 9260000 texts, found 2353587 unique words\n",
      "  Processed 9270000 texts, found 2354431 unique words\n",
      "  Processed 9280000 texts, found 2355240 unique words\n",
      "  Processed 9290000 texts, found 2356227 unique words\n",
      "  Processed 9300000 texts, found 2357038 unique words\n",
      "  Processed 9310000 texts, found 2357999 unique words\n",
      "  Processed 9320000 texts, found 2358861 unique words\n",
      "  Processed 9330000 texts, found 2359707 unique words\n",
      "  Processed 9340000 texts, found 2360510 unique words\n",
      "  Processed 9350000 texts, found 2361336 unique words\n",
      "  Processed 9360000 texts, found 2362152 unique words\n",
      "  Processed 9370000 texts, found 2362960 unique words\n",
      "  Processed 9380000 texts, found 2363954 unique words\n",
      "  Processed 9390000 texts, found 2364810 unique words\n",
      "  Processed 9400000 texts, found 2365765 unique words\n",
      "  Processed 9410000 texts, found 2366679 unique words\n",
      "  Processed 9420000 texts, found 2367557 unique words\n",
      "  Processed 9430000 texts, found 2368441 unique words\n",
      "  Processed 9440000 texts, found 2369243 unique words\n",
      "  Processed 9450000 texts, found 2370125 unique words\n",
      "  Processed 9460000 texts, found 2370891 unique words\n",
      "  Processed 9470000 texts, found 2371884 unique words\n",
      "  Processed 9480000 texts, found 2372668 unique words\n",
      "  Processed 9490000 texts, found 2373558 unique words\n",
      "  Processed 9500000 texts, found 2374373 unique words\n",
      "  Processed 9510000 texts, found 2375196 unique words\n",
      "  Processed 9520000 texts, found 2376128 unique words\n",
      "  Processed 9530000 texts, found 2377036 unique words\n",
      "  Processed 9540000 texts, found 2377988 unique words\n",
      "  Processed 9550000 texts, found 2378854 unique words\n",
      "  Processed 9560000 texts, found 2379656 unique words\n",
      "  Processed 9570000 texts, found 2380470 unique words\n",
      "  Processed 9580000 texts, found 2381400 unique words\n",
      "  Processed 9590000 texts, found 2382300 unique words\n",
      "  Processed 9600000 texts, found 2383470 unique words\n",
      "  Processed 9610000 texts, found 2384303 unique words\n",
      "  Processed 9620000 texts, found 2385118 unique words\n",
      "  Processed 9630000 texts, found 2386117 unique words\n",
      "  Processed 9640000 texts, found 2387055 unique words\n",
      "  Processed 9650000 texts, found 2387990 unique words\n",
      "  Processed 9660000 texts, found 2388728 unique words\n",
      "  Processed 9670000 texts, found 2389559 unique words\n",
      "  Processed 9680000 texts, found 2390430 unique words\n",
      "  Processed 9690000 texts, found 2391324 unique words\n",
      "  Processed 9700000 texts, found 2392138 unique words\n",
      "  Processed 9710000 texts, found 2392976 unique words\n",
      "  Processed 9720000 texts, found 2393828 unique words\n",
      "  Processed 9730000 texts, found 2394573 unique words\n",
      "  Processed 9740000 texts, found 2395410 unique words\n",
      "  Processed 9750000 texts, found 2396240 unique words\n",
      "  Processed 9760000 texts, found 2397107 unique words\n",
      "  Processed 9770000 texts, found 2397925 unique words\n",
      "  Processed 9780000 texts, found 2399016 unique words\n",
      "  Processed 9790000 texts, found 2399863 unique words\n",
      "  Processed 9800000 texts, found 2400719 unique words\n",
      "  Processed 9810000 texts, found 2401572 unique words\n",
      "  Processed 9820000 texts, found 2402351 unique words\n",
      "  Processed 9830000 texts, found 2403334 unique words\n",
      "  Processed 9840000 texts, found 2404198 unique words\n",
      "  Processed 9850000 texts, found 2404989 unique words\n",
      "  Processed 9860000 texts, found 2405742 unique words\n",
      "  Processed 9870000 texts, found 2406463 unique words\n",
      "  Processed 9880000 texts, found 2407366 unique words\n",
      "  Processed 9890000 texts, found 2408285 unique words\n",
      "  Processed 9900000 texts, found 2409078 unique words\n",
      "  Processed 9910000 texts, found 2409903 unique words\n",
      "  Processed 9920000 texts, found 2410671 unique words\n",
      "  Processed 9930000 texts, found 2411573 unique words\n",
      "  Processed 9940000 texts, found 2412405 unique words\n",
      "  Processed 9950000 texts, found 2413193 unique words\n",
      "  Processed 9960000 texts, found 2414016 unique words\n",
      "  Processed 9970000 texts, found 2414791 unique words\n",
      "  Processed 9980000 texts, found 2415577 unique words\n",
      "  Processed 9990000 texts, found 2416407 unique words\n",
      "  Processed 10000000 texts, found 2417244 unique words\n",
      "  Processed 10010000 texts, found 2417971 unique words\n",
      "  Processed 10020000 texts, found 2418698 unique words\n",
      "  Processed 10030000 texts, found 2419489 unique words\n",
      "  Processed 10040000 texts, found 2420365 unique words\n",
      "  Processed 10050000 texts, found 2421281 unique words\n",
      "  Processed 10060000 texts, found 2422046 unique words\n",
      "  Processed 10070000 texts, found 2422838 unique words\n",
      "  Processed 10080000 texts, found 2423581 unique words\n",
      "  Processed 10090000 texts, found 2424376 unique words\n",
      "  Processed 10100000 texts, found 2425141 unique words\n",
      "  Processed 10110000 texts, found 2425878 unique words\n",
      "  Processed 10120000 texts, found 2426671 unique words\n",
      "  Processed 10130000 texts, found 2427514 unique words\n",
      "  Processed 10140000 texts, found 2428303 unique words\n",
      "  Processed 10150000 texts, found 2429045 unique words\n",
      "  Processed 10160000 texts, found 2429791 unique words\n",
      "  Processed 10170000 texts, found 2430726 unique words\n",
      "  Processed 10180000 texts, found 2431489 unique words\n",
      "  Processed 10190000 texts, found 2432333 unique words\n",
      "  Processed 10200000 texts, found 2433034 unique words\n",
      "  Processed 10210000 texts, found 2433884 unique words\n",
      "  Processed 10220000 texts, found 2434659 unique words\n",
      "  Processed 10230000 texts, found 2435447 unique words\n",
      "  Processed 10240000 texts, found 2436271 unique words\n",
      "  Processed 10250000 texts, found 2437040 unique words\n",
      "  Processed 10260000 texts, found 2437774 unique words\n",
      "  Processed 10270000 texts, found 2438640 unique words\n",
      "  Processed 10280000 texts, found 2439516 unique words\n",
      "  Processed 10290000 texts, found 2440333 unique words\n",
      "  Processed 10300000 texts, found 2441148 unique words\n",
      "  Processed 10310000 texts, found 2441894 unique words\n",
      "  Processed 10320000 texts, found 2442695 unique words\n",
      "  Processed 10330000 texts, found 2443372 unique words\n",
      "  Processed 10340000 texts, found 2444209 unique words\n",
      "  Processed 10350000 texts, found 2444991 unique words\n",
      "  Processed 10360000 texts, found 2445820 unique words\n",
      "  Processed 10370000 texts, found 2446620 unique words\n",
      "  Processed 10380000 texts, found 2447395 unique words\n",
      "  Processed 10390000 texts, found 2448155 unique words\n",
      "  Processed 10400000 texts, found 2448870 unique words\n",
      "  Processed 10410000 texts, found 2449557 unique words\n",
      "  Processed 10420000 texts, found 2450348 unique words\n",
      "  Processed 10430000 texts, found 2451235 unique words\n",
      "  Processed 10440000 texts, found 2451930 unique words\n",
      "  Processed 10450000 texts, found 2452647 unique words\n",
      "  Processed 10460000 texts, found 2453613 unique words\n",
      "  Processed 10470000 texts, found 2454498 unique words\n",
      "  Processed 10480000 texts, found 2455245 unique words\n",
      "  Processed 10490000 texts, found 2456025 unique words\n",
      "  Processed 10500000 texts, found 2456711 unique words\n",
      "  Processed 10510000 texts, found 2457425 unique words\n",
      "  Processed 10520000 texts, found 2458120 unique words\n",
      "  Processed 10530000 texts, found 2458863 unique words\n",
      "  Processed 10540000 texts, found 2459555 unique words\n",
      "  Processed 10550000 texts, found 2460295 unique words\n",
      "  Processed 10560000 texts, found 2461086 unique words\n",
      "  Processed 10570000 texts, found 2461901 unique words\n",
      "  Processed 10580000 texts, found 2462686 unique words\n",
      "  Processed 10590000 texts, found 2463480 unique words\n",
      "  Processed 10600000 texts, found 2464236 unique words\n",
      "  Processed 10610000 texts, found 2465107 unique words\n",
      "  Processed 10620000 texts, found 2465773 unique words\n",
      "  Processed 10630000 texts, found 2466554 unique words\n",
      "  Processed 10640000 texts, found 2467284 unique words\n",
      "  Processed 10650000 texts, found 2468138 unique words\n",
      "  Processed 10660000 texts, found 2468912 unique words\n",
      "  Processed 10670000 texts, found 2469628 unique words\n",
      "  Processed 10680000 texts, found 2470362 unique words\n",
      "  Processed 10690000 texts, found 2471107 unique words\n",
      "  Processed 10700000 texts, found 2471854 unique words\n",
      "  Processed 10710000 texts, found 2472634 unique words\n",
      "  Processed 10720000 texts, found 2473458 unique words\n",
      "  Processed 10730000 texts, found 2474186 unique words\n",
      "  Processed 10740000 texts, found 2474869 unique words\n",
      "  Processed 10750000 texts, found 2475590 unique words\n",
      "  Processed 10760000 texts, found 2476274 unique words\n",
      "  Processed 10770000 texts, found 2477010 unique words\n",
      "  Processed 10780000 texts, found 2477861 unique words\n",
      "  Processed 10790000 texts, found 2478582 unique words\n",
      "  Processed 10800000 texts, found 2479311 unique words\n",
      "  Processed 10810000 texts, found 2479959 unique words\n",
      "  Processed 10820000 texts, found 2480722 unique words\n",
      "  Processed 10830000 texts, found 2481571 unique words\n",
      "  Processed 10840000 texts, found 2482337 unique words\n",
      "  Processed 10850000 texts, found 2483070 unique words\n",
      "  Processed 10860000 texts, found 2483770 unique words\n",
      "  Processed 10870000 texts, found 2484511 unique words\n",
      "  Processed 10880000 texts, found 2485366 unique words\n",
      "  Processed 10890000 texts, found 2486002 unique words\n",
      "  Processed 10900000 texts, found 2486772 unique words\n",
      "  Processed 10910000 texts, found 2487495 unique words\n",
      "  Processed 10920000 texts, found 2488254 unique words\n",
      "  Processed 10930000 texts, found 2489047 unique words\n",
      "  Processed 10940000 texts, found 2489830 unique words\n",
      "  Processed 10950000 texts, found 2490583 unique words\n",
      "  Processed 10960000 texts, found 2491331 unique words\n",
      "  Processed 10970000 texts, found 2492097 unique words\n",
      "  Processed 10980000 texts, found 2492905 unique words\n",
      "  Processed 10990000 texts, found 2493643 unique words\n",
      "  Processed 11000000 texts, found 2494317 unique words\n",
      "  Processed 11010000 texts, found 2495004 unique words\n",
      "  Processed 11020000 texts, found 2495738 unique words\n",
      "  Processed 11030000 texts, found 2496405 unique words\n",
      "  Processed 11040000 texts, found 2497112 unique words\n",
      "  Processed 11050000 texts, found 2498036 unique words\n",
      "  Processed 11060000 texts, found 2498828 unique words\n",
      "  Processed 11070000 texts, found 2499612 unique words\n",
      "  Processed 11080000 texts, found 2500309 unique words\n",
      "  Processed 11090000 texts, found 2501248 unique words\n",
      "  Processed 11100000 texts, found 2502051 unique words\n",
      "  Processed 11110000 texts, found 2502733 unique words\n",
      "  Processed 11120000 texts, found 2503469 unique words\n",
      "  Processed 11130000 texts, found 2504190 unique words\n",
      "  Processed 11140000 texts, found 2504880 unique words\n",
      "  Processed 11150000 texts, found 2505554 unique words\n",
      "  Processed 11160000 texts, found 2506347 unique words\n",
      "  Processed 11170000 texts, found 2507029 unique words\n",
      "  Processed 11180000 texts, found 2508477 unique words\n",
      "  Processed 11190000 texts, found 2509194 unique words\n",
      "  Processed 11200000 texts, found 2509922 unique words\n",
      "  Processed 11210000 texts, found 2510707 unique words\n",
      "  Processed 11220000 texts, found 2511399 unique words\n",
      "  Processed 11230000 texts, found 2512137 unique words\n",
      "  Processed 11240000 texts, found 2512873 unique words\n",
      "  Processed 11250000 texts, found 2513578 unique words\n",
      "  Processed 11260000 texts, found 2514329 unique words\n",
      "  Processed 11270000 texts, found 2515023 unique words\n",
      "  Processed 11280000 texts, found 2515741 unique words\n",
      "  Processed 11290000 texts, found 2516416 unique words\n",
      "  Processed 11300000 texts, found 2517138 unique words\n",
      "  Processed 11310000 texts, found 2517829 unique words\n",
      "  Processed 11320000 texts, found 2518525 unique words\n",
      "  Processed 11330000 texts, found 2519208 unique words\n",
      "  Processed 11340000 texts, found 2519909 unique words\n",
      "  Processed 11350000 texts, found 2520695 unique words\n",
      "  Processed 11360000 texts, found 2521351 unique words\n",
      "  Processed 11370000 texts, found 2522150 unique words\n",
      "  Processed 11380000 texts, found 2522838 unique words\n",
      "  Processed 11390000 texts, found 2523535 unique words\n",
      "  Processed 11400000 texts, found 2524215 unique words\n",
      "  Processed 11410000 texts, found 2524875 unique words\n",
      "  Processed 11420000 texts, found 2525606 unique words\n",
      "  Processed 11430000 texts, found 2526307 unique words\n",
      "  Processed 11440000 texts, found 2526990 unique words\n",
      "  Processed 11450000 texts, found 2527675 unique words\n",
      "  Processed 11460000 texts, found 2528395 unique words\n",
      "  Processed 11470000 texts, found 2529091 unique words\n",
      "  Processed 11480000 texts, found 2529805 unique words\n",
      "  Processed 11490000 texts, found 2530544 unique words\n",
      "  Processed 11500000 texts, found 2531286 unique words\n",
      "  Processed 11510000 texts, found 2532029 unique words\n",
      "  Processed 11520000 texts, found 2532706 unique words\n",
      "  Processed 11530000 texts, found 2533592 unique words\n",
      "  Processed 11540000 texts, found 2534239 unique words\n",
      "  Processed 11550000 texts, found 2534917 unique words\n",
      "  Processed 11560000 texts, found 2535701 unique words\n",
      "  Processed 11570000 texts, found 2536393 unique words\n",
      "  Processed 11580000 texts, found 2537285 unique words\n",
      "  Processed 11590000 texts, found 2537963 unique words\n",
      "  Processed 11600000 texts, found 2538609 unique words\n",
      "  Processed 11610000 texts, found 2539308 unique words\n",
      "  Processed 11620000 texts, found 2540098 unique words\n",
      "  Processed 11630000 texts, found 2540743 unique words\n",
      "  Processed 11640000 texts, found 2541473 unique words\n",
      "  Processed 11650000 texts, found 2542239 unique words\n",
      "  Processed 11660000 texts, found 2542938 unique words\n",
      "  Processed 11670000 texts, found 2543689 unique words\n",
      "  Processed 11680000 texts, found 2544329 unique words\n",
      "  Processed 11690000 texts, found 2544999 unique words\n",
      "  Processed 11700000 texts, found 2545700 unique words\n",
      "  Processed 11710000 texts, found 2546340 unique words\n",
      "  Processed 11720000 texts, found 2547048 unique words\n",
      "  Processed 11730000 texts, found 2547751 unique words\n",
      "  Processed 11740000 texts, found 2548459 unique words\n",
      "  Processed 11750000 texts, found 2549107 unique words\n",
      "  Processed 11760000 texts, found 2549768 unique words\n",
      "  Processed 11770000 texts, found 2550477 unique words\n",
      "  Processed 11780000 texts, found 2551336 unique words\n",
      "  Processed 11790000 texts, found 2552147 unique words\n",
      "  Processed 11800000 texts, found 2552827 unique words\n",
      "  Processed 11810000 texts, found 2553510 unique words\n",
      "  Processed 11820000 texts, found 2554116 unique words\n",
      "  Processed 11830000 texts, found 2554776 unique words\n",
      "  Processed 11840000 texts, found 2555456 unique words\n",
      "  Processed 11850000 texts, found 2556161 unique words\n",
      "  Processed 11860000 texts, found 2556845 unique words\n",
      "  Processed 11870000 texts, found 2557610 unique words\n",
      "  Processed 11880000 texts, found 2558287 unique words\n",
      "  Processed 11890000 texts, found 2558998 unique words\n",
      "  Processed 11900000 texts, found 2559648 unique words\n",
      "  Processed 11910000 texts, found 2560347 unique words\n",
      "  Processed 11920000 texts, found 2561033 unique words\n",
      "  Processed 11930000 texts, found 2561729 unique words\n",
      "  Processed 11940000 texts, found 2562402 unique words\n",
      "  Processed 11950000 texts, found 2563029 unique words\n",
      "  Processed 11960000 texts, found 2563742 unique words\n",
      "  Processed 11970000 texts, found 2564409 unique words\n",
      "  Processed 11980000 texts, found 2565112 unique words\n",
      "  Processed 11990000 texts, found 2565791 unique words\n",
      "  Processed 12000000 texts, found 2566463 unique words\n",
      "  Processed 12010000 texts, found 2567112 unique words\n",
      "  Processed 12020000 texts, found 2567842 unique words\n",
      "  Processed 12030000 texts, found 2568486 unique words\n",
      "  Processed 12040000 texts, found 2569275 unique words\n",
      "  Processed 12050000 texts, found 2569958 unique words\n",
      "  Processed 12060000 texts, found 2570567 unique words\n",
      "  Processed 12070000 texts, found 2571221 unique words\n",
      "  Processed 12080000 texts, found 2571931 unique words\n",
      "  Processed 12090000 texts, found 2572587 unique words\n",
      "  Processed 12100000 texts, found 2573228 unique words\n",
      "  Processed 12110000 texts, found 2574075 unique words\n",
      "  Processed 12120000 texts, found 2574735 unique words\n",
      "  Processed 12130000 texts, found 2575447 unique words\n",
      "  Processed 12140000 texts, found 2576108 unique words\n",
      "  Processed 12150000 texts, found 2576786 unique words\n",
      "  Processed 12160000 texts, found 2577483 unique words\n",
      "  Processed 12170000 texts, found 2578147 unique words\n",
      "  Processed 12180000 texts, found 2578878 unique words\n",
      "  Processed 12190000 texts, found 2579559 unique words\n",
      "  Processed 12200000 texts, found 2580184 unique words\n",
      "  Processed 12210000 texts, found 2580783 unique words\n",
      "  Processed 12220000 texts, found 2581398 unique words\n",
      "  Processed 12230000 texts, found 2582044 unique words\n",
      "  Processed 12240000 texts, found 2582775 unique words\n",
      "  Processed 12250000 texts, found 2583422 unique words\n",
      "  Processed 12260000 texts, found 2584095 unique words\n",
      "  Processed 12270000 texts, found 2584671 unique words\n",
      "  Processed 12280000 texts, found 2585335 unique words\n",
      "  Processed 12290000 texts, found 2586009 unique words\n",
      "  Processed 12300000 texts, found 2586638 unique words\n",
      "  Processed 12310000 texts, found 2587257 unique words\n",
      "  Processed 12320000 texts, found 2587912 unique words\n",
      "  Processed 12330000 texts, found 2588531 unique words\n",
      "  Processed 12340000 texts, found 2589061 unique words\n",
      "  Processed 12350000 texts, found 2589736 unique words\n",
      "  Processed 12360000 texts, found 2590408 unique words\n",
      "  Processed 12370000 texts, found 2591010 unique words\n",
      "  Processed 12380000 texts, found 2591687 unique words\n",
      "  Processed 12390000 texts, found 2592390 unique words\n",
      "  Processed 12400000 texts, found 2592999 unique words\n",
      "  Processed 12410000 texts, found 2593632 unique words\n",
      "  Processed 12420000 texts, found 2594237 unique words\n",
      "  Processed 12430000 texts, found 2594911 unique words\n",
      "  Processed 12440000 texts, found 2595525 unique words\n",
      "  Processed 12450000 texts, found 2596163 unique words\n",
      "  Processed 12460000 texts, found 2596761 unique words\n",
      "  Processed 12470000 texts, found 2597353 unique words\n",
      "  Processed 12480000 texts, found 2598135 unique words\n",
      "  Processed 12490000 texts, found 2598735 unique words\n",
      "  Processed 12500000 texts, found 2599410 unique words\n",
      "  Processed 12510000 texts, found 2600058 unique words\n",
      "  Processed 12520000 texts, found 2600708 unique words\n",
      "  Processed 12530000 texts, found 2601377 unique words\n",
      "  Processed 12540000 texts, found 2602011 unique words\n",
      "  Processed 12550000 texts, found 2602573 unique words\n",
      "  Processed 12560000 texts, found 2603192 unique words\n",
      "  Processed 12570000 texts, found 2603798 unique words\n",
      "  Processed 12580000 texts, found 2604408 unique words\n",
      "  Processed 12590000 texts, found 2604984 unique words\n",
      "  Processed 12600000 texts, found 2605600 unique words\n",
      "  Processed 12610000 texts, found 2606241 unique words\n",
      "  Processed 12620000 texts, found 2606881 unique words\n",
      "  Processed 12630000 texts, found 2607464 unique words\n",
      "  Processed 12640000 texts, found 2608083 unique words\n",
      "  Processed 12650000 texts, found 2608716 unique words\n",
      "  Processed 12660000 texts, found 2609302 unique words\n",
      "  Processed 12670000 texts, found 2609875 unique words\n",
      "  Processed 12680000 texts, found 2610494 unique words\n",
      "  Processed 12690000 texts, found 2611108 unique words\n",
      "  Processed 12700000 texts, found 2611813 unique words\n",
      "  Processed 12710000 texts, found 2612351 unique words\n",
      "  Processed 12720000 texts, found 2612968 unique words\n",
      "  Processed 12730000 texts, found 2613497 unique words\n",
      "  Processed 12740000 texts, found 2614157 unique words\n",
      "  Processed 12750000 texts, found 2614848 unique words\n",
      "  Processed 12760000 texts, found 2615502 unique words\n",
      "  Processed 12770000 texts, found 2616069 unique words\n",
      "  Processed 12780000 texts, found 2616624 unique words\n",
      "  Processed 12790000 texts, found 2617295 unique words\n",
      "  Processed 12800000 texts, found 2617951 unique words\n",
      "  Processed 12810000 texts, found 2618517 unique words\n",
      "  Processed 12820000 texts, found 2619126 unique words\n",
      "  Processed 12830000 texts, found 2619728 unique words\n",
      "  Processed 12840000 texts, found 2620311 unique words\n",
      "  Processed 12850000 texts, found 2620979 unique words\n",
      "  Processed 12860000 texts, found 2621615 unique words\n",
      "  Processed 12870000 texts, found 2622219 unique words\n",
      "  Processed 12880000 texts, found 2622838 unique words\n",
      "  Processed 12890000 texts, found 2623473 unique words\n",
      "  Processed 12900000 texts, found 2624112 unique words\n",
      "  Processed 12910000 texts, found 2624725 unique words\n",
      "  Processed 12920000 texts, found 2625339 unique words\n",
      "  Processed 12930000 texts, found 2625953 unique words\n",
      "  Processed 12940000 texts, found 2626659 unique words\n",
      "  Processed 12950000 texts, found 2627260 unique words\n",
      "  Processed 12960000 texts, found 2627831 unique words\n",
      "  Processed 12970000 texts, found 2628456 unique words\n",
      "  Processed 12980000 texts, found 2629067 unique words\n",
      "  Processed 12990000 texts, found 2629623 unique words\n",
      "  Processed 13000000 texts, found 2630206 unique words\n",
      "  Processed 13010000 texts, found 2630765 unique words\n",
      "  Processed 13020000 texts, found 2631309 unique words\n",
      "  Processed 13030000 texts, found 2631895 unique words\n",
      "  Processed 13040000 texts, found 2632433 unique words\n",
      "  Processed 13050000 texts, found 2633014 unique words\n",
      "  Processed 13060000 texts, found 2633607 unique words\n",
      "  Processed 13070000 texts, found 2634194 unique words\n",
      "  Processed 13080000 texts, found 2634751 unique words\n",
      "  Processed 13090000 texts, found 2635401 unique words\n",
      "  Processed 13100000 texts, found 2635974 unique words\n",
      "  Processed 13110000 texts, found 2636600 unique words\n",
      "  Processed 13120000 texts, found 2637181 unique words\n",
      "  Processed 13130000 texts, found 2637776 unique words\n",
      "  Processed 13140000 texts, found 2638419 unique words\n",
      "\n",
      " Total unique words: 2638494\n",
      " Using 5000 words for OCR dataset\n",
      "\n",
      "===== SPLIT SUMMARY =====\n",
      "Train: 4000 words\n",
      "Val  : 500 words\n",
      "Test : 500 words\n",
      "\n",
      "Sample words (train): ['', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "# -----------------------------------------\n",
    "# Extract Nepali words\n",
    "# -----------------------------------------\n",
    "def extract_nepali_words(text):\n",
    "    \"\"\"Extract Devanagari words from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    matches = re.findall(r\"[\\u0900-\\u097F]+\", text)\n",
    "    return [w for w in matches if 2 <= len(w) <= 30]\n",
    "\n",
    "all_words = set()\n",
    "print(\"Extracting Nepali words from dataset...\")\n",
    "\n",
    "for i, text in enumerate(train_texts):\n",
    "    words = extract_nepali_words(text)\n",
    "    all_words.update(words)\n",
    "\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  Processed {i + 1} texts, found {len(all_words)} unique words\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Shuffle and sample 5000 words\n",
    "# -----------------------------------------\n",
    "all_words = list(all_words)\n",
    "random.shuffle(all_words)\n",
    "\n",
    "training_pool = all_words[:5000]     # Use exactly 5000 for splitting\n",
    "print(f\"\\n Total unique words: {len(all_words)}\")\n",
    "print(f\" Using {len(training_pool)} words for OCR dataset\")\n",
    "\n",
    "# -----------------------------------------\n",
    "# Create train/val/test split\n",
    "# -----------------------------------------\n",
    "random.seed(42)  # for reproducibility\n",
    "random.shuffle(training_pool)\n",
    "\n",
    "total = len(training_pool)\n",
    "val_size = int(total * 0.1)\n",
    "test_size = int(total * 0.1)\n",
    "train_size = total - val_size - test_size\n",
    "\n",
    "train_words = training_pool[:train_size]\n",
    "val_words   = training_pool[train_size:train_size + val_size]\n",
    "test_words  = training_pool[train_size + val_size:]\n",
    "\n",
    "print(\"\\n===== SPLIT SUMMARY =====\")\n",
    "print(f\"Train: {len(train_words)} words\")\n",
    "print(f\"Val  : {len(val_words)} words\")\n",
    "print(f\"Test : {len(test_words)} words\")\n",
    "print(\"\\nSample words (train):\", train_words[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5b3a36-f4ba-427a-a4c4-ab27f1daa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "\n",
    "\n",
    "class SyntheticHarfBuzzOCRDatasetGenerator:\n",
    "    \"\"\"Generate synthetic OCR dataset with HarfBuzz shaping.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        strings,\n",
    "        fonts_dir=\"fonts\",\n",
    "        output_dir=\"data/word_images\",\n",
    "        font_size_range=(40, 56),\n",
    "        random_blur=True,\n",
    "        random_noise=True,\n",
    "        random_rotate=True,\n",
    "        random_distortion=True,\n",
    "        background_mode=\"random\",\n",
    "        max_image_size=1024\n",
    "    ):\n",
    "        self.strings = strings\n",
    "        self.fonts = glob.glob(os.path.join(fonts_dir, \"**/*.ttf\"), recursive=True)\n",
    "        if not self.fonts:\n",
    "            raise ValueError(f\"No fonts found in {fonts_dir}\")\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        self.font_size_range = font_size_range\n",
    "        self.random_blur = random_blur\n",
    "        self.random_noise = random_noise\n",
    "        self.random_rotate = random_rotate\n",
    "        self.random_distortion = random_distortion\n",
    "        self.background_mode = background_mode\n",
    "        self.MAX_SIZE = max_image_size\n",
    "\n",
    "    def _clamp_image_size(self, img):\n",
    "        w, h = img.size\n",
    "        if w > self.MAX_SIZE or h > self.MAX_SIZE:\n",
    "            img.thumbnail((self.MAX_SIZE, self.MAX_SIZE), Image.LANCZOS)\n",
    "        return img\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        for idx, text in enumerate(self.strings, start=1):\n",
    "            img = self.render_text_image(text)\n",
    "            image_path = os.path.join(self.output_dir, f\"{idx:05d}.png\")\n",
    "            label_path = os.path.join(self.output_dir, f\"{idx:05d}.txt\")\n",
    "            img.save(image_path)\n",
    "            with open(label_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "            if idx % 500 == 0:\n",
    "                print(f\"  [{idx}/{len(self.strings)}] Generated images\")\n",
    "\n",
    "    def render_text_image(self, text, padding=20):\n",
    "        font_path = random.choice(self.fonts)\n",
    "        font_size = random.randint(*self.font_size_range)\n",
    "        face = freetype.Face(font_path)\n",
    "        face.set_char_size(font_size * 64)\n",
    "\n",
    "        # HarfBuzz shaping\n",
    "        hb_blob = hb.Blob.from_file_path(font_path)\n",
    "        hb_face = hb.Face(hb_blob, 0)\n",
    "        hb_font = hb.Font(hb_face)\n",
    "        hb_font.scale = (face.size.ascender, face.size.ascender)\n",
    "\n",
    "        buf = hb.Buffer()\n",
    "        buf.add_str(text)\n",
    "        buf.guess_segment_properties()\n",
    "        hb.shape(hb_font, buf)\n",
    "\n",
    "        infos = buf.glyph_infos\n",
    "        positions = buf.glyph_positions\n",
    "\n",
    "        width = sum(pos.x_advance for pos in positions) // 64 + 2*padding\n",
    "        height = font_size + 2*padding\n",
    "\n",
    "        if self.background_mode == \"white\":\n",
    "            img = Image.new(\"RGB\", (width, height), \"white\")\n",
    "        elif self.background_mode == \"lightgray\":\n",
    "            img = Image.new(\"RGB\", (width, height), \"lightgray\")\n",
    "        else:\n",
    "            arr = np.random.randint(200, 255, (height, width, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "\n",
    "        x, y = padding, padding + font_size\n",
    "\n",
    "        for info, pos in zip(infos, positions):\n",
    "            glyph_index = info.codepoint\n",
    "            face.load_glyph(glyph_index, freetype.FT_LOAD_RENDER)\n",
    "            bitmap = face.glyph.bitmap\n",
    "            top = face.glyph.bitmap_top\n",
    "            left = face.glyph.bitmap_left\n",
    "\n",
    "            if bitmap.width > 0 and bitmap.rows > 0:\n",
    "                glyph_img = Image.frombytes(\"L\", (bitmap.width, bitmap.rows), bytes(bitmap.buffer))\n",
    "                colored_glyph = Image.new(\"RGB\", glyph_img.size, \"black\")\n",
    "                img.paste(colored_glyph, (int(x + left), int(y - top)), glyph_img)\n",
    "\n",
    "            x += pos.x_advance / 64\n",
    "            y -= pos.y_advance / 64\n",
    "\n",
    "        img = self._clamp_image_size(img)\n",
    "\n",
    "        # --- FIXED BLUR SECTION ---\n",
    "        from PIL import ImageFilter\n",
    "\n",
    "        if self.random_blur and random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))\n",
    "\n",
    "\n",
    "        if self.random_rotate:\n",
    "            angle = random.randint(-7, 7)\n",
    "            img = img.rotate(angle, expand=True, fillcolor=\"white\")\n",
    "            img = self._clamp_image_size(img)\n",
    "\n",
    "        if self.random_distortion:\n",
    "            img = self.perspective_distortion(img)\n",
    "\n",
    "        if self.random_noise:\n",
    "            img = self.add_noise(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def perspective_distortion(self, img):\n",
    "        img = self._clamp_image_size(img)\n",
    "        w, h = img.size\n",
    "        arr = np.array(img)\n",
    "        shift = min(w, h) * 0.1\n",
    "\n",
    "        pts1 = np.float32([[0,0],[w,0],[0,h],[w,h]])\n",
    "        pts2 = np.float32([\n",
    "            [random.uniform(-shift, shift), random.uniform(-shift, shift)],\n",
    "            [w + random.uniform(-shift, shift), random.uniform(-shift, shift)],\n",
    "            [random.uniform(-shift, shift), h + random.uniform(-shift, shift)],\n",
    "            [w + random.uniform(-shift, shift), h + random.uniform(-shift, shift)],\n",
    "        ])\n",
    "        matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "        warped = cv2.warpPerspective(arr, matrix, (w,h), borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))\n",
    "        return Image.fromarray(warped)\n",
    "\n",
    "    def add_noise(self, img):\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        if random.random() < 0.5:\n",
    "            arr += np.random.normal(0, 10, arr.shape)\n",
    "        if random.random() < 0.5:\n",
    "            amount = 0.02\n",
    "            num_salt = int(arr.size * amount * 0.5)\n",
    "            num_pepper = int(arr.size * amount * 0.5)\n",
    "            coords = [np.random.randint(0, i - 1, num_salt) for i in arr.shape]\n",
    "            arr[tuple(coords)] = 255\n",
    "            coords = [np.random.randint(0, i - 1, num_pepper) for i in arr.shape]\n",
    "            arr[tuple(coords)] = 0\n",
    "        arr = np.clip(arr, 0, 255)\n",
    "        return Image.fromarray(arr.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "506d9bbe-2909-4ba0-a601-b7fddfd5ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating split: data/ocr_dataset/train ===\n",
      "  [500/4000] Generated images\n",
      "  [1000/4000] Generated images\n",
      "  [1500/4000] Generated images\n",
      "  [2000/4000] Generated images\n",
      "  [2500/4000] Generated images\n",
      "  [3000/4000] Generated images\n",
      "  [3500/4000] Generated images\n",
      "  [4000/4000] Generated images\n",
      " Completed: data/ocr_dataset/train\n",
      "\n",
      "=== Generating split: data/ocr_dataset/val ===\n",
      "  [500/500] Generated images\n",
      " Completed: data/ocr_dataset/val\n",
      "\n",
      "=== Generating split: data/ocr_dataset/test ===\n",
      "  [500/500] Generated images\n",
      " Completed: data/ocr_dataset/test\n",
      "\n",
      "==============================\n",
      " OCR DATASET GENERATION DONE\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# from synthetic_generator import SyntheticHarfBuzzOCRDatasetGenerator  # your class\n",
    "\n",
    "def generate_split(words, out_dir):\n",
    "    \"\"\"Use your existing generator to create a split.\"\"\"\n",
    "    print(f\"\\n=== Generating split: {out_dir} ===\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    generator = SyntheticHarfBuzzOCRDatasetGenerator(\n",
    "        strings=words,\n",
    "        fonts_dir=\"fonts\",\n",
    "        output_dir=out_dir,\n",
    "        font_size_range=(40, 56),\n",
    "        random_blur=True,\n",
    "        random_noise=True,\n",
    "        random_rotate=True,\n",
    "        random_distortion=True,\n",
    "        background_mode=\"random\",\n",
    "        max_image_size=1024\n",
    "    )\n",
    "\n",
    "    generator.generate_dataset()\n",
    "\n",
    "    print(f\" Completed: {out_dir}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#              MAIN  GENERATE DATASET SPLITS\n",
    "# ============================================================\n",
    "\n",
    "root = \"data/ocr_dataset\"\n",
    "\n",
    "train_dir = os.path.join(root, \"train\")\n",
    "val_dir   = os.path.join(root, \"val\")\n",
    "test_dir  = os.path.join(root, \"test\")\n",
    "\n",
    "os.makedirs(root, exist_ok=True)\n",
    "\n",
    "# --- Generate all ---\n",
    "generate_split(train_words, train_dir)\n",
    "generate_split(val_words, val_dir)\n",
    "generate_split(test_words, test_dir)\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\" OCR DATASET GENERATION DONE\")\n",
    "print(\"==============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a7cc4a-bb54-40d9-b2aa-7e527cddc173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHARSET GENERATION COMPLETE\n",
      "======================================================================\n",
      " Total characters: 77\n",
      " Charset saved to: data/charset.txt\n",
      " num_classes = 78 (includes CTC Blank)\n",
      "Sample characters: ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "#      BUILD CHARSET FROM ALL SPLITS\n",
    "# ===========================================\n",
    "\n",
    "all_dataset_words = list(train_words) + list(val_words) + list(test_words)\n",
    "\n",
    "# Extract unique characters\n",
    "charset = set()\n",
    "for word in all_dataset_words:\n",
    "    charset.update(word)\n",
    "\n",
    "charset = sorted(list(charset))\n",
    "\n",
    "# Save to file\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "charset_path = \"data/charset.txt\"\n",
    "\n",
    "with open(charset_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(charset))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHARSET GENERATION COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\" Total characters: {len(charset)}\")\n",
    "print(f\" Charset saved to: {charset_path}\")\n",
    "print(f\" num_classes = {len(charset) + 1} (includes CTC Blank)\")\n",
    "print(\"Sample characters:\", charset[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8538a04f-05b0-4c96-bcd8-0b7ad9624192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pretrained ResNet-18 CRNN defined (with BiLSTM + CTC)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# -------------------------------\n",
    "# Pretrained ResNet-18 Feature Extractor\n",
    "# -------------------------------\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        \n",
    "        # Modify first conv layer if input is grayscale (1 channel)\n",
    "        if img_channels != 3:\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "        \n",
    "        # Copy remaining layers (exclude avgpool and fc)\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "\n",
    "        # Adaptive pooling to height=1\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.adaptive_pool(x)  # [B, C, 1, W]\n",
    "        x = x.squeeze(2)           # [B, C, W]\n",
    "        x = x.permute(2, 0, 1)     # [W, B, C] for BiLSTM\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# BiLSTM for CRNN\n",
    "# -------------------------------\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "# -------------------------------\n",
    "# OCR Model with Pretrained ResNet18 + BiLSTM\n",
    "# -------------------------------\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        preds_log = preds.log_softmax(2)\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "print(\" Pretrained ResNet-18 CRNN defined (with BiLSTM + CTC)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2b89e8a-ad1f-42a7-9003-77cc0343fcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " config.yaml created\n",
      " Charset size: 77  78 classes\n",
      "\n",
      "To start training, run: python scripts/train_word_ocr.py\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Create config\n",
    "config_content = f\"\"\"# CRNN OCR Configuration\n",
    "num_classes: {len(charset) + 1}\n",
    "num_channels: 1\n",
    "hidden_size: 256\n",
    "\n",
    "img_height: 32\n",
    "img_width: 256\n",
    "\n",
    "batch_size: 32\n",
    "epochs: 100\n",
    "learning_rate: 0.001\n",
    "weight_decay: 1e-5\n",
    "scheduler_step: 15\n",
    "scheduler_gamma: 0.5\n",
    "\n",
    "train_samples: 5000\n",
    "samples_per_word: 1\n",
    "fonts_dir: \"fonts\"\n",
    "output_dir: \"data/word_images\"\n",
    "charset_path: \"charset.txt\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\" config.yaml created\")\n",
    "print(f\" Charset size: {len(charset)}  {len(charset) + 1} classes\")\n",
    "print(\"\\nTo start training, run: python scripts/train_word_ocr.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6a7c644-df81-4588-8740-cbc1a2680069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: {'num_classes': 78, 'num_channels': 1, 'hidden_size': 256, 'img_height': 32, 'img_width': 256, 'batch_size': 32, 'epochs': 100, 'learning_rate': 0.001, 'weight_decay': '1e-5', 'scheduler_step': 15, 'scheduler_gamma': 0.5, 'train_samples': 5000, 'samples_per_word': 1, 'fonts_dir': 'fonts', 'output_dir': 'data/word_images', 'charset_path': 'charset.txt'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded config:\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad882efd-0f20-4a67-a4b0-71d69e6f013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# FIX the incorrect type\n",
    "cfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09b93a39-9475-4de9-8442-caa465b70c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# OCR Dataset for Nepali CRNN\n",
    "# ===============================\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root, charset_path, img_h=32, img_w=256, debug=False):\n",
    "        \"\"\"\n",
    "        root: path to train/val/test folder\n",
    "        charset_path: path to charset.txt\n",
    "        img_h, img_w: target image size\n",
    "        debug: print info for each sample\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.debug = debug\n",
    "\n",
    "        # Load all image files\n",
    "        self.files = sorted([f for f in os.listdir(root) if f.endswith(\".png\")])\n",
    "\n",
    "        # Load charset\n",
    "        with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.charset = [\"blank\"] + list(f.read().strip())\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.charset)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "        txt_path = os.path.join(self.root, img_name[:-4] + \".txt\")\n",
    "\n",
    "        # Load and resize image (height fixed, width scaled)\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        w, h = img.size\n",
    "        new_h = self.img_h\n",
    "        new_w = int(w * (new_h / h))\n",
    "        img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # Pad width to img_w if needed\n",
    "        if new_w < self.img_w:\n",
    "            padded = Image.new(\"L\", (self.img_w, new_h), 255)\n",
    "            padded.paste(img, (0, 0))\n",
    "            img = padded\n",
    "        else:\n",
    "            img = img.crop((0, 0, self.img_w, new_h))  # crop if wider than img_w\n",
    "\n",
    "        img = np.array(img)\n",
    "        img = torch.from_numpy(img).float().unsqueeze(0) / 255.0  # [1,H,W]\n",
    "\n",
    "        # Load label\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        label = self.encode(text)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[{idx}] {img_name}: text='{text}', label={label}, len={len(label)}\")\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# ===============================\n",
    "# Collate function for DataLoader\n",
    "# ===============================\n",
    "def ocr_collate(batch):\n",
    "    imgs, labels, label_lens = [], [], []\n",
    "\n",
    "    for img, label in batch:\n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "        label_lens.append(len(label))\n",
    "\n",
    "    if len(imgs) == 0:\n",
    "        raise ValueError(\"All labels in this batch are empty. Check charset or txt files.\")\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    labels = torch.cat(labels)\n",
    "    label_lens = torch.tensor(label_lens, dtype=torch.int32)\n",
    "\n",
    "    return imgs, labels, label_lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fce0890-ef13-415a-98a5-fed9d70606cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images: torch.Size([32, 1, 32, 256]) [B,C,H,W]\n",
      "Labels length: tensor([10, 11,  5, 11, 10, 13,  6, 18, 13, 11, 11,  7, 10, 13, 11, 12,  9,  8,\n",
      "        12, 12,  7,  5,  7, 10,  4,  8, 10,  7, 14, 10,  6,  7],\n",
      "       dtype=torch.int32)\n",
      "Total labels: torch.Size([308])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ===============================\n",
    "# Paths and Parameters\n",
    "# ===============================\n",
    "train_root = \"data/ocr_dataset/train\"\n",
    "val_root   = \"data/ocr_dataset/val\"\n",
    "test_root  = \"data/ocr_dataset/test\"\n",
    "\n",
    "charset_path = \"data/charset.txt\"\n",
    "\n",
    "img_h = 32\n",
    "img_w = 256\n",
    "batch_size = 32\n",
    "num_workers = 4  # adjust based on your CPU\n",
    "\n",
    "# ===============================\n",
    "# Create Datasets\n",
    "# ===============================\n",
    "train_dataset = OCRDataset(train_root, charset_path, img_h=img_h, img_w=img_w, debug=False)\n",
    "val_dataset   = OCRDataset(val_root, charset_path, img_h=img_h, img_w=img_w, debug=False)\n",
    "test_dataset  = OCRDataset(test_root, charset_path, img_h=img_h, img_w=img_w, debug=False)\n",
    "\n",
    "# ===============================\n",
    "# Create DataLoaders\n",
    "# ===============================\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=ocr_collate,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=ocr_collate,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=ocr_collate,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# ===============================\n",
    "# Verification\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    batch = next(iter(train_loader))\n",
    "    imgs, labels, label_lens = batch\n",
    "    print(f\"Images: {imgs.shape} [B,C,H,W]\")\n",
    "    print(f\"Labels length: {label_lens}\")\n",
    "    print(f\"Total labels: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65d26da3-d675-4bf9-be01-6dd6818b4c42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_image\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OCRDataset, ocr_collate  \u001b[38;5;66;03m# your dataset code\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcrnn_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OCRModelResNet18     \u001b[38;5;66;03m# your CRNN model code\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Load config\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from dataset import OCRDataset, ocr_collate  # your dataset code\n",
    "from crnn_model import OCRModelResNet18     # your CRNN model code\n",
    "\n",
    "# ===============================\n",
    "# Load config\n",
    "# ===============================\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===============================\n",
    "# Paths\n",
    "# ===============================\n",
    "train_root = os.path.join(cfg[\"output_dir\"], \"train\")\n",
    "val_root   = os.path.join(cfg[\"output_dir\"], \"val\")\n",
    "test_root  = os.path.join(cfg[\"output_dir\"], \"test\")\n",
    "charset_path = cfg[\"charset_path\"]\n",
    "\n",
    "# ===============================\n",
    "# Datasets & DataLoaders\n",
    "# ===============================\n",
    "train_dataset = OCRDataset(train_root, charset_path, img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "val_dataset   = OCRDataset(val_root, charset_path, img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "test_dataset  = OCRDataset(test_root, charset_path, img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ===============================\n",
    "# Model, optimizer, scheduler\n",
    "# ===============================\n",
    "model = OCRModelResNet18(num_classes=cfg[\"num_classes\"], img_channels=cfg[\"num_channels\"],\n",
    "                         hidden_size=cfg[\"hidden_size\"]).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg[\"learning_rate\"], weight_decay=cfg[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=cfg[\"scheduler_step\"], gamma=cfg[\"scheduler_gamma\"])\n",
    "\n",
    "# ===============================\n",
    "# Training loop\n",
    "# ===============================\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels, label_lens in tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg['epochs']}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        pred_lengths = torch.full((batch_size,), preds.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "        loss = model.compute_ctc_loss(preds, labels, pred_lengths, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{cfg['epochs']}]  Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save sample image from validation set\n",
    "    model.eval()\n",
    "    sample_img, sample_label = next(iter(val_loader))\n",
    "    sample_img = sample_img.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(sample_img)\n",
    "        pred_text_idx = pred.argmax(2)[:, 0].cpu().numpy()  # greedy decode\n",
    "        pred_text = \"\".join([train_dataset.charset[i] for i in pred_text_idx if i != 0])\n",
    "\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_sample.png\")\n",
    "    save_image(sample_img[0], save_path)\n",
    "    print(f\"Sample prediction saved: {save_path}  Predicted text: {pred_text}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"crnn_epoch_{epoch}.pth\"))\n",
    "    scheduler.step()\n",
    "\n",
    "print(\" Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83275a96-db63-4a34-9d5f-53cb6feb5b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config: {'num_classes': 79, 'num_channels': 1, 'hidden_size': 256, 'img_height': 32, 'img_width': 256, 'batch_size': 64, 'epochs': 100, 'learning_rate': 0.001, 'weight_decay': '1e-5', 'scheduler_step': 15, 'scheduler_gamma': 0.5, 'train_samples': 5000, 'samples_per_word': 1, 'fonts_dir': 'fonts', 'output_dir': 'data/ocr_dataset', 'charset_path': 'charset.txt'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"Loaded config:\", cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8d9f752-03b4-4314-b2d3-d9c4712311f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# FIX the incorrect type\n",
    "cfg[\"weight_decay\"] = float(cfg[\"weight_decay\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27c2c6a-16ba-4cb6-9b3c-1036b258b02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f16ac7ce-3873-4c3a-8ef5-1294dc41890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|| 63/63 [00:05<00:00, 11.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]  Average Loss: nan\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 219\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Save sample image from validation\u001b[39;00m\n\u001b[1;32m    218\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 219\u001b[0m sample_img, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(val_loader))\n\u001b[1;32m    220\u001b[0m sample_img \u001b[38;5;241m=\u001b[39m sample_img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# OCR Dataset\n",
    "# -------------------------------\n",
    "# ===============================\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root, charset_path, img_h=32, img_w=256, debug=False):\n",
    "        self.root = root\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.debug = debug\n",
    "\n",
    "        self.files = sorted([f for f in os.listdir(root) if f.endswith(\".png\")])\n",
    "\n",
    "        with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.charset = [\"blank\"] + list(f.read().strip())\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.charset)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "        txt_path = os.path.join(self.root, img_name[:-4] + \".txt\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        w, h = img.size\n",
    "        new_h = self.img_h\n",
    "        new_w = int(w * (new_h / h))\n",
    "        img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # pad width\n",
    "        if new_w < self.img_w:\n",
    "            padded = Image.new(\"L\", (self.img_w, new_h), 255)\n",
    "            padded.paste(img, (0, 0))\n",
    "            img = padded\n",
    "        else:\n",
    "            img = img.crop((0, 0, self.img_w, new_h))\n",
    "\n",
    "        img = np.array(img)\n",
    "        img = torch.from_numpy(img).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        label = self.encode(text)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[{idx}] {img_name}: text='{text}', label={label}, len={len(label)}\")\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def ocr_collate(batch):\n",
    "    imgs, labels, label_lens = [], [], []\n",
    "    for img, label in batch:\n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "        label_lens.append(len(label))\n",
    "\n",
    "    if len(imgs) == 0:\n",
    "        raise ValueError(\"All labels in this batch are empty.\")\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    labels = torch.cat(labels)\n",
    "    label_lens = torch.tensor(label_lens, dtype=torch.int32)\n",
    "    return imgs, labels, label_lens\n",
    "\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# CRNN Model with pretrained ResNet-18\n",
    "# -------------------------------\n",
    "# ===============================\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        if img_channels != 3:\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, 7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        preds_log = preds.log_softmax(2)\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# Load Config\n",
    "# ------------------------------\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Use GPU 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===============================\n",
    "# Prepare DataLoaders\n",
    "# ===============================\n",
    "train_dataset = OCRDataset(os.path.join(cfg[\"output_dir\"], \"train\"), cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "val_dataset   = OCRDataset(os.path.join(cfg[\"output_dir\"], \"val\"), cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "test_dataset  = OCRDataset(os.path.join(cfg[\"output_dir\"], \"test\"), cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ===============================\n",
    "# Initialize model, optimizer, scheduler\n",
    "# ===============================\n",
    "model = OCRModelResNet18(num_classes=cfg[\"num_classes\"], img_channels=cfg[\"num_channels\"],\n",
    "                         hidden_size=cfg[\"hidden_size\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg[\"learning_rate\"], weight_decay=cfg[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=cfg[\"scheduler_step\"], gamma=cfg[\"scheduler_gamma\"])\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ===============================\n",
    "# Training loop\n",
    "# ===============================\n",
    "for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels, label_lens in tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg['epochs']}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        pred_lengths = torch.full((batch_size,), preds.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "        loss = model.compute_ctc_loss(preds, labels, pred_lengths, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{cfg['epochs']}]  Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save sample image from validation\n",
    "    model.eval()\n",
    "    sample_img, _ = next(iter(val_loader))\n",
    "    sample_img = sample_img.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(sample_img)\n",
    "        pred_text_idx = pred.argmax(2)[:,0].cpu().numpy()\n",
    "        pred_text = \"\".join([train_dataset.charset[i] for i in pred_text_idx if i != 0])\n",
    "\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_sample.png\")\n",
    "    save_image(sample_img[0], save_path)\n",
    "    print(f\"Sample prediction saved: {save_path}  Predicted text: {pred_text}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"crnn_epoch_{epoch}.pth\"))\n",
    "    scheduler.step()\n",
    "\n",
    "print(\" Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b0ac4ca-0375-4922-9c8a-5dea8ea2351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|| 63/63 [00:04<00:00, 12.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]  Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_1_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|| 63/63 [00:04<00:00, 12.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]  Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_2_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|| 63/63 [00:04<00:00, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]  Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_3_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|| 63/63 [00:04<00:00, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]  Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_4_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100:   8%|         | 5/63 [00:00<00:09,  6.33it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 213\u001b[0m\n\u001b[1;32m    210\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    211\u001b[0m pred_lengths \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((batch_size,), preds\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 213\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_ctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    215\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[27], line 152\u001b[0m, in \u001b[0;36mOCRModelResNet18.compute_ctc_loss\u001b[0;34m(self, preds, targets, pred_lengths, target_lengths)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_ctc_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, targets, pred_lengths, target_lengths):\n\u001b[1;32m    151\u001b[0m     preds_log \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_log\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_lengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/loss.py:2019\u001b[0m, in \u001b[0;36mCTCLoss.forward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2014\u001b[0m     log_probs: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2017\u001b[0m     target_lengths: Tensor,\n\u001b[1;32m   2018\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/functional.py:3075\u001b[0m, in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(log_probs, targets, input_lengths, target_lengths):\n\u001b[1;32m   3064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3065\u001b[0m         ctc_loss,\n\u001b[1;32m   3066\u001b[0m         (log_probs, targets, input_lengths, target_lengths),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3073\u001b[0m         zero_infinity\u001b[38;5;241m=\u001b[39mzero_infinity,\n\u001b[1;32m   3074\u001b[0m     )\n\u001b[0;32m-> 3075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctc_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3078\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_lengths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3081\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzero_infinity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# OCR Dataset\n",
    "# -------------------------------\n",
    "# ===============================\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root, charset_path, img_h=32, img_w=256, debug=False):\n",
    "        self.root = root\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.debug = debug\n",
    "\n",
    "        self.files = sorted([f for f in os.listdir(root) if f.endswith(\".png\")])\n",
    "\n",
    "        with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.charset = [\"blank\"] + list(f.read().strip())\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.charset)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "        txt_path = os.path.join(self.root, img_name[:-4] + \".txt\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        w, h = img.size\n",
    "        new_h = self.img_h\n",
    "        new_w = int(w * (new_h / h))\n",
    "        img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # pad width\n",
    "        if new_w < self.img_w:\n",
    "            padded = Image.new(\"L\", (self.img_w, new_h), 255)\n",
    "            padded.paste(img, (0, 0))\n",
    "            img = padded\n",
    "        else:\n",
    "            img = img.crop((0, 0, self.img_w, new_h))\n",
    "\n",
    "        img = np.array(img)\n",
    "        img = torch.from_numpy(img).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        label = self.encode(text)\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"[{idx}] {img_name}: text='{text}', label={label}, len={len(label)}\")\n",
    "\n",
    "        return img, label\n",
    "\n",
    "\n",
    "def ocr_collate(batch):\n",
    "    imgs, labels, label_lens = [], [], []\n",
    "    for img, label in batch:\n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "        label_lens.append(len(label))\n",
    "\n",
    "    if len(imgs) == 0:\n",
    "        raise ValueError(\"All labels in this batch are empty.\")\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    labels = torch.cat(labels)\n",
    "    label_lens = torch.tensor(label_lens, dtype=torch.int32)\n",
    "    return imgs, labels, label_lens\n",
    "\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# CRNN Model with pretrained ResNet-18\n",
    "# -------------------------------\n",
    "# ===============================\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        if img_channels != 3:\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, 7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        preds_log = preds.log_softmax(2)\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "# ===============================\n",
    "# -------------------------------\n",
    "# Load Config\n",
    "# ----------------==============\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "# Force floats for learning rate and weight decay\n",
    "learning_rate = float(cfg[\"learning_rate\"])\n",
    "weight_decay = float(cfg[\"weight_decay\"])\n",
    "\n",
    "# Use GPU 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===============================\n",
    "# Prepare DataLoaders\n",
    "# ===============================\n",
    "train_dataset = OCRDataset(\"data/ocr_dataset/train\", cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "val_dataset   = OCRDataset(\"data/ocr_dataset/val\", cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "test_dataset  = OCRDataset(\"data/ocr_dataset/test\", cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ===============================\n",
    "# Initialize model, optimizer, scheduler\n",
    "# ===============================\n",
    "model = OCRModelResNet18(num_classes=cfg[\"num_classes\"], img_channels=cfg[\"num_channels\"],\n",
    "                         hidden_size=cfg[\"hidden_size\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=cfg[\"scheduler_step\"], gamma=cfg[\"scheduler_gamma\"])\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ===============================\n",
    "# Training loop\n",
    "# ===============================\n",
    "for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels, label_lens in tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg['epochs']}\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "\n",
    "        batch_size = imgs.size(0)\n",
    "        pred_lengths = torch.full((batch_size,), preds.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "        loss = model.compute_ctc_loss(preds, labels, pred_lengths, label_lens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch}/{cfg['epochs']}]  Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save sample image from validation\n",
    "    model.eval()\n",
    "    sample_img, sample_labels, sample_lens = next(iter(val_loader))\n",
    "    sample_img = sample_img.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(sample_img)\n",
    "        pred_text_idx = pred.argmax(2)[:, 0].cpu().numpy()\n",
    "        pred_text = \"\".join([train_dataset.charset[i] for i in pred_text_idx if i != 0])\n",
    "\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_sample.png\")\n",
    "    save_image(sample_img[0], save_path)\n",
    "    print(f\"Sample prediction saved: {save_path}  Predicted text: {pred_text}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"crnn_epoch_{epoch}.pth\"))\n",
    "    scheduler.step()\n",
    "\n",
    "print(\" Training complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3bdbeefc-183c-4eb8-b7b4-20e959c6b2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3323772/869259267.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # AMP scaler\n",
      "/tmp/ipykernel_3323772/869259267.py:218: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/100 (Train): 100%|| 63/63 [00:04<00:00, 13.04it/s]\n",
      "Epoch 1/100 (Val): 100%|| 8/8 [00:00<00:00, 10.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]  Train Loss: 2.1151, Val Loss: 1.9152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_1_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 (Train): 100%|| 63/63 [00:04<00:00, 13.75it/s]\n",
      "Epoch 2/100 (Val): 100%|| 8/8 [00:00<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]  Train Loss: 2.1826, Val Loss: 1.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_2_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 (Train): 100%|| 63/63 [00:04<00:00, 13.66it/s]\n",
      "Epoch 3/100 (Val): 100%|| 8/8 [00:00<00:00, 10.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]  Train Loss: 2.1659, Val Loss: 2.0991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_3_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 (Train): 100%|| 63/63 [00:03<00:00, 18.44it/s]\n",
      "Epoch 4/100 (Val): 100%|| 8/8 [00:00<00:00, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]  Train Loss: 2.1877, Val Loss: 2.0772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_4_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 (Train): 100%|| 63/63 [00:03<00:00, 16.87it/s]\n",
      "Epoch 5/100 (Val): 100%|| 8/8 [00:00<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]  Train Loss: 2.1907, Val Loss: 2.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_5_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 (Train): 100%|| 63/63 [00:04<00:00, 12.67it/s]\n",
      "Epoch 6/100 (Val): 100%|| 8/8 [00:00<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]  Train Loss: 2.1871, Val Loss: 2.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_6_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 (Train): 100%|| 63/63 [00:04<00:00, 12.92it/s]\n",
      "Epoch 7/100 (Val): 100%|| 8/8 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]  Train Loss: 2.1891, Val Loss: 2.0760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_7_sample.png  Predicted text: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 (Train): 100%|| 63/63 [00:02<00:00, 27.15it/s]\n",
      "Epoch 8/100 (Val): 100%|| 8/8 [00:00<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]  Train Loss: 0.0843, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_8_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 (Train): 100%|| 63/63 [00:02<00:00, 26.56it/s]\n",
      "Epoch 9/100 (Val): 100%|| 8/8 [00:00<00:00, 10.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_9_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 (Train): 100%|| 63/63 [00:02<00:00, 24.83it/s]\n",
      "Epoch 10/100 (Val): 100%|| 8/8 [00:00<00:00, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_10_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 (Train): 100%|| 63/63 [00:02<00:00, 25.49it/s]\n",
      "Epoch 11/100 (Val): 100%|| 8/8 [00:00<00:00,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_11_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 (Train): 100%|| 63/63 [00:02<00:00, 28.10it/s]\n",
      "Epoch 12/100 (Val): 100%|| 8/8 [00:00<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_12_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 (Train): 100%|| 63/63 [00:03<00:00, 20.98it/s]\n",
      "Epoch 13/100 (Val): 100%|| 8/8 [00:00<00:00, 10.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_13_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 (Train): 100%|| 63/63 [00:02<00:00, 24.05it/s]\n",
      "Epoch 14/100 (Val): 100%|| 8/8 [00:00<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100]  Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Sample prediction saved: checkpoints/epoch_14_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 (Train): 100%|| 63/63 [00:02<00:00, 27.13it/s]\n",
      "Epoch 15/100 (Val): 100%|| 8/8 [00:00<00:00, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_15_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 (Train): 100%|| 63/63 [00:02<00:00, 23.89it/s]\n",
      "Epoch 16/100 (Val): 100%|| 8/8 [00:00<00:00, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_16_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 (Train): 100%|| 63/63 [00:02<00:00, 23.48it/s]\n",
      "Epoch 17/100 (Val): 100%|| 8/8 [00:00<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_17_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 (Train): 100%|| 63/63 [00:02<00:00, 27.28it/s]\n",
      "Epoch 18/100 (Val): 100%|| 8/8 [00:00<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_18_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 (Train): 100%|| 63/63 [00:02<00:00, 27.44it/s]\n",
      "Epoch 19/100 (Val): 100%|| 8/8 [00:00<00:00, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_19_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 (Train): 100%|| 63/63 [00:02<00:00, 24.28it/s]\n",
      "Epoch 20/100 (Val): 100%|| 8/8 [00:00<00:00, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_20_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 (Train): 100%|| 63/63 [00:02<00:00, 31.10it/s]\n",
      "Epoch 21/100 (Val): 100%|| 8/8 [00:00<00:00, 12.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100]  Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Sample prediction saved: checkpoints/epoch_21_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 (Train): 100%|| 63/63 [00:02<00:00, 29.52it/s]\n",
      "Epoch 22/100 (Val): 100%|| 8/8 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_22_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 (Train): 100%|| 63/63 [00:02<00:00, 25.15it/s]\n",
      "Epoch 23/100 (Val): 100%|| 8/8 [00:00<00:00, 12.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_23_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 (Train): 100%|| 63/63 [00:02<00:00, 31.04it/s]\n",
      "Epoch 24/100 (Val): 100%|| 8/8 [00:00<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_24_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 (Train): 100%|| 63/63 [00:02<00:00, 26.55it/s]\n",
      "Epoch 25/100 (Val): 100%|| 8/8 [00:00<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_25_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 (Train): 100%|| 63/63 [00:02<00:00, 25.03it/s]\n",
      "Epoch 26/100 (Val): 100%|| 8/8 [00:00<00:00, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_26_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 (Train): 100%|| 63/63 [00:02<00:00, 29.82it/s]\n",
      "Epoch 27/100 (Val): 100%|| 8/8 [00:00<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_27_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 (Train): 100%|| 63/63 [00:02<00:00, 26.59it/s]\n",
      "Epoch 28/100 (Val): 100%|| 8/8 [00:00<00:00, 12.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_28_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 (Train): 100%|| 63/63 [00:02<00:00, 24.42it/s]\n",
      "Epoch 29/100 (Val): 100%|| 8/8 [00:00<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_29_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 (Train): 100%|| 63/63 [00:02<00:00, 28.61it/s]\n",
      "Epoch 30/100 (Val): 100%|| 8/8 [00:00<00:00, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_30_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 (Train): 100%|| 63/63 [00:02<00:00, 25.40it/s]\n",
      "Epoch 31/100 (Val): 100%|| 8/8 [00:00<00:00, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_31_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 (Train): 100%|| 63/63 [00:02<00:00, 24.53it/s]\n",
      "Epoch 32/100 (Val): 100%|| 8/8 [00:00<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_32_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 (Train): 100%|| 63/63 [00:02<00:00, 27.52it/s]\n",
      "Epoch 33/100 (Val): 100%|| 8/8 [00:00<00:00, 10.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_33_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 (Train): 100%|| 63/63 [00:02<00:00, 24.17it/s]\n",
      "Epoch 34/100 (Val): 100%|| 8/8 [00:00<00:00, 10.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_34_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 (Train): 100%|| 63/63 [00:02<00:00, 24.49it/s]\n",
      "Epoch 35/100 (Val): 100%|| 8/8 [00:00<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_35_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 (Train): 100%|| 63/63 [00:02<00:00, 28.99it/s]\n",
      "Epoch 36/100 (Val): 100%|| 8/8 [00:00<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_36_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 (Train): 100%|| 63/63 [00:02<00:00, 27.18it/s]\n",
      "Epoch 37/100 (Val): 100%|| 8/8 [00:00<00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_37_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 (Train): 100%|| 63/63 [00:02<00:00, 23.91it/s]\n",
      "Epoch 38/100 (Val): 100%|| 8/8 [00:00<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_38_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 (Train): 100%|| 63/63 [00:02<00:00, 28.58it/s]\n",
      "Epoch 39/100 (Val): 100%|| 8/8 [00:00<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_39_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 (Train): 100%|| 63/63 [00:02<00:00, 29.45it/s]\n",
      "Epoch 40/100 (Val): 100%|| 8/8 [00:00<00:00, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_40_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 (Train): 100%|| 63/63 [00:02<00:00, 26.27it/s]\n",
      "Epoch 41/100 (Val): 100%|| 8/8 [00:00<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_41_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 (Train): 100%|| 63/63 [00:02<00:00, 30.33it/s]\n",
      "Epoch 42/100 (Val): 100%|| 8/8 [00:00<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_42_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 (Train): 100%|| 63/63 [00:02<00:00, 27.51it/s]\n",
      "Epoch 43/100 (Val): 100%|| 8/8 [00:00<00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_43_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 (Train): 100%|| 63/63 [00:02<00:00, 25.31it/s]\n",
      "Epoch 44/100 (Val): 100%|| 8/8 [00:00<00:00, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_44_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 (Train): 100%|| 63/63 [00:02<00:00, 30.14it/s]\n",
      "Epoch 45/100 (Val): 100%|| 8/8 [00:00<00:00, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_45_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 (Train): 100%|| 63/63 [00:02<00:00, 26.80it/s]\n",
      "Epoch 46/100 (Val): 100%|| 8/8 [00:00<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_46_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 (Train): 100%|| 63/63 [00:02<00:00, 25.07it/s]\n",
      "Epoch 47/100 (Val): 100%|| 8/8 [00:00<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_47_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 (Train): 100%|| 63/63 [00:02<00:00, 27.49it/s]\n",
      "Epoch 48/100 (Val): 100%|| 8/8 [00:00<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_48_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 (Train): 100%|| 63/63 [00:02<00:00, 26.37it/s]\n",
      "Epoch 49/100 (Val): 100%|| 8/8 [00:00<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_49_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 (Train): 100%|| 63/63 [00:02<00:00, 23.11it/s]\n",
      "Epoch 50/100 (Val): 100%|| 8/8 [00:00<00:00, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_50_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 (Train): 100%|| 63/63 [00:02<00:00, 28.28it/s]\n",
      "Epoch 51/100 (Val): 100%|| 8/8 [00:00<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_51_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 (Train): 100%|| 63/63 [00:02<00:00, 25.12it/s]\n",
      "Epoch 52/100 (Val): 100%|| 8/8 [00:00<00:00, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_52_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 (Train): 100%|| 63/63 [00:02<00:00, 25.17it/s]\n",
      "Epoch 53/100 (Val): 100%|| 8/8 [00:00<00:00, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_53_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 (Train): 100%|| 63/63 [00:02<00:00, 29.34it/s]\n",
      "Epoch 54/100 (Val): 100%|| 8/8 [00:00<00:00,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_54_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 (Train): 100%|| 63/63 [00:02<00:00, 29.69it/s]\n",
      "Epoch 55/100 (Val): 100%|| 8/8 [00:00<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_55_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 (Train): 100%|| 63/63 [00:02<00:00, 24.61it/s]\n",
      "Epoch 56/100 (Val): 100%|| 8/8 [00:00<00:00, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_56_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 (Train): 100%|| 63/63 [00:02<00:00, 28.45it/s]\n",
      "Epoch 57/100 (Val): 100%|| 8/8 [00:00<00:00,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_57_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 (Train): 100%|| 63/63 [00:02<00:00, 29.06it/s]\n",
      "Epoch 58/100 (Val): 100%|| 8/8 [00:00<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_58_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 (Train): 100%|| 63/63 [00:02<00:00, 25.99it/s]\n",
      "Epoch 59/100 (Val): 100%|| 8/8 [00:00<00:00, 13.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_59_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 (Train): 100%|| 63/63 [00:02<00:00, 29.20it/s]\n",
      "Epoch 60/100 (Val): 100%|| 8/8 [00:00<00:00, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_60_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 (Train): 100%|| 63/63 [00:02<00:00, 25.44it/s]\n",
      "Epoch 61/100 (Val): 100%|| 8/8 [00:00<00:00, 13.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_61_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 (Train): 100%|| 63/63 [00:02<00:00, 25.06it/s]\n",
      "Epoch 62/100 (Val): 100%|| 8/8 [00:00<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_62_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 (Train): 100%|| 63/63 [00:02<00:00, 30.70it/s]\n",
      "Epoch 63/100 (Val): 100%|| 8/8 [00:00<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100]  Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Sample prediction saved: checkpoints/epoch_63_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 (Train): 100%|| 63/63 [00:02<00:00, 25.14it/s]\n",
      "Epoch 64/100 (Val): 100%|| 8/8 [00:00<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100]  Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Sample prediction saved: checkpoints/epoch_64_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 (Train): 100%|| 63/63 [00:02<00:00, 23.48it/s]\n",
      "Epoch 65/100 (Val): 100%|| 8/8 [00:00<00:00, 10.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_65_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 (Train): 100%|| 63/63 [00:02<00:00, 29.52it/s]\n",
      "Epoch 66/100 (Val): 100%|| 8/8 [00:00<00:00, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_66_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 (Train): 100%|| 63/63 [00:02<00:00, 25.44it/s]\n",
      "Epoch 67/100 (Val): 100%|| 8/8 [00:00<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_67_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 (Train): 100%|| 63/63 [00:02<00:00, 25.22it/s]\n",
      "Epoch 68/100 (Val): 100%|| 8/8 [00:00<00:00, 12.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_68_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 (Train): 100%|| 63/63 [00:02<00:00, 28.22it/s]\n",
      "Epoch 69/100 (Val): 100%|| 8/8 [00:00<00:00, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_69_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 (Train): 100%|| 63/63 [00:02<00:00, 25.88it/s]\n",
      "Epoch 70/100 (Val): 100%|| 8/8 [00:00<00:00, 11.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_70_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 (Train): 100%|| 63/63 [00:02<00:00, 24.50it/s]\n",
      "Epoch 71/100 (Val): 100%|| 8/8 [00:00<00:00, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_71_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 (Train): 100%|| 63/63 [00:02<00:00, 28.89it/s]\n",
      "Epoch 72/100 (Val): 100%|| 8/8 [00:00<00:00, 13.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_72_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 (Train): 100%|| 63/63 [00:02<00:00, 27.01it/s]\n",
      "Epoch 73/100 (Val): 100%|| 8/8 [00:00<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_73_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 (Train): 100%|| 63/63 [00:02<00:00, 26.57it/s]\n",
      "Epoch 74/100 (Val): 100%|| 8/8 [00:00<00:00,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_74_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 (Train): 100%|| 63/63 [00:02<00:00, 27.97it/s]\n",
      "Epoch 75/100 (Val): 100%|| 8/8 [00:00<00:00, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_75_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 (Train): 100%|| 63/63 [00:02<00:00, 24.42it/s]\n",
      "Epoch 76/100 (Val): 100%|| 8/8 [00:00<00:00, 14.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_76_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 (Train): 100%|| 63/63 [00:02<00:00, 24.94it/s]\n",
      "Epoch 77/100 (Val): 100%|| 8/8 [00:00<00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_77_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 (Train): 100%|| 63/63 [00:01<00:00, 31.64it/s]\n",
      "Epoch 78/100 (Val): 100%|| 8/8 [00:00<00:00, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_78_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 (Train): 100%|| 63/63 [00:02<00:00, 26.56it/s]\n",
      "Epoch 79/100 (Val): 100%|| 8/8 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_79_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 (Train): 100%|| 63/63 [00:02<00:00, 24.97it/s]\n",
      "Epoch 80/100 (Val): 100%|| 8/8 [00:00<00:00, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_80_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 (Train): 100%|| 63/63 [00:02<00:00, 27.50it/s]\n",
      "Epoch 81/100 (Val): 100%|| 8/8 [00:00<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_81_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 (Train): 100%|| 63/63 [00:02<00:00, 24.33it/s]\n",
      "Epoch 82/100 (Val): 100%|| 8/8 [00:00<00:00, 10.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_82_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 (Train): 100%|| 63/63 [00:02<00:00, 24.32it/s]\n",
      "Epoch 83/100 (Val): 100%|| 8/8 [00:00<00:00, 11.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_83_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 (Train): 100%|| 63/63 [00:02<00:00, 27.60it/s]\n",
      "Epoch 84/100 (Val): 100%|| 8/8 [00:00<00:00,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_84_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 (Train): 100%|| 63/63 [00:02<00:00, 27.18it/s]\n",
      "Epoch 85/100 (Val): 100%|| 8/8 [00:00<00:00, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_85_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 (Train): 100%|| 63/63 [00:02<00:00, 24.51it/s]\n",
      "Epoch 86/100 (Val): 100%|| 8/8 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_86_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 (Train): 100%|| 63/63 [00:02<00:00, 27.54it/s]\n",
      "Epoch 87/100 (Val): 100%|| 8/8 [00:00<00:00, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_87_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 (Train): 100%|| 63/63 [00:02<00:00, 25.28it/s]\n",
      "Epoch 88/100 (Val): 100%|| 8/8 [00:00<00:00, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_88_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 (Train): 100%|| 63/63 [00:02<00:00, 23.55it/s]\n",
      "Epoch 89/100 (Val): 100%|| 8/8 [00:00<00:00, 11.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_89_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 (Train): 100%|| 63/63 [00:02<00:00, 28.42it/s]\n",
      "Epoch 90/100 (Val): 100%|| 8/8 [00:00<00:00, 11.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_90_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 (Train): 100%|| 63/63 [00:02<00:00, 25.89it/s]\n",
      "Epoch 91/100 (Val): 100%|| 8/8 [00:00<00:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_91_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 (Train): 100%|| 63/63 [00:02<00:00, 24.15it/s]\n",
      "Epoch 92/100 (Val): 100%|| 8/8 [00:00<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_92_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 (Train): 100%|| 63/63 [00:02<00:00, 29.34it/s]\n",
      "Epoch 93/100 (Val): 100%|| 8/8 [00:00<00:00, 13.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_93_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 (Train): 100%|| 63/63 [00:02<00:00, 27.53it/s]\n",
      "Epoch 94/100 (Val): 100%|| 8/8 [00:00<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_94_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 (Train): 100%|| 63/63 [00:02<00:00, 27.47it/s]\n",
      "Epoch 95/100 (Val): 100%|| 8/8 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_95_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 (Train): 100%|| 63/63 [00:02<00:00, 27.89it/s]\n",
      "Epoch 96/100 (Val): 100%|| 8/8 [00:00<00:00, 13.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100]  Train Loss: 0.0000, Val Loss: 0.0000\n",
      "Sample prediction saved: checkpoints/epoch_96_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 (Train): 100%|| 63/63 [00:02<00:00, 27.03it/s]\n",
      "Epoch 97/100 (Val): 100%|| 8/8 [00:00<00:00, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_97_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 (Train): 100%|| 63/63 [00:02<00:00, 24.81it/s]\n",
      "Epoch 98/100 (Val): 100%|| 8/8 [00:00<00:00, 10.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_98_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 (Train): 100%|| 63/63 [00:02<00:00, 29.87it/s]\n",
      "Epoch 99/100 (Val): 100%|| 8/8 [00:00<00:00, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_99_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 (Train): 100%|| 63/63 [00:02<00:00, 24.82it/s]\n",
      "Epoch 100/100 (Val): 100%|| 8/8 [00:00<00:00, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100]  Train Loss: 0.0000, Val Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_100_sample.png  Predicted text: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# ===============================\n",
    "# OCR Dataset\n",
    "# ===============================\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root, charset_path, img_h=32, img_w=512, debug=False):\n",
    "        self.root = root\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.debug = debug\n",
    "\n",
    "        self.files = sorted([f for f in os.listdir(root) \n",
    "                             if f.endswith(\".png\") and os.path.getsize(os.path.join(root, f[:-4]+\".txt\")) > 0])\n",
    "\n",
    "        with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.charset = [\"blank\"] + list(f.read().strip())\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.charset)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "        txt_path = os.path.join(self.root, img_name[:-4] + \".txt\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        w, h = img.size\n",
    "        new_h = self.img_h\n",
    "        new_w = int(w * (new_h / h))\n",
    "        img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # pad width\n",
    "        if new_w < self.img_w:\n",
    "            padded = Image.new(\"L\", (self.img_w, new_h), 255)\n",
    "            padded.paste(img, (0, 0))\n",
    "            img = padded\n",
    "        else:\n",
    "            img = img.crop((0, 0, self.img_w, new_h))\n",
    "\n",
    "        img = np.array(img)\n",
    "        img = torch.from_numpy(img).float().unsqueeze(0) / 255.0\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        label = self.encode(text)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def ocr_collate(batch):\n",
    "    imgs, labels, label_lens = [], [], []\n",
    "    for img, label in batch:\n",
    "        if len(label) == 0:\n",
    "            continue\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "        label_lens.append(len(label))\n",
    "\n",
    "    if len(imgs) == 0:\n",
    "        raise ValueError(\"All labels in this batch are empty.\")\n",
    "\n",
    "    imgs = torch.stack(imgs)\n",
    "    labels = torch.cat(labels)\n",
    "    label_lens = torch.tensor(label_lens, dtype=torch.long)\n",
    "    return imgs, labels, label_lens\n",
    "\n",
    "# ===============================\n",
    "# CRNN Model with pretrained ResNet-18\n",
    "# ===============================\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        if img_channels != 3:\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, 7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.permute(2, 0, 1)  # [seq_len, batch, channels]\n",
    "        return x\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        preds_log = preds.log_softmax(2)\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "# ===============================\n",
    "# Greedy CTC decoder\n",
    "# ===============================\n",
    "def greedy_ctc_decode(preds, charset):\n",
    "    argmax = preds.argmax(2).cpu().numpy()\n",
    "    decoded = []\n",
    "    for b in range(argmax.shape[1]):\n",
    "        last_idx = 0\n",
    "        chars = []\n",
    "        for t in range(argmax.shape[0]):\n",
    "            idx = argmax[t, b]\n",
    "            if idx != 0 and idx != last_idx:\n",
    "                chars.append(charset[idx])\n",
    "            last_idx = idx\n",
    "        decoded.append(\"\".join(chars))\n",
    "    return decoded\n",
    "\n",
    "# ===============================\n",
    "# Load Config\n",
    "# ===============================\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "learning_rate = float(cfg[\"learning_rate\"])\n",
    "weight_decay = float(cfg[\"weight_decay\"])\n",
    "\n",
    "# Use GPU 2\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ===============================\n",
    "# Prepare DataLoaders\n",
    "# ===============================\n",
    "train_dataset = OCRDataset(\"data/ocr_dataset/train\", cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "val_dataset   = OCRDataset(\"data/ocr_dataset/val\", cfg[\"charset_path\"],\n",
    "                           img_h=cfg[\"img_height\"], img_w=cfg[\"img_width\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg[\"batch_size\"], shuffle=True,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=cfg[\"batch_size\"], shuffle=False,\n",
    "                          collate_fn=ocr_collate, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ===============================\n",
    "# Initialize model, optimizer, scheduler\n",
    "# ===============================\n",
    "model = OCRModelResNet18(num_classes=cfg[\"num_classes\"], img_channels=cfg[\"num_channels\"],\n",
    "                         hidden_size=cfg[\"hidden_size\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=cfg[\"scheduler_step\"], gamma=cfg[\"scheduler_gamma\"])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()  # AMP scaler\n",
    "\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# ===============================\n",
    "# Training loop\n",
    "# ===============================\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "    # -------------------\n",
    "    # Training\n",
    "    # -------------------\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for imgs, labels, label_lens in tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg['epochs']} (Train)\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(imgs)\n",
    "            batch_size = imgs.size(0)\n",
    "            pred_lengths = torch.full((batch_size,), preds.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "            max_label_len = label_lens.max().item()\n",
    "            if preds.size(0) < max_label_len:\n",
    "                pad = torch.zeros(max_label_len - preds.size(0), preds.size(1), preds.size(2), device=device)\n",
    "                preds = torch.cat([preds, pad], dim=0)\n",
    "\n",
    "            loss = model.compute_ctc_loss(preds, labels, pred_lengths, label_lens)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # -------------------\n",
    "    # Validation\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lens in tqdm(val_loader, desc=f\"Epoch {epoch}/{cfg['epochs']} (Val)\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            preds = model(imgs)\n",
    "            batch_size = imgs.size(0)\n",
    "            pred_lengths = torch.full((batch_size,), preds.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "            max_label_len = label_lens.max().item()\n",
    "            if preds.size(0) < max_label_len:\n",
    "                pad = torch.zeros(max_label_len - preds.size(0), preds.size(1), preds.size(2), device=device)\n",
    "                preds = torch.cat([preds, pad], dim=0)\n",
    "\n",
    "            loss = model.compute_ctc_loss(preds, labels, pred_lengths, label_lens)\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                continue\n",
    "            val_running_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_running_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{cfg['epochs']}]  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Save sample prediction (optional)\n",
    "    sample_img, _, _ = next(iter(val_loader))\n",
    "    sample_img = sample_img.to(device)\n",
    "    preds = model(sample_img)\n",
    "    decoded_texts = greedy_ctc_decode(preds, train_dataset.charset)\n",
    "    save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_sample.png\")\n",
    "    save_image(sample_img[0], save_path)\n",
    "    print(f\"Sample prediction saved: {save_path}  Predicted text: {decoded_texts[0]}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoint_dir, f\"crnn_epoch_{epoch}.pth\"))\n",
    "    scheduler.step()\n",
    "\n",
    "# -------------------\n",
    "# Plot loss curves\n",
    "# -------------------\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"CTC Loss\")\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df7aee42-70f0-4d6c-a878-9201ec574120",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# ====== IMPORT YOUR MODEL & DATASTRUCTURES ======\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_code\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OCRModelResNet18     \u001b[38;5;66;03m# <-- your model file\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                                               \u001b[38;5;66;03m# If model class is inside the same file, remove this line and paste the class definition here.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# ====== Load Charset ======\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_charset\u001b[39m(path):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_code'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "# ====== IMPORT YOUR MODEL & DATASTRUCTURES ======\n",
    "from model_code import OCRModelResNet18     # <-- your model file\n",
    "                                              # If model class is inside the same file, remove this line and paste the class definition here.\n",
    "\n",
    "\n",
    "# ====== Load Charset ======\n",
    "def load_charset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        charset = [\"blank\"] + list(f.read().strip())\n",
    "    return charset\n",
    "\n",
    "\n",
    "# ====== Image Preprocessing (same as training) ======\n",
    "def preprocess_image(img_path, img_h, img_w):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    w, h = img.size\n",
    "\n",
    "    new_h = img_h\n",
    "    new_w = int(w * (new_h / h))\n",
    "    img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    if new_w < img_w:\n",
    "        padded = Image.new(\"L\", (img_w, new_h), 255)\n",
    "        padded.paste(img, (0, 0))\n",
    "        img = padded\n",
    "    else:\n",
    "        img = img.crop((0, 0, img_w, new_h))\n",
    "\n",
    "    img = np.array(img)\n",
    "    img = torch.from_numpy(img).float().unsqueeze(0).unsqueeze(0) / 255.0\n",
    "    return img\n",
    "\n",
    "\n",
    "# ====== Greedy CTC Decode with Confidence ======\n",
    "def greedy_decode_with_conf(preds, charset):\n",
    "    \"\"\"\n",
    "    preds: [T, B, C]\n",
    "    returns: text, confidence\n",
    "    \"\"\"\n",
    "    preds_softmax = preds.softmax(2)              # [T,B,C]\n",
    "    pred_idx = preds_softmax.argmax(2).cpu()      # [T,B]\n",
    "\n",
    "    batch_texts = []\n",
    "    batch_confs = []\n",
    "\n",
    "    for b in range(pred_idx.shape[1]):\n",
    "        last = 0\n",
    "        chars, probs = [], []\n",
    "\n",
    "        for t in range(pred_idx.shape[0]):\n",
    "            idx = pred_idx[t, b].item()\n",
    "            prob = preds_softmax[t, b, idx].item()\n",
    "\n",
    "            if idx != 0 and idx != last:   # skip blank + repeats\n",
    "                chars.append(charset[idx])\n",
    "                probs.append(prob)\n",
    "            last = idx\n",
    "\n",
    "        text = \"\".join(chars)\n",
    "        conf = float(np.mean(probs)) if len(probs) > 0 else 0.0\n",
    "\n",
    "        batch_texts.append(text)\n",
    "        batch_confs.append(conf)\n",
    "\n",
    "    return batch_texts, batch_confs\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "#                   INFERENCE MAIN\n",
    "# =======================================================\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load config\n",
    "    with open(\"config.yaml\", \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load charset\n",
    "    charset = load_charset(cfg[\"charset_path\"])\n",
    "\n",
    "    # Build model\n",
    "    model = OCRModelResNet18(\n",
    "        num_classes=cfg[\"num_classes\"],\n",
    "        img_channels=cfg[\"num_channels\"],\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint_path = \"checkpoints/crnn_epoch_XX.pth\"   # <-- REPLACE with last epoch file\n",
    "    print(f\"Loading checkpoint: {checkpoint_path}\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_dir = \"data/ocr_dataset/test\"\n",
    "    test_images = sorted([f for f in os.listdir(test_dir) if f.endswith(\".png\")])\n",
    "\n",
    "    print(f\"Found {len(test_images)} test images.\")\n",
    "\n",
    "    for img_name in tqdm(test_images):\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "\n",
    "        img_tensor = preprocess_image(img_path, cfg[\"img_height\"], cfg[\"img_width\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(img_tensor)   # [T, B=1, C]\n",
    "\n",
    "        # Decode\n",
    "        decoded, conf = greedy_decode_with_conf(preds, charset)\n",
    "\n",
    "        print(f\"\\nImage: {img_name}\")\n",
    "        print(f\"Prediction: {decoded[0]}\")\n",
    "        print(f\"Confidence: {conf[0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f551b069-e7fe-418f-b9c4-b381a3d291c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/nepali_crnn_ctc.pth\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for OCRModelResNet18:\n\tMissing key(s) in state_dict: \"cnn.conv1.weight\", \"cnn.bn1.weight\", \"cnn.bn1.bias\", \"cnn.bn1.running_mean\", \"cnn.bn1.running_var\", \"cnn.layer1.0.conv1.weight\", \"cnn.layer1.0.bn1.weight\", \"cnn.layer1.0.bn1.bias\", \"cnn.layer1.0.bn1.running_mean\", \"cnn.layer1.0.bn1.running_var\", \"cnn.layer1.0.conv2.weight\", \"cnn.layer1.0.bn2.weight\", \"cnn.layer1.0.bn2.bias\", \"cnn.layer1.0.bn2.running_mean\", \"cnn.layer1.0.bn2.running_var\", \"cnn.layer1.1.conv1.weight\", \"cnn.layer1.1.bn1.weight\", \"cnn.layer1.1.bn1.bias\", \"cnn.layer1.1.bn1.running_mean\", \"cnn.layer1.1.bn1.running_var\", \"cnn.layer1.1.conv2.weight\", \"cnn.layer1.1.bn2.weight\", \"cnn.layer1.1.bn2.bias\", \"cnn.layer1.1.bn2.running_mean\", \"cnn.layer1.1.bn2.running_var\", \"cnn.layer2.0.conv1.weight\", \"cnn.layer2.0.bn1.weight\", \"cnn.layer2.0.bn1.bias\", \"cnn.layer2.0.bn1.running_mean\", \"cnn.layer2.0.bn1.running_var\", \"cnn.layer2.0.conv2.weight\", \"cnn.layer2.0.bn2.weight\", \"cnn.layer2.0.bn2.bias\", \"cnn.layer2.0.bn2.running_mean\", \"cnn.layer2.0.bn2.running_var\", \"cnn.layer2.0.downsample.0.weight\", \"cnn.layer2.0.downsample.1.weight\", \"cnn.layer2.0.downsample.1.bias\", \"cnn.layer2.0.downsample.1.running_mean\", \"cnn.layer2.0.downsample.1.running_var\", \"cnn.layer2.1.conv1.weight\", \"cnn.layer2.1.bn1.weight\", \"cnn.layer2.1.bn1.bias\", \"cnn.layer2.1.bn1.running_mean\", \"cnn.layer2.1.bn1.running_var\", \"cnn.layer2.1.conv2.weight\", \"cnn.layer2.1.bn2.weight\", \"cnn.layer2.1.bn2.bias\", \"cnn.layer2.1.bn2.running_mean\", \"cnn.layer2.1.bn2.running_var\", \"cnn.layer3.0.conv1.weight\", \"cnn.layer3.0.bn1.weight\", \"cnn.layer3.0.bn1.bias\", \"cnn.layer3.0.bn1.running_mean\", \"cnn.layer3.0.bn1.running_var\", \"cnn.layer3.0.conv2.weight\", \"cnn.layer3.0.bn2.weight\", \"cnn.layer3.0.bn2.bias\", \"cnn.layer3.0.bn2.running_mean\", \"cnn.layer3.0.bn2.running_var\", \"cnn.layer3.0.downsample.0.weight\", \"cnn.layer3.0.downsample.1.weight\", \"cnn.layer3.0.downsample.1.bias\", \"cnn.layer3.0.downsample.1.running_mean\", \"cnn.layer3.0.downsample.1.running_var\", \"cnn.layer3.1.conv1.weight\", \"cnn.layer3.1.bn1.weight\", \"cnn.layer3.1.bn1.bias\", \"cnn.layer3.1.bn1.running_mean\", \"cnn.layer3.1.bn1.running_var\", \"cnn.layer3.1.conv2.weight\", \"cnn.layer3.1.bn2.weight\", \"cnn.layer3.1.bn2.bias\", \"cnn.layer3.1.bn2.running_mean\", \"cnn.layer3.1.bn2.running_var\", \"cnn.layer4.0.conv1.weight\", \"cnn.layer4.0.bn1.weight\", \"cnn.layer4.0.bn1.bias\", \"cnn.layer4.0.bn1.running_mean\", \"cnn.layer4.0.bn1.running_var\", \"cnn.layer4.0.conv2.weight\", \"cnn.layer4.0.bn2.weight\", \"cnn.layer4.0.bn2.bias\", \"cnn.layer4.0.bn2.running_mean\", \"cnn.layer4.0.bn2.running_var\", \"cnn.layer4.0.downsample.0.weight\", \"cnn.layer4.0.downsample.1.weight\", \"cnn.layer4.0.downsample.1.bias\", \"cnn.layer4.0.downsample.1.running_mean\", \"cnn.layer4.0.downsample.1.running_var\", \"cnn.layer4.1.conv1.weight\", \"cnn.layer4.1.bn1.weight\", \"cnn.layer4.1.bn1.bias\", \"cnn.layer4.1.bn1.running_mean\", \"cnn.layer4.1.bn1.running_var\", \"cnn.layer4.1.conv2.weight\", \"cnn.layer4.1.bn2.weight\", \"cnn.layer4.1.bn2.bias\", \"cnn.layer4.1.bn2.running_mean\", \"cnn.layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"cnn.cnn.0.weight\", \"cnn.cnn.0.bias\", \"cnn.cnn.3.weight\", \"cnn.cnn.3.bias\", \"cnn.cnn.6.weight\", \"cnn.cnn.6.bias\", \"cnn.cnn.8.weight\", \"cnn.cnn.8.bias\", \"cnn.cnn.8.running_mean\", \"cnn.cnn.8.running_var\", \"cnn.cnn.8.num_batches_tracked\", \"cnn.cnn.9.weight\", \"cnn.cnn.9.bias\", \"cnn.cnn.12.weight\", \"cnn.cnn.12.bias\", \"cnn.cnn.14.weight\", \"cnn.cnn.14.bias\", \"cnn.cnn.14.running_mean\", \"cnn.cnn.14.running_var\", \"cnn.cnn.14.num_batches_tracked\", \"cnn.cnn.15.weight\", \"cnn.cnn.15.bias\", \"cnn.cnn.18.weight\", \"cnn.cnn.18.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 155\u001b[0m\n\u001b[1;32m    153\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ckpt_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(ckpt_dir) \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m, ckpt)\n\u001b[0;32m--> 155\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Load first 15 test images\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/nn/modules/module.py:2624\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2616\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2617\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2618\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2619\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2620\u001b[0m             ),\n\u001b[1;32m   2621\u001b[0m         )\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2624\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2625\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2626\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2627\u001b[0m         )\n\u001b[1;32m   2628\u001b[0m     )\n\u001b[1;32m   2629\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for OCRModelResNet18:\n\tMissing key(s) in state_dict: \"cnn.conv1.weight\", \"cnn.bn1.weight\", \"cnn.bn1.bias\", \"cnn.bn1.running_mean\", \"cnn.bn1.running_var\", \"cnn.layer1.0.conv1.weight\", \"cnn.layer1.0.bn1.weight\", \"cnn.layer1.0.bn1.bias\", \"cnn.layer1.0.bn1.running_mean\", \"cnn.layer1.0.bn1.running_var\", \"cnn.layer1.0.conv2.weight\", \"cnn.layer1.0.bn2.weight\", \"cnn.layer1.0.bn2.bias\", \"cnn.layer1.0.bn2.running_mean\", \"cnn.layer1.0.bn2.running_var\", \"cnn.layer1.1.conv1.weight\", \"cnn.layer1.1.bn1.weight\", \"cnn.layer1.1.bn1.bias\", \"cnn.layer1.1.bn1.running_mean\", \"cnn.layer1.1.bn1.running_var\", \"cnn.layer1.1.conv2.weight\", \"cnn.layer1.1.bn2.weight\", \"cnn.layer1.1.bn2.bias\", \"cnn.layer1.1.bn2.running_mean\", \"cnn.layer1.1.bn2.running_var\", \"cnn.layer2.0.conv1.weight\", \"cnn.layer2.0.bn1.weight\", \"cnn.layer2.0.bn1.bias\", \"cnn.layer2.0.bn1.running_mean\", \"cnn.layer2.0.bn1.running_var\", \"cnn.layer2.0.conv2.weight\", \"cnn.layer2.0.bn2.weight\", \"cnn.layer2.0.bn2.bias\", \"cnn.layer2.0.bn2.running_mean\", \"cnn.layer2.0.bn2.running_var\", \"cnn.layer2.0.downsample.0.weight\", \"cnn.layer2.0.downsample.1.weight\", \"cnn.layer2.0.downsample.1.bias\", \"cnn.layer2.0.downsample.1.running_mean\", \"cnn.layer2.0.downsample.1.running_var\", \"cnn.layer2.1.conv1.weight\", \"cnn.layer2.1.bn1.weight\", \"cnn.layer2.1.bn1.bias\", \"cnn.layer2.1.bn1.running_mean\", \"cnn.layer2.1.bn1.running_var\", \"cnn.layer2.1.conv2.weight\", \"cnn.layer2.1.bn2.weight\", \"cnn.layer2.1.bn2.bias\", \"cnn.layer2.1.bn2.running_mean\", \"cnn.layer2.1.bn2.running_var\", \"cnn.layer3.0.conv1.weight\", \"cnn.layer3.0.bn1.weight\", \"cnn.layer3.0.bn1.bias\", \"cnn.layer3.0.bn1.running_mean\", \"cnn.layer3.0.bn1.running_var\", \"cnn.layer3.0.conv2.weight\", \"cnn.layer3.0.bn2.weight\", \"cnn.layer3.0.bn2.bias\", \"cnn.layer3.0.bn2.running_mean\", \"cnn.layer3.0.bn2.running_var\", \"cnn.layer3.0.downsample.0.weight\", \"cnn.layer3.0.downsample.1.weight\", \"cnn.layer3.0.downsample.1.bias\", \"cnn.layer3.0.downsample.1.running_mean\", \"cnn.layer3.0.downsample.1.running_var\", \"cnn.layer3.1.conv1.weight\", \"cnn.layer3.1.bn1.weight\", \"cnn.layer3.1.bn1.bias\", \"cnn.layer3.1.bn1.running_mean\", \"cnn.layer3.1.bn1.running_var\", \"cnn.layer3.1.conv2.weight\", \"cnn.layer3.1.bn2.weight\", \"cnn.layer3.1.bn2.bias\", \"cnn.layer3.1.bn2.running_mean\", \"cnn.layer3.1.bn2.running_var\", \"cnn.layer4.0.conv1.weight\", \"cnn.layer4.0.bn1.weight\", \"cnn.layer4.0.bn1.bias\", \"cnn.layer4.0.bn1.running_mean\", \"cnn.layer4.0.bn1.running_var\", \"cnn.layer4.0.conv2.weight\", \"cnn.layer4.0.bn2.weight\", \"cnn.layer4.0.bn2.bias\", \"cnn.layer4.0.bn2.running_mean\", \"cnn.layer4.0.bn2.running_var\", \"cnn.layer4.0.downsample.0.weight\", \"cnn.layer4.0.downsample.1.weight\", \"cnn.layer4.0.downsample.1.bias\", \"cnn.layer4.0.downsample.1.running_mean\", \"cnn.layer4.0.downsample.1.running_var\", \"cnn.layer4.1.conv1.weight\", \"cnn.layer4.1.bn1.weight\", \"cnn.layer4.1.bn1.bias\", \"cnn.layer4.1.bn1.running_mean\", \"cnn.layer4.1.bn1.running_var\", \"cnn.layer4.1.conv2.weight\", \"cnn.layer4.1.bn2.weight\", \"cnn.layer4.1.bn2.bias\", \"cnn.layer4.1.bn2.running_mean\", \"cnn.layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"cnn.cnn.0.weight\", \"cnn.cnn.0.bias\", \"cnn.cnn.3.weight\", \"cnn.cnn.3.bias\", \"cnn.cnn.6.weight\", \"cnn.cnn.6.bias\", \"cnn.cnn.8.weight\", \"cnn.cnn.8.bias\", \"cnn.cnn.8.running_mean\", \"cnn.cnn.8.running_var\", \"cnn.cnn.8.num_batches_tracked\", \"cnn.cnn.9.weight\", \"cnn.cnn.9.bias\", \"cnn.cnn.12.weight\", \"cnn.cnn.12.bias\", \"cnn.cnn.14.weight\", \"cnn.cnn.14.bias\", \"cnn.cnn.14.running_mean\", \"cnn.cnn.14.running_var\", \"cnn.cnn.14.num_batches_tracked\", \"cnn.cnn.15.weight\", \"cnn.cnn.15.bias\", \"cnn.cnn.18.weight\", \"cnn.cnn.18.bias\". "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# ============================================================\n",
    "#      MODEL DEFINITIONS (copy from your training file)\n",
    "# ============================================================\n",
    "\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "\n",
    "        if img_channels != 3:\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, 7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.permute(2, 0, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def preprocess_image(img_path, img_h, img_w):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    w, h = img.size\n",
    "\n",
    "    new_h = img_h\n",
    "    new_w = int(w * (new_h / h))\n",
    "    img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    if new_w < img_w:\n",
    "        padded = Image.new(\"L\", (img_w, new_h), 255)\n",
    "        padded.paste(img, (0, 0))\n",
    "        img = padded\n",
    "    else:\n",
    "        img = img.crop((0, 0, img_w, new_h))\n",
    "\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_charset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        charset = [\"blank\"] + list(f.read().strip())\n",
    "    return charset\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#          GREEDY DECODER WITH CONFIDENCE SCORE\n",
    "# ============================================================\n",
    "\n",
    "def greedy_decode_with_conf(preds, charset):\n",
    "    \"\"\"\n",
    "    preds: [T, B, C]\n",
    "    Returns: text, confidence\n",
    "    \"\"\"\n",
    "    sm = preds.softmax(2)\n",
    "    idxs = sm.argmax(2).cpu().numpy()\n",
    "\n",
    "    batch_texts = []\n",
    "    batch_confs = []\n",
    "\n",
    "    for b in range(idxs.shape[1]):\n",
    "        last = 0\n",
    "        chars = []\n",
    "        probs = []\n",
    "\n",
    "        for t in range(idxs.shape[0]):\n",
    "            idx = idxs[t, b]\n",
    "            prob = sm[t, b, idx].item()\n",
    "\n",
    "            if idx != 0 and idx != last:  # skip blank + repeat\n",
    "                chars.append(charset[idx])\n",
    "                probs.append(prob)\n",
    "\n",
    "            last = idx\n",
    "\n",
    "        text = \"\".join(chars)\n",
    "        conf = float(np.mean(probs)) if probs else 0.0\n",
    "\n",
    "        batch_texts.append(text)\n",
    "        batch_confs.append(conf)\n",
    "\n",
    "    return batch_texts, batch_confs\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#                        MAIN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load config\n",
    "    with open(\"config.yaml\", \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    charset = load_charset(cfg[\"charset_path\"])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load model\n",
    "    model = OCRModelResNet18(\n",
    "        num_classes=cfg[\"num_classes\"],\n",
    "        img_channels=cfg[\"num_channels\"],\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "\n",
    "    ckpt = \"checkpoints/crnn_epoch_66.pth\"   # <<< change this\n",
    "    print(\"Loading:\", ckpt)\n",
    "    model.load_state_dict(torch.load(ckpt, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_dir = \"data/ocr_dataset/test\"\n",
    "    test_files = sorted([f for f in os.listdir(test_dir) if f.endswith(\".png\")])\n",
    "\n",
    "    for name in tqdm(test_files):\n",
    "        img_path = os.path.join(test_dir, name)\n",
    "        img = preprocess_image(img_path, cfg[\"img_height\"], cfg[\"img_width\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(img)\n",
    "\n",
    "        text, conf = greedy_decode_with_conf(preds, charset)\n",
    "\n",
    "        print(f\"\\nImage: {name}\")\n",
    "        print(\"Prediction:\", text[0])\n",
    "        print(\"Confidence:\", round(conf[0], 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "065d46f5-8b5d-4b81-8936-8bc6a4863ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00312.png', '00390.txt', '00228.txt', '00104.txt', '00209.txt', '00455.png', '00471.png', '00102.png', '00022.png', '00024.png', '00096.txt', '00403.txt', '00441.txt', '00288.png', '00399.png', '00309.txt', '00424.png', '00094.txt', '00431.txt', '00138.txt', '00373.txt', '00456.png', '00267.png', '00109.txt', '00133.txt', '00309.png', '00448.png', '00063.txt', '00213.txt', '00397.png', '00315.txt', '00442.png', '00197.png', '00438.txt', '00366.png', '00233.txt', '00043.png', '00263.png', '00205.txt', '00402.png', '00299.png', '00113.png', '00093.png', '00235.txt', '00272.txt', '00179.txt', '00095.txt', '00262.png', '00306.txt', '00433.png', '00059.png', '00352.txt', '00379.png', '00081.png', '00087.txt', '00165.png', '00297.txt', '00290.txt', '00258.txt', '00219.txt', '00303.txt', '00499.png', '00012.png', '00430.png', '00447.png', '00239.txt', '00370.txt', '00394.txt', '00098.png', '00242.png', '00199.png', '00344.png', '00140.txt', '00044.png', '00027.png', '00361.png', '00069.txt', '00006.png', '00476.png', '00358.txt', '00170.txt', '00491.txt', '00494.png', '00060.png', '00302.png', '00480.png', '00129.png', '00437.txt', '00264.txt', '00125.png', '00217.png', '00033.txt', '00414.png', '00471.txt', '00451.txt', '00048.png', '00152.txt', '00169.txt', '00463.txt', '00320.txt', '00321.txt', '00068.png', '00346.png', '00425.txt', '00377.png', '00348.txt', '00284.txt', '00337.txt', '00320.png', '00477.png', '00175.png', '00388.png', '00396.png', '00116.txt', '00023.txt', '00159.txt', '00134.png', '00190.txt', '00337.png', '00068.txt', '00342.txt', '00365.png', '00257.txt', '00297.png', '00479.png', '00292.png', '00192.png', '00117.txt', '00475.txt', '00493.txt', '00049.png', '00408.txt', '00076.png', '00489.txt', '00261.txt', '00276.png', '00212.png', '00282.png', '00056.txt', '00174.png', '00298.txt', '00282.txt', '00395.txt', '00429.png', '00017.txt', '00278.png', '00185.png', '00296.txt', '00256.png', '00294.txt', '00415.png', '00404.png', '00362.txt', '00040.png', '00452.png', '00302.txt', '00500.txt', '00050.png', '00310.png', '00025.txt', '00010.png', '00012.txt', '00430.txt', '00380.png', '00412.png', '00212.txt', '00235.png', '00349.txt', '00323.txt', '00350.png', '00136.png', '00382.png', '00046.txt', '00334.txt', '00267.txt', '00305.png', '00130.png', '00332.txt', '00034.txt', '00063.png', '00141.png', '00327.png', '00367.txt', '00146.png', '00131.txt', '00021.png', '00262.txt', '00429.txt', '00096.png', '00403.png', '00189.txt', '00344.txt', '00464.png', '00195.png', '00449.txt', '00150.txt', '00232.png', '00028.png', '00017.png', '00464.txt', '00193.txt', '00364.png', '00202.png', '00271.txt', '00277.txt', '00497.png', '00400.png', '00211.txt', '00301.png', '00498.png', '00317.png', '00176.png', '00248.png', '00205.png', '00390.png', '00296.png', '00183.png', '00409.png', '00010.txt', '00250.txt', '00405.png', '00221.png', '00421.txt', '00362.png', '00057.txt', '00131.png', '00492.png', '00109.png', '00357.txt', '00423.txt', '00100.txt', '00374.txt', '00163.png', '00481.png', '00223.txt', '00173.txt', '00074.png', '00163.txt', '00346.txt', '00376.png', '00486.png', '00108.png', '00052.txt', '00412.txt', '00323.png', '00313.txt', '00207.txt', '00240.png', '00351.png', '00194.txt', '00359.png', '00011.png', '00360.png', '00500.png', '00160.png', '00273.txt', '00177.txt', '00336.txt', '00363.png', '00392.png', '00411.txt', '00498.txt', '00115.txt', '00142.txt', '00064.txt', '00005.png', '00462.txt', '00401.txt', '00086.txt', '00420.png', '00485.txt', '00184.png', '00419.png', '00333.txt', '00153.png', '00036.txt', '00134.txt', '00207.png', '00126.txt', '00246.png', '00378.txt', '00486.txt', '00046.png', '00083.txt', '00225.txt', '00175.txt', '00325.txt', '00384.txt', '00159.png', '00343.png', '00051.txt', '00055.txt', '00054.png', '00473.txt', '00074.txt', '00376.txt', '00254.png', '00311.txt', '00094.png', '00127.txt', '00014.png', '00171.png', '00106.png', '00087.png', '00180.txt', '00285.png', '00492.txt', '00107.txt', '00101.png', '00124.png', '00240.txt', '00092.png', '00025.png', '00141.txt', '00360.txt', '00285.txt', '00372.txt', '00336.png', '00091.png', '00072.png', '00088.png', '00180.png', '00196.png', '00459.txt', '00353.png', '00162.txt', '00277.png', '00283.png', '00106.txt', '00469.txt', '00304.png', '00419.txt', '00364.txt', '00052.png', '00182.png', '00071.txt', '00455.txt', '00413.txt', '00104.png', '00478.png', '00339.png', '00145.txt', '00139.txt', '00385.txt', '00120.txt', '00437.png', '00307.png', '00420.txt', '00382.txt', '00073.png', '00307.txt', '00331.txt', '00151.txt', '00009.txt', '00497.txt', '00287.png', '00463.png', '00333.png', '00465.png', '00233.png', '00350.txt', '00099.png', '00335.txt', '00291.png', '00258.png', '00148.png', '00148.txt', '00203.txt', '00125.txt', '00266.txt', '00078.png', '00394.png', '00171.txt', '00118.txt', '00158.txt', '00021.txt', '00139.png', '00343.txt', '00368.txt', '00432.txt', '00253.png', '00314.txt', '00384.png', '00138.png', '00431.png', '00214.png', '00056.png', '00338.png', '00454.png', '00069.png', '00067.txt', '00236.txt', '00440.png', '00195.txt', '00119.txt', '00206.txt', '00366.txt', '00177.png', '00263.txt', '00379.txt', '00060.txt', '00295.png', '00468.png', '00298.png', '00442.txt', '00169.png', '00024.txt', '00172.png', '00268.png', '00427.png', '00354.txt', '00435.txt', '00234.png', '00321.png', '00197.txt', '00019.png', '00066.txt', '00319.txt', '00279.png', '00015.txt', '00033.png', '00300.png', '00032.txt', '00389.png', '00123.png', '00482.txt', '00200.png', '00016.png', '00062.txt', '00122.png', '00385.png', '00252.txt', '00449.png', '00300.txt', '00086.png', '00410.png', '00440.txt', '00416.png', '00006.txt', '00112.png', '00494.txt', '00014.txt', '00444.png', '00238.txt', '00453.png', '00293.txt', '00391.txt', '00303.png', '00470.png', '00221.txt', '00165.txt', '00146.txt', '00439.txt', '00286.png', '00156.png', '00227.txt', '00108.txt', '00341.png', '00269.png', '00259.txt', '00371.txt', '00491.png', '00275.png', '00363.txt', '00275.txt', '00186.txt', '00295.txt', '00457.png', '00372.png', '00135.txt', '00037.txt', '00075.png', '00439.png', '00111.txt', '00361.txt', '00387.png', '00003.png', '00438.png', '00083.png', '00398.png', '00103.png', '00310.txt', '00354.png', '00142.png', '00480.txt', '00418.txt', '00224.png', '00454.txt', '00400.txt', '00149.png', '00204.txt', '00355.txt', '00035.txt', '00388.txt', '00178.txt', '00081.txt', '00373.png', '00244.txt', '00196.txt', '00064.png', '00487.txt', '00469.png', '00229.png', '00356.png', '00422.txt', '00201.txt', '00035.png', '00434.png', '00220.txt', '00495.png', '00220.png', '00121.txt', '00044.txt', '00237.txt', '00117.png', '00079.txt', '00123.txt', '00051.png', '00475.png', '00102.txt', '00050.txt', '00316.txt', '00324.txt', '00238.png', '00140.png', '00198.txt', '00381.png', '00082.txt', '00450.png', '00284.png', '00418.png', '00265.txt', '00406.png', '00406.txt', '00465.txt', '00493.png', '00347.png', '00075.txt', '00099.txt', '00357.png', '00375.txt', '00325.png', '00253.txt', '00040.txt', '00167.png', '00135.png', '00260.txt', '00432.png', '00091.txt', '00468.txt', '00355.png', '00374.png', '00255.txt', '00026.png', '00133.png', '00009.png', '00398.txt', '00231.txt', '00030.txt', '00110.txt', '00237.png', '00168.png', '00164.txt', '00365.txt', '00198.png', '00210.png', '00080.txt', '00250.png', '00203.png', '00045.png', '00188.png', '00435.png', '00137.txt', '00254.txt', '00179.png', '00020.txt', '00433.txt', '00048.txt', '00001.txt', '00062.png', '00415.txt', '00230.txt', '00461.txt', '00157.png', '00214.txt', '00226.png', '00011.txt', '00243.png', '00036.png', '00232.txt', '00030.png', '00257.png', '00194.png', '00487.png', '00472.txt', '00414.txt', '00095.png', '00329.txt', '00481.txt', '00436.txt', '00213.png', '00145.png', '00279.txt', '00187.txt', '00129.txt', '00193.png', '00314.png', '00059.txt', '00348.png', '00341.txt', '00466.txt', '00472.png', '00201.png', '00280.txt', '00172.txt', '00222.png', '00483.png', '00335.png', '00445.txt', '00446.txt', '00019.txt', '00244.png', '00122.txt', '00367.png', '00246.txt', '00293.png', '00039.txt', '00316.png', '00005.txt', '00283.txt', '00351.txt', '00375.png', '00304.txt', '00161.txt', '00002.png', '00265.png', '00218.png', '00210.txt', '00308.png', '00482.png', '00215.png', '00132.png', '00098.txt', '00422.png', '00499.txt', '00088.txt', '00004.txt', '00393.png', '00286.txt', '00391.png', '00281.png', '00058.txt', '00264.png', '00039.png', '00452.txt', '00234.txt', '00340.txt', '00222.txt', '00218.txt', '00149.txt', '00358.png', '00216.txt', '00322.txt', '00153.txt', '00097.txt', '00084.txt', '00301.txt', '00426.png', '00047.txt', '00312.txt', '00457.txt', '00038.png', '00407.png', '00413.png', '00101.txt', '00085.txt', '00401.png', '00426.txt', '00154.png', '00167.txt', '00397.txt', '00274.txt', '00459.png', '00038.txt', '00347.txt', '00276.txt', '00112.txt', '00428.txt', '00255.png', '00190.png', '00417.txt', '00268.txt', '00484.png', '00287.txt', '00359.txt', '00168.txt', '00324.png', '00290.png', '00016.txt', '00152.png', '00289.png', '00349.png', '00256.txt', '00120.png', '00479.txt', '00273.png', '00417.png', '00184.txt', '00383.png', '00271.png', '00078.txt', '00460.txt', '00370.png', '00386.png', '00484.txt', '00352.png', '00114.txt', '00338.txt', '00423.png', '00027.txt', '00158.png', '00204.png', '00186.png', '00299.txt', '00345.txt', '00322.png', '00187.png', '00383.txt', '00178.png', '00029.png', '00311.png', '00368.png', '00409.txt', '00266.png', '00199.txt', '00326.txt', '00072.txt', '00110.png', '00015.png', '00447.txt', '00053.png', '00490.txt', '00215.txt', '00023.png', '00013.txt', '00162.png', '00115.png', '00061.txt', '00330.txt', '00161.png', '00170.png', '00041.txt', '00425.png', '00007.png', '00103.txt', '00356.txt', '00085.png', '00065.txt', '00031.png', '00008.png', '00477.txt', '00245.txt', '00424.txt', '00318.txt', '00049.txt', '00166.txt', '00489.png', '00247.txt', '00496.png', '00026.txt', '00461.png', '00243.txt', '00136.txt', '00274.png', '00399.txt', '00270.txt', '00126.png', '00124.txt', '00411.png', '00416.txt', '00470.txt', '00393.txt', '00070.png', '00280.png', '00113.txt', '00118.png', '00236.png', '00183.txt', '00181.txt', '00097.png', '00073.txt', '00121.png', '00188.txt', '00092.txt', '00259.png', '00380.txt', '00070.txt', '00467.png', '00054.txt', '.ipynb_checkpoints', '00241.png', '00319.png', '00318.png', '00288.txt', '00082.png', '00377.txt', '00226.txt', '00407.txt', '00157.txt', '00387.txt', '00143.png', '00241.txt', '00200.txt', '00061.png', '00034.png', '00071.png', '00164.png', '00269.txt', '00405.txt', '00229.txt', '00018.txt', '00008.txt', '00450.txt', '00445.png', '00079.png', '00421.png', '00378.png', '00206.png', '00328.png', '00055.png', '00022.txt', '00029.txt', '00208.txt', '00105.txt', '00458.txt', '00331.png', '00219.png', '00224.txt', '00067.png', '00270.png', '00028.txt', '00090.png', '00272.png', '00327.txt', '00434.txt', '00230.png', '00436.png', '00315.png', '00066.png', '00043.txt', '00132.txt', '00002.txt', '00427.txt', '00192.txt', '00119.png', '00245.png', '00041.png', '00402.txt', '00137.png', '00045.txt', '00058.png', '00458.png', '00032.png', '00396.txt', '00076.txt', '00182.txt', '00441.png', '00239.png', '00191.txt', '00252.png', '00353.txt', '00381.txt', '00490.png', '00242.txt', '00339.txt', '00448.txt', '00127.png', '00483.txt', '00173.png', '00488.png', '00249.png', '00004.png', '00185.txt', '00003.txt', '00225.png', '00144.png', '00485.png', '00408.png', '00317.txt', '00476.txt', '00111.png', '00084.png', '00261.png', '00227.png', '00208.png', '00077.png', '00291.txt', '00345.png', '00077.txt', '00020.png', '00007.txt', '00443.png', '00147.txt', '00155.txt', '00114.png', '00389.txt', '00342.png', '00181.png', '00247.png', '00306.png', '00462.png', '00018.png', '00089.png', '00057.png', '00392.txt', '00100.png', '00334.png', '00031.txt', '00166.png', '00260.png', '00001.png', '00065.png', '00053.txt', '00202.txt', '00328.txt', '00473.png', '00453.txt', '00456.txt', '00090.txt', '00155.png', '00128.txt', '00174.txt', '00451.png', '00340.png', '00150.png', '00144.txt', '00369.png', '00160.txt', '00249.txt', '00143.txt', '00089.txt', '00428.png', '00042.png', '00231.png', '00495.txt', '00460.png', '00308.txt', '00080.png', '00467.txt', '00189.png', '00047.png', '00130.txt', '00248.txt', '00446.png', '00251.txt', '00211.png', '00209.png', '00105.png', '00223.png', '00496.txt', '00305.txt', '00116.png', '00217.txt', '00443.txt', '00444.txt', '00151.png', '00466.png', '00326.png', '00147.png', '00107.png', '00281.txt', '00278.txt', '00042.txt', '00156.txt', '00369.txt', '00404.txt', '00013.png', '00251.png', '00289.txt', '00332.png', '00395.png', '00488.txt', '00292.txt', '00330.png', '00474.png', '00474.txt', '00478.txt', '00410.txt', '00037.png', '00329.png', '00313.png', '00371.png', '00216.png', '00176.txt', '00154.txt', '00128.png', '00093.txt', '00294.png', '00228.png', '00386.txt', '00191.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"data/ocr_dataset/test\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fbad4b3-25a5-4213-a7d9-40f775471100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint: checkpoints/crnn_epoch_66.pth\n",
      "\n",
      "========== 15 RANDOM TEST IMAGES ==========\n",
      "\n",
      "Image: 00215.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00087.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00311.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00388.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00156.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00155.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00073.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00014.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00190.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00089.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00460.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00012.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00420.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00103.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n",
      "Image: 00020.png\n",
      "Ground Truth : \n",
      "Prediction   : \n",
      "Confidence   : 0.0\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import yaml\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# ============================================================\n",
    "# MODEL DEFINITIONS (must match training exactly)\n",
    "# ============================================================\n",
    "\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "\n",
    "        if img_channels != 3:\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, 7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.squeeze(2)\n",
    "        x = x.permute(2, 0, 1)  # [seq_len, batch, channels]\n",
    "        return x\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "# ============================================================\n",
    "# PREPROCESSING\n",
    "# ============================================================\n",
    "\n",
    "def preprocess_image(img_path, img_h, img_w):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    w, h = img.size\n",
    "    new_h = img_h\n",
    "    new_w = int(w * (new_h / h))\n",
    "    img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    if new_w < img_w:\n",
    "        padded = Image.new(\"L\", (img_w, new_h), 255)\n",
    "        padded.paste(img, (0, 0))\n",
    "        img = padded\n",
    "    else:\n",
    "        img = img.crop((0, 0, img_w, new_h))\n",
    "\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    img = torch.from_numpy(img).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "    return img\n",
    "\n",
    "def load_charset(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [\"blank\"] + list(f.read().strip())\n",
    "\n",
    "# ============================================================\n",
    "# GREEDY DECODER WITH CONFIDENCE\n",
    "# ============================================================\n",
    "\n",
    "def decode_with_conf(preds, charset):\n",
    "    sm = preds.softmax(2)\n",
    "    idxs = sm.argmax(2).cpu().numpy()\n",
    "    all_texts, all_confs = [], []\n",
    "\n",
    "    for b in range(idxs.shape[1]):\n",
    "        last = 0\n",
    "        chars, probs = [], []\n",
    "        for t in range(idxs.shape[0]):\n",
    "            i = idxs[t, b]\n",
    "            p = sm[t, b, i].item()\n",
    "            if i != 0 and i != last:  # skip blank + repeat\n",
    "                chars.append(charset[i])\n",
    "                probs.append(p)\n",
    "            last = i\n",
    "        all_texts.append(\"\".join(chars))\n",
    "        all_confs.append(float(np.mean(probs)) if probs else 0.0)\n",
    "    return all_texts, all_confs\n",
    "\n",
    "# ============================================================\n",
    "# MAIN INFERENCE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load config\n",
    "    with open(\"config.yaml\", \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    charset = load_charset(cfg[\"charset_path\"])\n",
    "\n",
    "    # Initialize model\n",
    "    model = OCRModelResNet18(\n",
    "        num_classes=cfg[\"num_classes\"],\n",
    "        img_channels=cfg[\"num_channels\"],\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        pretrained=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Load checkpoint (state_dict directly)\n",
    "    ckpt_path = \"checkpoints/crnn_epoch_66.pth\"\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state, strict=True)\n",
    "    model.eval()\n",
    "    print(f\"Loaded checkpoint: {ckpt_path}\")\n",
    "\n",
    "    # Get 15 random test images\n",
    "    test_dir = \"data/ocr_dataset/test\"\n",
    "    png_files = sorted([f for f in os.listdir(test_dir) if f.endswith(\".png\")])\n",
    "    sample_files = random.sample(png_files, 15)\n",
    "\n",
    "    print(\"\\n========== 15 RANDOM TEST IMAGES ==========\\n\")\n",
    "    for img_name in sample_files:\n",
    "        img_path = os.path.join(test_dir, img_name)\n",
    "        txt_path = img_path.replace(\".png\", \".txt\")\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            gt = f.read().strip()\n",
    "\n",
    "        img = preprocess_image(img_path, cfg[\"img_height\"], cfg[\"img_width\"]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(img)\n",
    "\n",
    "        text, conf = decode_with_conf(preds, charset)\n",
    "\n",
    "        print(f\"Image: {img_name}\")\n",
    "        print(\"Ground Truth :\", gt)\n",
    "        print(\"Prediction   :\", text[0])\n",
    "        print(\"Confidence   :\", round(conf[0], 4))\n",
    "        print(\"--------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4e47d5d-f920-45a1-b0f8-0da76b750ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"checkpoints/crnn_epoch_66.pth\", map_location=\"cpu\")\n",
    "print(len(ckpt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58eba459-2419-44ce-b27f-57d2acc5dd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:2\n",
      "Number of classes (including blank): 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3323772/1517452690.py:232: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_3323772/1517452690.py:266: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/100 (Train): 100%|| 63/63 [00:05<00:00, 10.54it/s, train_loss=1.79]\n",
      "Epoch 1/100 (Val): 100%|| 8/8 [00:00<00:00, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]  Train Loss: 1.7868, Val Loss: 1.5989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_1_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 (Train): 100%|| 63/63 [00:04<00:00, 15.71it/s, train_loss=1.66]\n",
      "Epoch 2/100 (Val): 100%|| 8/8 [00:00<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100]  Train Loss: 1.6572, Val Loss: 1.5619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_2_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 (Train): 100%|| 63/63 [00:03<00:00, 17.16it/s, train_loss=1.62]\n",
      "Epoch 3/100 (Val): 100%|| 8/8 [00:00<00:00, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100]  Train Loss: 1.6243, Val Loss: 1.5580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_3_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 (Train): 100%|| 63/63 [00:03<00:00, 16.26it/s, train_loss=1.61]\n",
      "Epoch 4/100 (Val): 100%|| 8/8 [00:00<00:00, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100]  Train Loss: 1.6140, Val Loss: 1.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_4_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 (Train): 100%|| 63/63 [00:03<00:00, 17.77it/s, train_loss=1.58]\n",
      "Epoch 5/100 (Val): 100%|| 8/8 [00:00<00:00, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/100]  Train Loss: 1.5804, Val Loss: 1.4757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_5_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 (Train): 100%|| 63/63 [00:03<00:00, 17.75it/s, train_loss=1.55]\n",
      "Epoch 6/100 (Val): 100%|| 8/8 [00:00<00:00, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/100]  Train Loss: 1.5455, Val Loss: 1.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_6_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 (Train): 100%|| 63/63 [00:03<00:00, 16.28it/s, train_loss=1.52]\n",
      "Epoch 7/100 (Val): 100%|| 8/8 [00:00<00:00, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/100]  Train Loss: 1.5237, Val Loss: 1.4440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_7_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 (Train): 100%|| 63/63 [00:03<00:00, 16.07it/s, train_loss=1.49]\n",
      "Epoch 8/100 (Val): 100%|| 8/8 [00:00<00:00, 12.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/100]  Train Loss: 1.4942, Val Loss: 1.4219\n",
      "Sample prediction saved: checkpoints/epoch_8_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 (Train): 100%|| 63/63 [00:03<00:00, 17.54it/s, train_loss=1.48]\n",
      "Epoch 9/100 (Val): 100%|| 8/8 [00:00<00:00, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/100]  Train Loss: 1.4814, Val Loss: 1.4531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_9_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 (Train): 100%|| 63/63 [00:03<00:00, 19.22it/s, train_loss=1.46]\n",
      "Epoch 10/100 (Val): 100%|| 8/8 [00:00<00:00, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100]  Train Loss: 1.4610, Val Loss: 1.4207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_10_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 (Train): 100%|| 63/63 [00:03<00:00, 20.02it/s, train_loss=1.45]\n",
      "Epoch 11/100 (Val): 100%|| 8/8 [00:00<00:00, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/100]  Train Loss: 1.4451, Val Loss: 1.3732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_11_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 (Train): 100%|| 63/63 [00:03<00:00, 18.07it/s, train_loss=1.42]\n",
      "Epoch 12/100 (Val): 100%|| 8/8 [00:00<00:00, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/100]  Train Loss: 1.4224, Val Loss: 1.3654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_12_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 (Train): 100%|| 63/63 [00:03<00:00, 16.51it/s, train_loss=1.41]\n",
      "Epoch 13/100 (Val): 100%|| 8/8 [00:00<00:00, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/100]  Train Loss: 1.4061, Val Loss: 1.3495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_13_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 (Train): 100%|| 63/63 [00:03<00:00, 17.45it/s, train_loss=1.38]\n",
      "Epoch 14/100 (Val): 100%|| 8/8 [00:00<00:00, 12.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/100]  Train Loss: 1.3778, Val Loss: 1.4392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_14_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 (Train): 100%|| 63/63 [00:03<00:00, 17.77it/s, train_loss=1.37]\n",
      "Epoch 15/100 (Val): 100%|| 8/8 [00:00<00:00,  8.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/100]  Train Loss: 1.3737, Val Loss: 1.3819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_15_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 (Train): 100%|| 63/63 [00:03<00:00, 19.35it/s, train_loss=1.32]\n",
      "Epoch 16/100 (Val): 100%|| 8/8 [00:01<00:00,  6.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/100]  Train Loss: 1.3241, Val Loss: 1.4440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_16_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 (Train): 100%|| 63/63 [00:03<00:00, 17.72it/s, train_loss=1.31]\n",
      "Epoch 17/100 (Val): 100%|| 8/8 [00:00<00:00, 12.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/100]  Train Loss: 1.3114, Val Loss: 1.2859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_17_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 (Train): 100%|| 63/63 [00:03<00:00, 17.29it/s, train_loss=1.29]\n",
      "Epoch 18/100 (Val): 100%|| 8/8 [00:00<00:00, 12.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/100]  Train Loss: 1.2929, Val Loss: 1.3752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_18_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 (Train): 100%|| 63/63 [00:03<00:00, 17.88it/s, train_loss=1.28]\n",
      "Epoch 19/100 (Val): 100%|| 8/8 [00:00<00:00, 11.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100]  Train Loss: 1.2783, Val Loss: 1.2827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_19_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 (Train): 100%|| 63/63 [00:03<00:00, 18.73it/s, train_loss=1.27]\n",
      "Epoch 20/100 (Val): 100%|| 8/8 [00:00<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/100]  Train Loss: 1.2680, Val Loss: 1.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_20_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 (Train): 100%|| 63/63 [00:03<00:00, 16.85it/s, train_loss=1.25]\n",
      "Epoch 21/100 (Val): 100%|| 8/8 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/100]  Train Loss: 1.2547, Val Loss: 1.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_21_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 (Train): 100%|| 63/63 [00:03<00:00, 17.76it/s, train_loss=1.24]\n",
      "Epoch 22/100 (Val): 100%|| 8/8 [00:00<00:00, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/100]  Train Loss: 1.2356, Val Loss: 1.2984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_22_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 (Train): 100%|| 63/63 [00:03<00:00, 16.56it/s, train_loss=1.23]\n",
      "Epoch 23/100 (Val): 100%|| 8/8 [00:00<00:00, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/100]  Train Loss: 1.2251, Val Loss: 1.3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_23_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 (Train): 100%|| 63/63 [00:03<00:00, 16.28it/s, train_loss=1.21]\n",
      "Epoch 24/100 (Val): 100%|| 8/8 [00:00<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/100]  Train Loss: 1.2146, Val Loss: 1.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_24_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 (Train): 100%|| 63/63 [00:03<00:00, 17.55it/s, train_loss=1.2] \n",
      "Epoch 25/100 (Val): 100%|| 8/8 [00:00<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/100]  Train Loss: 1.2008, Val Loss: 1.3160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_25_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 (Train): 100%|| 63/63 [00:03<00:00, 16.26it/s, train_loss=1.18]\n",
      "Epoch 26/100 (Val): 100%|| 8/8 [00:00<00:00, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/100]  Train Loss: 1.1777, Val Loss: 1.3074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_26_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 (Train): 100%|| 63/63 [00:03<00:00, 18.46it/s, train_loss=1.17]\n",
      "Epoch 27/100 (Val): 100%|| 8/8 [00:00<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/100]  Train Loss: 1.1723, Val Loss: 1.3357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_27_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 (Train): 100%|| 63/63 [00:03<00:00, 16.38it/s, train_loss=1.14]\n",
      "Epoch 28/100 (Val): 100%|| 8/8 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/100]  Train Loss: 1.1448, Val Loss: 1.2878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_28_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 (Train): 100%|| 63/63 [00:03<00:00, 16.93it/s, train_loss=1.14]\n",
      "Epoch 29/100 (Val): 100%|| 8/8 [00:00<00:00, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/100]  Train Loss: 1.1449, Val Loss: 1.3316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_29_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 (Train): 100%|| 63/63 [00:03<00:00, 16.75it/s, train_loss=1.14]\n",
      "Epoch 30/100 (Val): 100%|| 8/8 [00:00<00:00, 12.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/100]  Train Loss: 1.1355, Val Loss: 1.2901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_30_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 (Train): 100%|| 63/63 [00:03<00:00, 18.17it/s, train_loss=1.09]\n",
      "Epoch 31/100 (Val): 100%|| 8/8 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/100]  Train Loss: 1.0925, Val Loss: 1.2944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_31_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 (Train): 100%|| 63/63 [00:03<00:00, 17.75it/s, train_loss=1.07]\n",
      "Epoch 32/100 (Val): 100%|| 8/8 [00:00<00:00,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/100]  Train Loss: 1.0676, Val Loss: 1.3049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_32_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 (Train): 100%|| 63/63 [00:03<00:00, 16.58it/s, train_loss=1.05]\n",
      "Epoch 33/100 (Val): 100%|| 8/8 [00:00<00:00, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/100]  Train Loss: 1.0513, Val Loss: 1.3372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_33_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 (Train): 100%|| 63/63 [00:03<00:00, 15.88it/s, train_loss=1.04]\n",
      "Epoch 34/100 (Val): 100%|| 8/8 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/100]  Train Loss: 1.0392, Val Loss: 1.3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_34_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 (Train): 100%|| 63/63 [00:03<00:00, 19.33it/s, train_loss=1.03] \n",
      "Epoch 35/100 (Val): 100%|| 8/8 [00:00<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/100]  Train Loss: 1.0290, Val Loss: 1.3055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_35_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 (Train): 100%|| 63/63 [00:03<00:00, 19.03it/s, train_loss=1.02] \n",
      "Epoch 36/100 (Val): 100%|| 8/8 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/100]  Train Loss: 1.0160, Val Loss: 1.3258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_36_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 (Train): 100%|| 63/63 [00:03<00:00, 17.66it/s, train_loss=1.01]\n",
      "Epoch 37/100 (Val): 100%|| 8/8 [00:01<00:00,  5.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/100]  Train Loss: 1.0059, Val Loss: 1.3411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_37_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 (Train): 100%|| 63/63 [00:03<00:00, 16.87it/s, train_loss=0.998]\n",
      "Epoch 38/100 (Val): 100%|| 8/8 [00:00<00:00, 12.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/100]  Train Loss: 0.9980, Val Loss: 1.3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_38_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 (Train): 100%|| 63/63 [00:03<00:00, 18.77it/s, train_loss=0.992]\n",
      "Epoch 39/100 (Val): 100%|| 8/8 [00:00<00:00, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/100]  Train Loss: 0.9916, Val Loss: 1.3262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_39_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 (Train): 100%|| 63/63 [00:03<00:00, 16.96it/s, train_loss=0.973]\n",
      "Epoch 40/100 (Val): 100%|| 8/8 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/100]  Train Loss: 0.9731, Val Loss: 1.3326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_40_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 (Train): 100%|| 63/63 [00:03<00:00, 19.22it/s, train_loss=0.962]\n",
      "Epoch 41/100 (Val): 100%|| 8/8 [00:01<00:00,  6.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/100]  Train Loss: 0.9617, Val Loss: 1.3335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_41_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 (Train): 100%|| 63/63 [00:03<00:00, 19.30it/s, train_loss=0.955]\n",
      "Epoch 42/100 (Val): 100%|| 8/8 [00:00<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/100]  Train Loss: 0.9553, Val Loss: 1.3407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_42_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 (Train): 100%|| 63/63 [00:03<00:00, 18.93it/s, train_loss=0.943]\n",
      "Epoch 43/100 (Val): 100%|| 8/8 [00:00<00:00,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/100]  Train Loss: 0.9426, Val Loss: 1.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_43_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 (Train): 100%|| 63/63 [00:03<00:00, 16.80it/s, train_loss=0.927]\n",
      "Epoch 44/100 (Val): 100%|| 8/8 [00:00<00:00, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/100]  Train Loss: 0.9273, Val Loss: 1.3687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_44_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 (Train): 100%|| 63/63 [00:03<00:00, 18.36it/s, train_loss=0.915]\n",
      "Epoch 45/100 (Val): 100%|| 8/8 [00:00<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/100]  Train Loss: 0.9153, Val Loss: 1.3700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_45_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 (Train): 100%|| 63/63 [00:03<00:00, 18.42it/s, train_loss=0.886]\n",
      "Epoch 46/100 (Val): 100%|| 8/8 [00:00<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/100]  Train Loss: 0.8861, Val Loss: 1.3507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_46_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 (Train): 100%|| 63/63 [00:03<00:00, 17.70it/s, train_loss=0.872]\n",
      "Epoch 47/100 (Val): 100%|| 8/8 [00:00<00:00,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/100]  Train Loss: 0.8717, Val Loss: 1.3588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_47_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 (Train): 100%|| 63/63 [00:03<00:00, 18.69it/s, train_loss=0.865]\n",
      "Epoch 48/100 (Val): 100%|| 8/8 [00:00<00:00, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/100]  Train Loss: 0.8652, Val Loss: 1.3655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_48_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 (Train): 100%|| 63/63 [00:03<00:00, 16.10it/s, train_loss=0.852]\n",
      "Epoch 49/100 (Val): 100%|| 8/8 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/100]  Train Loss: 0.8523, Val Loss: 1.3625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_49_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 (Train): 100%|| 63/63 [00:03<00:00, 17.45it/s, train_loss=0.848]\n",
      "Epoch 50/100 (Val): 100%|| 8/8 [00:00<00:00, 12.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/100]  Train Loss: 0.8483, Val Loss: 1.3762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_50_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 (Train): 100%|| 63/63 [00:03<00:00, 17.26it/s, train_loss=0.845]\n",
      "Epoch 51/100 (Val): 100%|| 8/8 [00:00<00:00, 11.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/100]  Train Loss: 0.8445, Val Loss: 1.3805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_51_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 (Train): 100%|| 63/63 [00:03<00:00, 16.54it/s, train_loss=0.836]\n",
      "Epoch 52/100 (Val): 100%|| 8/8 [00:00<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/100]  Train Loss: 0.8358, Val Loss: 1.3816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_52_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 (Train): 100%|| 63/63 [00:03<00:00, 16.50it/s, train_loss=0.827]\n",
      "Epoch 53/100 (Val): 100%|| 8/8 [00:00<00:00, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/100]  Train Loss: 0.8272, Val Loss: 1.3764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_53_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 (Train): 100%|| 63/63 [00:03<00:00, 18.56it/s, train_loss=0.825]\n",
      "Epoch 54/100 (Val): 100%|| 8/8 [00:00<00:00, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/100]  Train Loss: 0.8253, Val Loss: 1.3815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_54_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 (Train): 100%|| 63/63 [00:03<00:00, 16.45it/s, train_loss=0.81] \n",
      "Epoch 55/100 (Val): 100%|| 8/8 [00:01<00:00,  6.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/100]  Train Loss: 0.8096, Val Loss: 1.3895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_55_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 (Train): 100%|| 63/63 [00:03<00:00, 16.85it/s, train_loss=0.807]\n",
      "Epoch 56/100 (Val): 100%|| 8/8 [00:00<00:00, 11.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/100]  Train Loss: 0.8067, Val Loss: 1.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_56_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 (Train): 100%|| 63/63 [00:03<00:00, 15.79it/s, train_loss=0.797]\n",
      "Epoch 57/100 (Val): 100%|| 8/8 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/100]  Train Loss: 0.7968, Val Loss: 1.4038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_57_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 (Train): 100%|| 63/63 [00:03<00:00, 16.87it/s, train_loss=0.782]\n",
      "Epoch 58/100 (Val): 100%|| 8/8 [00:00<00:00, 11.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/100]  Train Loss: 0.7817, Val Loss: 1.4065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_58_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 (Train): 100%|| 63/63 [00:03<00:00, 17.68it/s, train_loss=0.778]\n",
      "Epoch 59/100 (Val): 100%|| 8/8 [00:00<00:00, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/100]  Train Loss: 0.7776, Val Loss: 1.4181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_59_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100 (Train): 100%|| 63/63 [00:03<00:00, 16.94it/s, train_loss=0.772]\n",
      "Epoch 60/100 (Val): 100%|| 8/8 [00:00<00:00, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/100]  Train Loss: 0.7725, Val Loss: 1.4236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_60_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100 (Train): 100%|| 63/63 [00:03<00:00, 17.40it/s, train_loss=0.751]\n",
      "Epoch 61/100 (Val): 100%|| 8/8 [00:00<00:00,  9.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/100]  Train Loss: 0.7509, Val Loss: 1.4127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_61_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100 (Train): 100%|| 63/63 [00:03<00:00, 16.40it/s, train_loss=0.745]\n",
      "Epoch 62/100 (Val): 100%|| 8/8 [00:00<00:00, 11.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/100]  Train Loss: 0.7446, Val Loss: 1.4257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_62_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100 (Train): 100%|| 63/63 [00:03<00:00, 19.17it/s, train_loss=0.743]\n",
      "Epoch 63/100 (Val): 100%|| 8/8 [00:00<00:00, 11.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/100]  Train Loss: 0.7429, Val Loss: 1.4293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_63_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100 (Train): 100%|| 63/63 [00:03<00:00, 18.63it/s, train_loss=0.741]\n",
      "Epoch 64/100 (Val): 100%|| 8/8 [00:00<00:00, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/100]  Train Loss: 0.7410, Val Loss: 1.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_64_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100 (Train): 100%|| 63/63 [00:03<00:00, 17.68it/s, train_loss=0.727]\n",
      "Epoch 65/100 (Val): 100%|| 8/8 [00:00<00:00,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/100]  Train Loss: 0.7275, Val Loss: 1.4318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_65_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100 (Train): 100%|| 63/63 [00:03<00:00, 17.21it/s, train_loss=0.724]\n",
      "Epoch 66/100 (Val): 100%|| 8/8 [00:00<00:00, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/100]  Train Loss: 0.7243, Val Loss: 1.4417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_66_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100 (Train): 100%|| 63/63 [00:03<00:00, 16.68it/s, train_loss=0.718]\n",
      "Epoch 67/100 (Val): 100%|| 8/8 [00:00<00:00, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/100]  Train Loss: 0.7184, Val Loss: 1.4369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_67_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100 (Train): 100%|| 63/63 [00:03<00:00, 16.95it/s, train_loss=0.718]\n",
      "Epoch 68/100 (Val): 100%|| 8/8 [00:01<00:00,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/100]  Train Loss: 0.7179, Val Loss: 1.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_68_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100 (Train): 100%|| 63/63 [00:03<00:00, 16.23it/s, train_loss=0.718]\n",
      "Epoch 69/100 (Val): 100%|| 8/8 [00:00<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/100]  Train Loss: 0.7176, Val Loss: 1.4466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_69_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100 (Train): 100%|| 63/63 [00:03<00:00, 16.13it/s, train_loss=0.716]\n",
      "Epoch 70/100 (Val): 100%|| 8/8 [00:00<00:00, 12.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/100]  Train Loss: 0.7165, Val Loss: 1.4435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_70_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100 (Train): 100%|| 63/63 [00:03<00:00, 16.69it/s, train_loss=0.71] \n",
      "Epoch 71/100 (Val): 100%|| 8/8 [00:00<00:00,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/100]  Train Loss: 0.7096, Val Loss: 1.4408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_71_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100 (Train): 100%|| 63/63 [00:03<00:00, 16.95it/s, train_loss=0.707]\n",
      "Epoch 72/100 (Val): 100%|| 8/8 [00:00<00:00,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/100]  Train Loss: 0.7065, Val Loss: 1.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_72_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100 (Train): 100%|| 63/63 [00:03<00:00, 16.05it/s, train_loss=0.694]\n",
      "Epoch 73/100 (Val): 100%|| 8/8 [00:00<00:00, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/100]  Train Loss: 0.6942, Val Loss: 1.4509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_73_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100 (Train): 100%|| 63/63 [00:03<00:00, 17.64it/s, train_loss=0.696]\n",
      "Epoch 74/100 (Val): 100%|| 8/8 [00:00<00:00,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/100]  Train Loss: 0.6959, Val Loss: 1.4511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_74_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100 (Train): 100%|| 63/63 [00:03<00:00, 16.84it/s, train_loss=0.69] \n",
      "Epoch 75/100 (Val): 100%|| 8/8 [00:00<00:00, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/100]  Train Loss: 0.6903, Val Loss: 1.4612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_75_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100 (Train): 100%|| 63/63 [00:03<00:00, 17.27it/s, train_loss=0.68] \n",
      "Epoch 76/100 (Val): 100%|| 8/8 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/100]  Train Loss: 0.6804, Val Loss: 1.4560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_76_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100 (Train): 100%|| 63/63 [00:04<00:00, 15.67it/s, train_loss=0.68] \n",
      "Epoch 77/100 (Val): 100%|| 8/8 [00:00<00:00, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/100]  Train Loss: 0.6802, Val Loss: 1.4630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_77_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100 (Train): 100%|| 63/63 [00:04<00:00, 15.64it/s, train_loss=0.676]\n",
      "Epoch 78/100 (Val): 100%|| 8/8 [00:00<00:00, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/100]  Train Loss: 0.6756, Val Loss: 1.4578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_78_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100 (Train): 100%|| 63/63 [00:03<00:00, 16.19it/s, train_loss=0.673]\n",
      "Epoch 79/100 (Val): 100%|| 8/8 [00:00<00:00,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/100]  Train Loss: 0.6735, Val Loss: 1.4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_79_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100 (Train): 100%|| 63/63 [00:03<00:00, 15.82it/s, train_loss=0.669]\n",
      "Epoch 80/100 (Val): 100%|| 8/8 [00:00<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/100]  Train Loss: 0.6694, Val Loss: 1.4611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_80_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100 (Train): 100%|| 63/63 [00:03<00:00, 17.29it/s, train_loss=0.664]\n",
      "Epoch 81/100 (Val): 100%|| 8/8 [00:00<00:00, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/100]  Train Loss: 0.6644, Val Loss: 1.4631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_81_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100 (Train): 100%|| 63/63 [00:03<00:00, 15.94it/s, train_loss=0.665]\n",
      "Epoch 82/100 (Val): 100%|| 8/8 [00:00<00:00, 11.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/100]  Train Loss: 0.6650, Val Loss: 1.4630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_82_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100 (Train): 100%|| 63/63 [00:04<00:00, 15.54it/s, train_loss=0.662]\n",
      "Epoch 83/100 (Val): 100%|| 8/8 [00:00<00:00, 12.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/100]  Train Loss: 0.6620, Val Loss: 1.4622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_83_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100 (Train): 100%|| 63/63 [00:03<00:00, 18.27it/s, train_loss=0.664]\n",
      "Epoch 84/100 (Val): 100%|| 8/8 [00:00<00:00,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/100]  Train Loss: 0.6637, Val Loss: 1.4613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_84_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100 (Train): 100%|| 63/63 [00:03<00:00, 16.51it/s, train_loss=0.652]\n",
      "Epoch 85/100 (Val): 100%|| 8/8 [00:00<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/100]  Train Loss: 0.6517, Val Loss: 1.4736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_85_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100 (Train): 100%|| 63/63 [00:03<00:00, 17.98it/s, train_loss=0.655]\n",
      "Epoch 86/100 (Val): 100%|| 8/8 [00:00<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/100]  Train Loss: 0.6553, Val Loss: 1.4730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_86_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100 (Train): 100%|| 63/63 [00:03<00:00, 17.02it/s, train_loss=0.652]\n",
      "Epoch 87/100 (Val): 100%|| 8/8 [00:00<00:00, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/100]  Train Loss: 0.6518, Val Loss: 1.4738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_87_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100 (Train): 100%|| 63/63 [00:03<00:00, 16.58it/s, train_loss=0.652]\n",
      "Epoch 88/100 (Val): 100%|| 8/8 [00:00<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/100]  Train Loss: 0.6517, Val Loss: 1.4790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_88_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100 (Train): 100%|| 63/63 [00:03<00:00, 17.85it/s, train_loss=0.648]\n",
      "Epoch 89/100 (Val): 100%|| 8/8 [00:00<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/100]  Train Loss: 0.6484, Val Loss: 1.4832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_89_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100 (Train): 100%|| 63/63 [00:03<00:00, 16.73it/s, train_loss=0.647]\n",
      "Epoch 90/100 (Val): 100%|| 8/8 [00:00<00:00, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/100]  Train Loss: 0.6470, Val Loss: 1.4855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_90_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100 (Train): 100%|| 63/63 [00:03<00:00, 18.49it/s, train_loss=0.64] \n",
      "Epoch 91/100 (Val): 100%|| 8/8 [00:00<00:00, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/100]  Train Loss: 0.6400, Val Loss: 1.4807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_91_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100 (Train): 100%|| 63/63 [00:03<00:00, 17.39it/s, train_loss=0.641]\n",
      "Epoch 92/100 (Val): 100%|| 8/8 [00:00<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/100]  Train Loss: 0.6405, Val Loss: 1.4823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_92_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100 (Train): 100%|| 63/63 [00:03<00:00, 16.18it/s, train_loss=0.64] \n",
      "Epoch 93/100 (Val): 100%|| 8/8 [00:00<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/100]  Train Loss: 0.6404, Val Loss: 1.4793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_93_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100 (Train): 100%|| 63/63 [00:03<00:00, 16.69it/s, train_loss=0.636]\n",
      "Epoch 94/100 (Val): 100%|| 8/8 [00:00<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/100]  Train Loss: 0.6365, Val Loss: 1.4819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_94_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100 (Train): 100%|| 63/63 [00:03<00:00, 17.77it/s, train_loss=0.635]\n",
      "Epoch 95/100 (Val): 100%|| 8/8 [00:00<00:00,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/100]  Train Loss: 0.6349, Val Loss: 1.4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_95_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100 (Train): 100%|| 63/63 [00:03<00:00, 16.77it/s, train_loss=0.636]\n",
      "Epoch 96/100 (Val): 100%|| 8/8 [00:00<00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/100]  Train Loss: 0.6363, Val Loss: 1.4870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_96_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100 (Train): 100%|| 63/63 [00:03<00:00, 16.76it/s, train_loss=0.636]\n",
      "Epoch 97/100 (Val): 100%|| 8/8 [00:00<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/100]  Train Loss: 0.6363, Val Loss: 1.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_97_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100 (Train): 100%|| 63/63 [00:03<00:00, 16.76it/s, train_loss=0.633]\n",
      "Epoch 98/100 (Val): 100%|| 8/8 [00:00<00:00, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/100]  Train Loss: 0.6331, Val Loss: 1.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_98_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100 (Train): 100%|| 63/63 [00:03<00:00, 16.44it/s, train_loss=0.63] \n",
      "Epoch 99/100 (Val): 100%|| 8/8 [00:00<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/100]  Train Loss: 0.6299, Val Loss: 1.4846\n",
      "Sample prediction saved: checkpoints/epoch_99_sample.png  Predicted text: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100 (Train): 100%|| 63/63 [00:03<00:00, 16.18it/s, train_loss=0.628]\n",
      "Epoch 100/100 (Val): 100%|| 8/8 [00:00<00:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100]  Train Loss: 0.6278, Val Loss: 1.4876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample prediction saved: checkpoints/epoch_100_sample.png  Predicted text: \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_crnn_resnet18.py\n",
    "import os\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.models as models\n",
    "\n",
    "# -----------------------\n",
    "# OCR Dataset\n",
    "# -----------------------\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, root, charset_path, img_h=32, img_w=512, debug=False):\n",
    "        self.root = root\n",
    "        self.img_h = img_h\n",
    "        self.img_w = img_w\n",
    "        self.debug = debug\n",
    "\n",
    "        # select PNG files that have a non-empty .txt label\n",
    "        files = []\n",
    "        for f in sorted(os.listdir(root)):\n",
    "            if f.endswith(\".png\"):\n",
    "                tpath = os.path.join(root, f[:-4] + \".txt\")\n",
    "                if os.path.exists(tpath) and os.path.getsize(tpath) > 0:\n",
    "                    files.append(f)\n",
    "        self.files = files\n",
    "\n",
    "        with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            charset_txt = f.read().strip()\n",
    "        # keep blank at index 0 for CTC blank\n",
    "        self.charset = [\"<blank>\"] + list(charset_txt)\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.charset)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_idx[c] for c in text if c in self.char_to_idx]\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img_path = os.path.join(self.root, img_name)\n",
    "        txt_path = os.path.join(self.root, img_name[:-4] + \".txt\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"L\")\n",
    "        w, h = img.size\n",
    "        new_h = self.img_h\n",
    "        new_w = int(w * (new_h / h))\n",
    "        img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "        # pad or crop width to fixed img_w\n",
    "        if new_w < self.img_w:\n",
    "            padded = Image.new(\"L\", (self.img_w, new_h), 255)\n",
    "            padded.paste(img, (0, 0))\n",
    "            img = padded\n",
    "        else:\n",
    "            img = img.crop((0, 0, self.img_w, new_h))\n",
    "\n",
    "        img = np.array(img).astype(np.float32) / 255.0  # [H, W]\n",
    "        # normalize to mean=0.5, std=0.5 (common)\n",
    "        img = (img - 0.5) / 0.5\n",
    "        img = torch.from_numpy(img).unsqueeze(0)  # [1, H, W]\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "        label = self.encode(text)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "def ocr_collate(batch):\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    label_lens = []\n",
    "    for img, label in batch:\n",
    "        if label.numel() == 0:\n",
    "            continue\n",
    "        imgs.append(img)\n",
    "        labels.append(label)\n",
    "        label_lens.append(label.size(0))\n",
    "\n",
    "    if len(imgs) == 0:\n",
    "        raise ValueError(\"All labels in this batch are empty.\")\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0)                 # [B, 1, H, W]\n",
    "    labels_cat = torch.cat(labels, dim=0)           # concatenated targets as 1D\n",
    "    label_lens = torch.tensor(label_lens, dtype=torch.long)\n",
    "    return imgs, labels_cat, label_lens\n",
    "\n",
    "# -----------------------\n",
    "# Model: ResNet18 backbone + BiLSTMs\n",
    "# -----------------------\n",
    "class ResNet18Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True, img_channels=1):\n",
    "        super().__init__()\n",
    "        # use new-style weights enum in recent torchvision\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        if img_channels != 3:\n",
    "            # replace conv1 to accept single channel\n",
    "            self.conv1 = nn.Conv2d(img_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        else:\n",
    "            self.conv1 = resnet.conv1\n",
    "        self.bn1 = resnet.bn1\n",
    "        self.relu = resnet.relu\n",
    "        self.maxpool = resnet.maxpool\n",
    "        self.layer1 = resnet.layer1\n",
    "        self.layer2 = resnet.layer2\n",
    "        self.layer3 = resnet.layer3\n",
    "        self.layer4 = resnet.layer4\n",
    "        # keep adaptive pooling so height -> 1 (sequence over width)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        # [B, 512, H', W']\n",
    "        x = self.adaptive_pool(x)     # [B, 512, 1, W_seq]\n",
    "        x = x.squeeze(2)             # [B, 512, W_seq]\n",
    "        x = x.permute(2, 0, 1)       # [T=W_seq, B, C=512]\n",
    "        return x\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size*2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)    # recurrent: [T, B, 2*hidden]\n",
    "        output = self.embedding(recurrent)  # [T, B, output_size]\n",
    "        return output\n",
    "\n",
    "class OCRModelResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.cnn = ResNet18Backbone(pretrained=pretrained, img_channels=img_channels)\n",
    "        # channel from ResNet18 is 512\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)   # [T, B, C]\n",
    "        out = self.rnn(features) # [T, B, num_classes]\n",
    "        return out\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        # preds: [T, B, C] (logits)\n",
    "        preds_log = preds.log_softmax(2)  # log-probs over classes\n",
    "        # CTC in PyTorch expects (T, N, C)\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "# -----------------------\n",
    "# Greedy decode\n",
    "# -----------------------\n",
    "def greedy_ctc_decode(preds, charset):\n",
    "    # preds: [T, B, C] logits or probs\n",
    "    argmax = preds.argmax(2).cpu().numpy()  # [T, B]\n",
    "    decoded = []\n",
    "    for b in range(argmax.shape[1]):\n",
    "        last_idx = 0\n",
    "        chars = []\n",
    "        for t in range(argmax.shape[0]):\n",
    "            idx = argmax[t, b]\n",
    "            if idx != 0 and idx != last_idx:  # skip blanks and repeats\n",
    "                chars.append(charset[idx])\n",
    "            last_idx = idx\n",
    "        decoded.append(\"\".join(chars))\n",
    "    return decoded\n",
    "\n",
    "# -----------------------\n",
    "# Load config\n",
    "# -----------------------\n",
    "with open(\"config.yaml\", \"r\") as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "\n",
    "learning_rate = float(cfg.get(\"learning_rate\", 1e-4))\n",
    "weight_decay = float(cfg.get(\"weight_decay\", 1e-5))\n",
    "\n",
    "# Use GPU device specified by environment variable if available\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# -----------------------\n",
    "# Prepare datasets & loaders\n",
    "# -----------------------\n",
    "train_dataset = OCRDataset(\"data/ocr_dataset/train\", cfg[\"charset_path\"],\n",
    "                           img_h=cfg.get(\"img_height\", 32), img_w=cfg.get(\"img_width\", 512))\n",
    "val_dataset = OCRDataset(\"data/ocr_dataset/val\", cfg[\"charset_path\"],\n",
    "                         img_h=cfg.get(\"img_height\", 32), img_w=cfg.get(\"img_width\", 512))\n",
    "\n",
    "# compute num_classes from charset (ensure cfg num_classes not stale)\n",
    "num_classes = len(train_dataset.charset)\n",
    "print(f\"Number of classes (including blank): {num_classes}\")\n",
    "\n",
    "# DataLoader note: on Windows, set num_workers=0 to avoid issues\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.get(\"batch_size\", 8), shuffle=True,\n",
    "                          collate_fn=ocr_collate, num_workers=cfg.get(\"num_workers\", 4), pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.get(\"batch_size\", 8), shuffle=False,\n",
    "                        collate_fn=ocr_collate, num_workers=cfg.get(\"num_workers\", 4), pin_memory=True)\n",
    "\n",
    "# -----------------------\n",
    "# Init model, optimizer, scheduler, AMP scaler\n",
    "# -----------------------\n",
    "model = OCRModelResNet18(num_classes=num_classes, img_channels=cfg.get(\"num_channels\", 1),\n",
    "                         hidden_size=cfg.get(\"hidden_size\", 256), pretrained=cfg.get(\"pretrained\", True)).to(device)\n",
    "\n",
    "# Optional: freeze CNN backbone initially (recommended for small datasets)\n",
    "if cfg.get(\"freeze_backbone_epochs\", 0) > 0:\n",
    "    for p in model.cnn.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"Backbone frozen for first {cfg.get('freeze_backbone_epochs')} epochs.\")\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                       lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=cfg.get(\"scheduler_step\", 15), gamma=cfg.get(\"scheduler_gamma\", 0.5))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "checkpoint_dir = cfg.get(\"checkpoint_dir\", \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Training loop\n",
    "# -----------------------\n",
    "epochs = cfg.get(\"epochs\", 30)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_steps = 0\n",
    "\n",
    "    # If we freeze backbone for some epochs, unfreeze at the right epoch\n",
    "    if cfg.get(\"freeze_backbone_epochs\", 0) > 0 and epoch == cfg.get(\"freeze_backbone_epochs\") + 1:\n",
    "        for p in model.cnn.parameters():\n",
    "            p.requires_grad = True\n",
    "        # re-create optimizer so it includes backbone params\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=cfg.get(\"scheduler_step\", 15), gamma=cfg.get(\"scheduler_gamma\", 0.5))\n",
    "        print(\"Unfroze backbone and reinitialized optimizer.\")\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} (Train)\")\n",
    "    for imgs, labels, label_lens in pbar:\n",
    "        imgs = imgs.to(device, non_blocking=True)\n",
    "        labels = labels.to(device)\n",
    "        label_lens = label_lens.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            preds = model(imgs)  # [T, B, C]\n",
    "            batch_size = imgs.size(0)\n",
    "            T = preds.size(0)\n",
    "            # pred_lengths: actual valid timesteps for each sample\n",
    "            pred_lengths = torch.full((batch_size,), T, dtype=torch.long, device=device)\n",
    "\n",
    "            # targets = concatenated labels (already in labels)\n",
    "            target_lengths = label_lens  # lengths per sample\n",
    "\n",
    "            loss = model.compute_ctc_loss(preds, labels, pred_lengths, target_lengths)\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"Skipping batch due to NaN/inf loss.\")\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        train_steps += 1\n",
    "        pbar.set_postfix(train_loss=running_loss / train_steps)\n",
    "\n",
    "    avg_train_loss = running_loss / max(1, train_steps)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # -------------------\n",
    "    # Validation\n",
    "    # -------------------\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_steps = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, label_lens in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} (Val)\"):\n",
    "            imgs = imgs.to(device, non_blocking=True)\n",
    "            labels = labels.to(device)\n",
    "            label_lens = label_lens.to(device)\n",
    "\n",
    "            preds = model(imgs)  # [T, B, C]\n",
    "            batch_size = imgs.size(0)\n",
    "            T = preds.size(0)\n",
    "            pred_lengths = torch.full((batch_size,), T, dtype=torch.long, device=device)\n",
    "            loss = model.compute_ctc_loss(preds, labels, pred_lengths, label_lens)\n",
    "\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(\"Skipping val batch due to NaN/inf loss.\")\n",
    "                continue\n",
    "            val_running_loss += loss.item()\n",
    "            val_steps += 1\n",
    "\n",
    "    avg_val_loss = val_running_loss / max(1, val_steps)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{epochs}]  Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # -------------------\n",
    "    # Save sample prediction (take first batch from val_loader)\n",
    "    # -------------------\n",
    "    try:\n",
    "        sample_imgs, _, _ = next(iter(val_loader))\n",
    "        sample_imgs = sample_imgs.to(device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            sample_preds = model(sample_imgs)  # [T, B, C]\n",
    "        decoded_texts = greedy_ctc_decode(sample_preds, train_dataset.charset)\n",
    "        # un-normalize first image and save\n",
    "        sample_to_save = sample_imgs[0].detach().cpu().clone()  # [1, H, W]\n",
    "        sample_to_save = (sample_to_save * 0.5) + 0.5  # unnormalize\n",
    "        sample_to_save = sample_to_save.clamp(0, 1)\n",
    "        save_path = os.path.join(checkpoint_dir, f\"epoch_{epoch}_sample.png\")\n",
    "        save_image(sample_to_save, save_path)\n",
    "        print(f\"Sample prediction saved: {save_path}  Predicted text: {decoded_texts[0]}\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not save sample prediction:\", e)\n",
    "\n",
    "    # -------------------\n",
    "    # Save checkpoint (model + optimizer + scaler + epoch)\n",
    "    # -------------------\n",
    "    ckpt = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "        \"train_loss\": avg_train_loss,\n",
    "        \"val_loss\": avg_val_loss,\n",
    "    }\n",
    "    torch.save(ckpt, os.path.join(checkpoint_dir, f\"crnn_epoch_{epoch}.pth\"))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# -----------------------\n",
    "# Plot loss curves (optional)\n",
    "# -----------------------\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\")\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"CTC Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4926a487-7e8c-4a90-9ade-d45e15562327",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 96\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# 5. Example Usage\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 96\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_image.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mocr_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcharset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharset.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacter Confidences:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mper_character_confidence\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "Cell \u001b[0;32mIn[49], line 73\u001b[0m, in \u001b[0;36minference\u001b[0;34m(img_path, model_path, charset_path)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m     72\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 73\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# Preprocess image\u001b[39;00m\n\u001b[1;32m     76\u001b[0m img \u001b[38;5;241m=\u001b[39m preprocess_image(img_path)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Load Charset\n",
    "# ----------------------------\n",
    "def load_charset(charset_path=\"charset.txt\"):\n",
    "    with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chars = [line.strip(\"\\n\") for line in f.readlines()]\n",
    "    return chars\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Preprocessing Function\n",
    "# ----------------------------\n",
    "def preprocess_image(img_path, img_height=32, img_width=256):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_height, img_width)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return transform(img).unsqueeze(0)  # shape (1, 1, H, W)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. CTC Greedy Decode with Confidence\n",
    "# ----------------------------\n",
    "def ctc_decode_with_confidence(logits, charset):\n",
    "    \"\"\"\n",
    "    logits: (T, N, C)\n",
    "    \"\"\"\n",
    "    log_probs = F.log_softmax(logits, dim=2)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    max_probs, max_idx = probs.max(dim=2)  # (T, N)\n",
    "\n",
    "    max_idx = max_idx.squeeze(1).cpu().numpy()\n",
    "    max_probs = max_probs.squeeze(1).cpu().numpy()\n",
    "\n",
    "    blank = len(charset)  # Last index is blank\n",
    "\n",
    "    text = \"\"\n",
    "    char_confidences = []\n",
    "    prev_idx = blank\n",
    "\n",
    "    for i in range(len(max_idx)):\n",
    "        if max_idx[i] != blank and max_idx[i] != prev_idx:\n",
    "            text += charset[max_idx[i]]\n",
    "            char_confidences.append(float(max_probs[i]))\n",
    "\n",
    "        prev_idx = max_idx[i]\n",
    "\n",
    "    # Overall confidence = average of character confidences\n",
    "    overall_conf = float(np.mean(char_confidences)) if char_confidences else 0.0\n",
    "\n",
    "    return text, char_confidences, overall_conf\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Inference Function\n",
    "# ----------------------------\n",
    "def inference(img_path, model_path, charset_path=\"charset.txt\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load charset\n",
    "    charset = load_charset(charset_path)\n",
    "    num_classes = len(charset) + 1  # +1 for CTC blank index\n",
    "\n",
    "    # Load model\n",
    "    model = torch.load(model_path, map_location=device)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess image\n",
    "    img = preprocess_image(img_path).to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = model(img)  # Expect (N, C, T)\n",
    "        logits = logits.permute(2, 0, 1)  #  (T, N, C)\n",
    "\n",
    "    # Decode\n",
    "    text, per_char_conf, overall_conf = ctc_decode_with_confidence(logits, charset)\n",
    "\n",
    "    return {\n",
    "        \"predicted_text\": text,\n",
    "        \"per_character_confidence\": per_char_conf,\n",
    "        \"overall_confidence\": overall_conf\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Example Usage\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    output = inference(\n",
    "        img_path=\"test_image.png\",\n",
    "        model_path=\"ocr_model.pth\",\n",
    "        charset_path=\"charset.txt\"\n",
    "    )\n",
    "\n",
    "    print(\"Predicted Text:\", output[\"predicted_text\"])\n",
    "    print(\"Character Confidences:\", output[\"per_character_confidence\"])\n",
    "    print(\"Overall Confidence:\", output[\"overall_confidence\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce2a982b-46fe-4f7f-939a-ce9cb99d4447",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \n",
      "Character Confidences: [0.09673946350812912, 0.6865260004997253, 0.2204488217830658, 0.33478111028671265]\n",
      "Overall Confidence: 0.3346238490194082\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------------\n",
    "# INFERENCE CODE (WORKS 100%)\n",
    "# -------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "def load_charset_inf(charset_path):\n",
    "    with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        t = f.read().strip()\n",
    "    return [\"<blank>\"] + list(t)\n",
    "\n",
    "\n",
    "def preprocess_inf(img_path, img_h=32, img_w=512):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    w, h = img.size\n",
    "\n",
    "    new_h = img_h\n",
    "    new_w = int(w * (new_h / h))\n",
    "    img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    if new_w < img_w:\n",
    "        padded = Image.new(\"L\", (img_w, new_h), 255)\n",
    "        padded.paste(img, (0, 0))\n",
    "        img = padded\n",
    "    else:\n",
    "        img = img.crop((0, 0, img_w, new_h))\n",
    "\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    img = (img - 0.5) / 0.5\n",
    "    return torch.tensor(img).unsqueeze(0).unsqueeze(0)  # [1,1,H,W]\n",
    "\n",
    "\n",
    "def decode_with_conf(preds, charset):\n",
    "    log_probs = F.log_softmax(preds, dim=2)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    max_probs, max_idx = probs.max(2)\n",
    "    max_idx = max_idx[:, 0].cpu().numpy()\n",
    "    max_probs = max_probs[:, 0].cpu().numpy()\n",
    "\n",
    "    blank = 0\n",
    "    text = \"\"\n",
    "    confs = []\n",
    "    prev = blank\n",
    "\n",
    "    for t in range(len(max_idx)):\n",
    "        idx = max_idx[t]\n",
    "        if idx != blank and idx != prev:\n",
    "            text += charset[idx]\n",
    "            confs.append(float(max_probs[t]))\n",
    "        prev = idx\n",
    "\n",
    "    overall = float(np.mean(confs)) if confs else 0.0\n",
    "    return text, confs, overall\n",
    "\n",
    "\n",
    "def inference(img_path, ckpt_path, config_path=\"config.yaml\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    charset = load_charset_inf(cfg[\"charset_path\"])\n",
    "    num_classes = len(charset)\n",
    "\n",
    "    # recreate model\n",
    "    model = OCRModelResNet18(\n",
    "        num_classes=num_classes,\n",
    "        img_channels=cfg[\"num_channels\"],\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "\n",
    "    # load checkpoint\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # preprocess\n",
    "    img = preprocess_inf(\n",
    "        img_path,\n",
    "        img_h=cfg[\"img_height\"],\n",
    "        img_w=cfg[\"img_width\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # forward\n",
    "    with torch.no_grad():\n",
    "        preds = model(img)\n",
    "\n",
    "    # decode\n",
    "    text, per_char, overall = decode_with_conf(preds, charset)\n",
    "\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"per_char_conf\": per_char,\n",
    "        \"overall_conf\": overall\n",
    "    }\n",
    "\n",
    "\n",
    "# Run if direct\n",
    "if __name__ == \"__main__\":\n",
    "    out = inference(\n",
    "        img_path=\"test_image.png\",\n",
    "        ckpt_path=\"checkpoints/crnn_epoch_30.pth\"\n",
    "    )\n",
    "\n",
    "    print(\"Predicted:\", out[\"text\"])\n",
    "    print(\"Character Confidences:\", out[\"per_char_conf\"])\n",
    "    print(\"Overall Confidence:\", out[\"overall_conf\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cf6f0ff4-b679-4e3b-994a-b155555a37fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====   OCR EVALUATION (15 RANDOM SAMPLES) =====\n",
      "\n",
      "Image: 00494.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.25442013144493103, 0.806685209274292, 0.1465686708688736, 0.12563499808311462, 0.6978088617324829]\n",
      "Overall Confidence: 0.4062\n",
      "------------------------------------------------------------\n",
      "Image: 00339.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.13507016003131866, 0.15263473987579346, 0.3132598400115967, 0.2154145985841751, 0.7853535413742065, 0.7531917691230774]\n",
      "Overall Confidence: 0.3925\n",
      "------------------------------------------------------------\n",
      "Image: 00250.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.19668440520763397, 0.2829357087612152, 0.29221397638320923, 0.10165660828351974, 0.271828830242157, 0.25077494978904724, 0.563072681427002, 0.2344239354133606]\n",
      "Overall Confidence: 0.2742\n",
      "------------------------------------------------------------\n",
      "Image: 00395.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.17079024016857147, 0.45782914757728577, 0.2903079092502594, 0.1593397557735443, 0.6525387167930603, 0.15760020911693573, 0.42550474405288696]\n",
      "Overall Confidence: 0.3306\n",
      "------------------------------------------------------------\n",
      "Image: 00273.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.2682948112487793, 0.3087223470211029, 0.551455020904541, 0.149308443069458, 0.36709603667259216, 0.1821916103363037]\n",
      "Overall Confidence: 0.3045\n",
      "------------------------------------------------------------\n",
      "Image: 00241.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.2366333305835724, 0.1598159670829773, 0.3158940374851227, 0.09975437074899673, 0.7014071345329285]\n",
      "Overall Confidence: 0.3027\n",
      "------------------------------------------------------------\n",
      "Image: 00461.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.30256888270378113, 0.6645694971084595, 0.22267679870128632, 0.35289719700813293, 0.18417294323444366]\n",
      "Overall Confidence: 0.3454\n",
      "------------------------------------------------------------\n",
      "Image: 00020.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.1353875696659088, 0.9691753387451172, 0.10960786044597626, 0.20862218737602234, 0.15941961109638214, 0.33687329292297363, 0.3512895107269287]\n",
      "Overall Confidence: 0.3243\n",
      "------------------------------------------------------------\n",
      "Image: 00263.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.18648499250411987, 0.354818731546402, 0.16374769806861877, 0.21732161939144135, 0.11705347150564194, 0.14556020498275757, 0.11482995748519897]\n",
      "Overall Confidence: 0.1857\n",
      "------------------------------------------------------------\n",
      "Image: 00313.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.25434544682502747, 0.5243816375732422, 0.27730894088745117, 0.21722878515720367, 0.26322388648986816]\n",
      "Overall Confidence: 0.3073\n",
      "------------------------------------------------------------\n",
      "Image: 00424.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.11366459727287292, 0.8259811401367188, 0.16486714780330658, 0.3486154079437256, 0.28978610038757324, 0.8613807559013367, 0.9359561204910278]\n",
      "Overall Confidence: 0.5058\n",
      "------------------------------------------------------------\n",
      "Image: 00223.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.2000032514333725, 0.13166485726833344, 0.13115741312503815, 0.13923504948616028, 0.17203781008720398, 0.10861954092979431, 0.3911277651786804]\n",
      "Overall Confidence: 0.1820\n",
      "------------------------------------------------------------\n",
      "Image: 00427.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.1217736154794693, 0.92750483751297, 0.17692090570926666, 0.8196194171905518, 0.12061575055122375, 0.15728268027305603, 0.6795614361763, 0.7245578765869141]\n",
      "Overall Confidence: 0.4660\n",
      "------------------------------------------------------------\n",
      "Image: 00200.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.1635524034500122, 0.4718160331249237, 0.15408499538898468, 0.22579987347126007, 0.8062317371368408, 0.5171877145767212]\n",
      "Overall Confidence: 0.3898\n",
      "------------------------------------------------------------\n",
      "Image: 00279.png\n",
      "Ground Truth: \n",
      "Predicted: \n",
      "Per-Character Confidence: [0.1865742802619934, 0.47569146752357483, 0.14219920337200165, 0.22105421125888824, 0.1310126781463623, 0.2144077718257904, 0.33662793040275574]\n",
      "Overall Confidence: 0.2439\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "#  YOUR EXISTING IMPORTS\n",
    "# -----------------------------\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# -----------------------------\n",
    "#  SAME LOADER FUNCTIONS\n",
    "# -----------------------------\n",
    "def load_charset_inf(charset_path):\n",
    "    with open(charset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        t = f.read().strip()\n",
    "    return [\"<blank>\"] + list(t)\n",
    "\n",
    "\n",
    "def preprocess_inf(img_path, img_h=32, img_w=512):\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    w, h = img.size\n",
    "\n",
    "    new_h = img_h\n",
    "    new_w = int(w * (new_h / h))\n",
    "    img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "\n",
    "    if new_w < img_w:\n",
    "        padded = Image.new(\"L\", (img_w, new_h), 255)\n",
    "        padded.paste(img, (0, 0))\n",
    "        img = padded\n",
    "    else:\n",
    "        img = img.crop((0, 0, img_w, new_h))\n",
    "\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    img = (img - 0.5) / 0.5\n",
    "    return torch.tensor(img).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "\n",
    "def decode_with_conf(preds, charset):\n",
    "    log_probs = F.log_softmax(preds, dim=2)\n",
    "    probs = torch.exp(log_probs)\n",
    "\n",
    "    max_probs, max_idx = probs.max(2)\n",
    "    max_idx = max_idx[:, 0].cpu().numpy()\n",
    "    max_probs = max_probs[:, 0].cpu().numpy()\n",
    "\n",
    "    blank = 0\n",
    "    text = \"\"\n",
    "    confs = []\n",
    "    prev = blank\n",
    "\n",
    "    for t in range(len(max_idx)):\n",
    "        idx = max_idx[t]\n",
    "        if idx != blank and idx != prev:\n",
    "            text += charset[idx]\n",
    "            confs.append(float(max_probs[t]))\n",
    "        prev = idx\n",
    "\n",
    "    overall = float(np.mean(confs)) if confs else 0.0\n",
    "    return text, confs, overall\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#   MAIN INFERENCE FUNCTION\n",
    "# -----------------------------\n",
    "def inference(img_path, ckpt_path, config_path=\"config.yaml\"):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "\n",
    "    # load charset\n",
    "    charset = load_charset_inf(cfg[\"charset_path\"])\n",
    "    num_classes = len(charset)\n",
    "\n",
    "    # recreate model\n",
    "    model = OCRModelResNet18(\n",
    "        num_classes=num_classes,\n",
    "        img_channels=cfg[\"num_channels\"],\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        pretrained=False\n",
    "    ).to(device)\n",
    "\n",
    "    # load checkpoint\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.eval()\n",
    "\n",
    "    # preprocess\n",
    "    img = preprocess_inf(\n",
    "        img_path,\n",
    "        img_h=cfg[\"img_height\"],\n",
    "        img_w=cfg[\"img_width\"]\n",
    "    ).to(device)\n",
    "\n",
    "    # forward\n",
    "    with torch.no_grad():\n",
    "        preds = model(img)\n",
    "\n",
    "    # decode prediction\n",
    "    pred, per_char, overall = decode_with_conf(preds, charset)\n",
    "\n",
    "    return pred, per_char, overall\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "#   NEW PART: RUN 15 RANDOM TEST SAMPLES WITH GT\n",
    "# ======================================================\n",
    "\n",
    "def evaluate_random_15(test_dir, ckpt_path, config_path=\"config.yaml\"):\n",
    "    # List all PNG images\n",
    "    all_pngs = [f for f in os.listdir(test_dir) if f.endswith(\".png\")]\n",
    "    random.shuffle(all_pngs)\n",
    "    selected = all_pngs[:15]\n",
    "\n",
    "    print(\"\\n=====   OCR EVALUATION (15 RANDOM SAMPLES) =====\\n\")\n",
    "\n",
    "    for img_name in selected:\n",
    "        base = img_name.replace(\".png\", \"\")\n",
    "        gt_file = os.path.join(test_dir, base + \".txt\")\n",
    "        img_file = os.path.join(test_dir, img_name)\n",
    "\n",
    "        # load ground truth\n",
    "        if os.path.exists(gt_file):\n",
    "            with open(gt_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                gt = f.read().strip()\n",
    "        else:\n",
    "            gt = \"[Missing GT]\"\n",
    "\n",
    "        # run inference\n",
    "        pred, char_conf, overall_conf = inference(\n",
    "            img_path=img_file,\n",
    "            ckpt_path=ckpt_path,\n",
    "            config_path=config_path\n",
    "        )\n",
    "\n",
    "        # print results\n",
    "        print(f\"Image: {img_name}\")\n",
    "        print(f\"Ground Truth: {gt}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"Per-Character Confidence: {char_conf}\")\n",
    "        print(f\"Overall Confidence: {overall_conf:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "#  RUN IT\n",
    "# ======================================================\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate_random_15(\n",
    "        test_dir=\"data/ocr_dataset/test\",\n",
    "        ckpt_path=\"checkpoints/crnn_epoch_30.pth\",\n",
    "        config_path=\"config.yaml\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfc3e99-8905-4808-af32-0b8414afdfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
