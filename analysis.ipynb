{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfb030d",
   "metadata": {},
   "source": [
    "# OCR-Devanagari-CRNN — Dataset Analysis & Training Pipeline\n",
    "\n",
    "This notebook creates an OCR for Devanagari using CRNN + LSTM, leveraging the HuggingFace dataset \"Sakonii/nepalitext-language-model-dataset\" to synthesize training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5482c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (25.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (80.9.0)\n",
      "Using cached pip-25.3-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\gaurav\\miniconda3\\envs\\ocr\\python.exe -m pip install --upgrade pip setuptools\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 2)) (0.24.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 3)) (2.9.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 4)) (4.4.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 5)) (4.57.1)\n",
      "Requirement already satisfied: trdg in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 6)) (0.1.1)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 7)) (9.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 8)) (2.2.6)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 9)) (6.0.3)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 10)) (4.12.0.88)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 11)) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 12)) (3.10.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 13)) (1.7.2)\n",
      "Requirement already satisfied: albumentations in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 14)) (2.0.8)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 15)) (1.2.1)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 16)) (8.1.8)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 18)) (1.9.4)\n",
      "Collecting uharfbuzz (from -r requirements.txt (line 19))\n",
      "  Downloading uharfbuzz-0.52.0-cp310-abi3-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: huggingface_hub[hf_xet] in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from -r requirements.txt (line 17)) (0.36.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from torch->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (0.70.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from datasets->-r requirements.txt (line 4)) (25.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from httpx<1.0.0->datasets->-r requirements.txt (line 4)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets->-r requirements.txt (line 4)) (0.16.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from transformers->-r requirements.txt (line 5)) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from transformers->-r requirements.txt (line 5)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from transformers->-r requirements.txt (line 5)) (0.6.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from trdg->-r requirements.txt (line 6)) (4.14.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from tqdm->-r requirements.txt (line 11)) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from matplotlib->-r requirements.txt (line 12)) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from matplotlib->-r requirements.txt (line 12)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from matplotlib->-r requirements.txt (line 12)) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from matplotlib->-r requirements.txt (line 12)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from matplotlib->-r requirements.txt (line 12)) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from matplotlib->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 13)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 13)) (3.6.0)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from albumentations->-r requirements.txt (line 14)) (2.12.4)\n",
      "Requirement already satisfied: albucore==0.0.24 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from albumentations->-r requirements.txt (line 14)) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from albumentations->-r requirements.txt (line 14)) (4.12.0.88)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 14)) (4.2.3)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from albucore==0.0.24->albumentations->-r requirements.txt (line 14)) (6.5.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 16)) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 16)) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 16)) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 16)) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipywidgets->-r requirements.txt (line 16)) (3.0.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from huggingface_hub[hf_xet]->-r requirements.txt (line 17)) (1.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets->-r requirements.txt (line 4)) (1.22.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from beautifulsoup4>=4.6.0->trdg->-r requirements.txt (line 6)) (2.8)\n",
      "Requirement already satisfied: decorator in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (0.2.1)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (0.8.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 14)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 14)) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from pydantic>=2.9.2->albumentations->-r requirements.txt (line 14)) (0.4.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 12)) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 4)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from requests>=2.32.2->datasets->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from anyio->httpx<1.0.0->datasets->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from pandas->datasets->-r requirements.txt (line 4)) (2025.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in c:\\users\\gaurav\\miniconda3\\envs\\ocr\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets->-r requirements.txt (line 16)) (0.2.3)\n",
      "Downloading uharfbuzz-0.52.0-cp310-abi3-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.3/1.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.0/1.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 3.5 MB/s  0:00:00\n",
      "Installing collected packages: uharfbuzz\n",
      "Successfully installed uharfbuzz-0.52.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade pip setuptools\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9368611",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import freetype\n",
    "import uharfbuzz as hb\n",
    "import cv2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20287711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NepaliText dataset\n",
    "dataset = load_dataset(\"Sakonii/nepalitext-language-model-dataset\")\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "print(f\"Loaded dataset with {len(train_texts)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f225a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean character function\n",
    "def clean_char(c):\n",
    "    if c in [\"\\n\", \"\\t\", \"\\r\"]:\n",
    "        return \"\"\n",
    "    if ord(c) in [8203, 8204, 8205, 8206, 8207]:\n",
    "        return \"\"\n",
    "    if 2304 <= ord(c) <= 2431:  # Devanagari block\n",
    "        return c\n",
    "    if c.isprintable():\n",
    "        return c\n",
    "    return \"\"\n",
    "\n",
    "# Character frequency analysis\n",
    "char_freq = Counter()\n",
    "for text in train_texts:\n",
    "    if isinstance(text, str):\n",
    "        cleaned = \"\".join(clean_char(c) for c in text)\n",
    "        char_freq.update(cleaned)\n",
    "\n",
    "print(f\"✓ Unique cleaned characters: {len(char_freq)}\")\n",
    "\n",
    "# Top 50 characters\n",
    "top_50 = char_freq.most_common(50)\n",
    "chars, freqs = zip(*top_50)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(chars, freqs)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.title(\"Top 50 Characters (Cleaned)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818175a",
   "metadata": {},
   "source": [
    "## Step 1: Extract 5000 Nepali Words\n",
    "\n",
    "Extract unique Devanagari words from the dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bab0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Devanagari words\n",
    "def extract_nepali_words(text):\n",
    "    \"\"\"Extract Devanagari words from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    matches = re.findall(r\"[\\u0900-\\u097F]+\", text)\n",
    "    return [w for w in matches if 2 <= len(w) <= 30]\n",
    "\n",
    "all_words = set()\n",
    "print(\"Extracting Nepali words from dataset...\")\n",
    "for i, text in enumerate(train_texts):\n",
    "    words = extract_nepali_words(text)\n",
    "    all_words.update(words)\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  Processed {i + 1} texts, found {len(all_words)} unique words\")\n",
    "\n",
    "# Sample 5000 words for training\n",
    "all_words = list(all_words)\n",
    "random.shuffle(all_words)\n",
    "training_words = all_words[:5000]\n",
    "\n",
    "print(f\"\\n✓ Total unique words: {len(all_words)}\")\n",
    "print(f\"✓ Using {len(training_words)} words for training\")\n",
    "print(f\"✓ Sample words: {training_words[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846268b",
   "metadata": {},
   "source": [
    "## Step 2: Synthetic Dataset Generator\n",
    "\n",
    "HarfBuzz-based generator for proper Devanagari shaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee9a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticHarfBuzzOCRDatasetGenerator:\n",
    "    \"\"\"Generate synthetic OCR dataset with HarfBuzz shaping.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        strings,\n",
    "        fonts_dir=\"fonts\",\n",
    "        output_dir=\"data/word_images\",\n",
    "        font_size_range=(40, 56),\n",
    "        random_blur=True,\n",
    "        random_noise=True,\n",
    "        random_rotate=True,\n",
    "        random_distortion=True,\n",
    "        background_mode=\"random\",\n",
    "        max_image_size=1024\n",
    "    ):\n",
    "        self.strings = strings\n",
    "        self.fonts = glob.glob(os.path.join(fonts_dir, \"**/*.ttf\"), recursive=True)\n",
    "        if not self.fonts:\n",
    "            raise ValueError(f\"No fonts found in {fonts_dir}\")\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        self.font_size_range = font_size_range\n",
    "        self.random_blur = random_blur\n",
    "        self.random_noise = random_noise\n",
    "        self.random_rotate = random_rotate\n",
    "        self.random_distortion = random_distortion\n",
    "        self.background_mode = background_mode\n",
    "        self.MAX_SIZE = max_image_size\n",
    "\n",
    "    def _clamp_image_size(self, img):\n",
    "        w, h = img.size\n",
    "        if w > self.MAX_SIZE or h > self.MAX_SIZE:\n",
    "            img.thumbnail((self.MAX_SIZE, self.MAX_SIZE), Image.LANCZOS)\n",
    "        return img\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        for idx, text in enumerate(self.strings, start=1):\n",
    "            img = self.render_text_image(text)\n",
    "            image_path = os.path.join(self.output_dir, f\"{idx:05d}.png\")\n",
    "            label_path = os.path.join(self.output_dir, f\"{idx:05d}.txt\")\n",
    "            img.save(image_path)\n",
    "            with open(label_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "            if idx % 500 == 0:\n",
    "                print(f\"  [{idx}/{len(self.strings)}] Generated images\")\n",
    "\n",
    "    def render_text_image(self, text, padding=20):\n",
    "        font_path = random.choice(self.fonts)\n",
    "        font_size = random.randint(*self.font_size_range)\n",
    "        face = freetype.Face(font_path)\n",
    "        face.set_char_size(font_size * 64)\n",
    "\n",
    "        # HarfBuzz shaping\n",
    "        hb_blob = hb.Blob.from_file_path(font_path)\n",
    "        hb_face = hb.Face(hb_blob, 0)\n",
    "        hb_font = hb.Font(hb_face)\n",
    "        hb_font.scale = (face.size.ascender, face.size.ascender)\n",
    "\n",
    "        buf = hb.Buffer()\n",
    "        buf.add_str(text)\n",
    "        buf.guess_segment_properties()\n",
    "        hb.shape(hb_font, buf)\n",
    "\n",
    "        infos = buf.glyph_infos\n",
    "        positions = buf.glyph_positions\n",
    "\n",
    "        width = sum(pos.x_advance for pos in positions) // 64 + 2*padding\n",
    "        height = font_size + 2*padding\n",
    "\n",
    "        if self.background_mode == \"white\":\n",
    "            img = Image.new(\"RGB\", (width, height), \"white\")\n",
    "        elif self.background_mode == \"lightgray\":\n",
    "            img = Image.new(\"RGB\", (width, height), \"lightgray\")\n",
    "        else:\n",
    "            arr = np.random.randint(200, 255, (height, width, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "\n",
    "        x, y = padding, padding + font_size\n",
    "\n",
    "        for info, pos in zip(infos, positions):\n",
    "            glyph_index = info.codepoint\n",
    "            face.load_glyph(glyph_index, freetype.FT_LOAD_RENDER)\n",
    "            bitmap = face.glyph.bitmap\n",
    "            top = face.glyph.bitmap_top\n",
    "            left = face.glyph.bitmap_left\n",
    "\n",
    "            if bitmap.width > 0 and bitmap.rows > 0:\n",
    "                glyph_img = Image.frombytes(\"L\", (bitmap.width, bitmap.rows), bytes(bitmap.buffer))\n",
    "                colored_glyph = Image.new(\"RGB\", glyph_img.size, \"black\")\n",
    "                img.paste(colored_glyph, (int(x + left), int(y - top)), glyph_img)\n",
    "\n",
    "            x += pos.x_advance / 64\n",
    "            y -= pos.y_advance / 64\n",
    "\n",
    "        img = self._clamp_image_size(img)\n",
    "\n",
    "        if self.random_blur and random.random() < 0.5:\n",
    "            img = img.filter(Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\n",
    "            from PIL import ImageFilter\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))\n",
    "\n",
    "        if self.random_rotate:\n",
    "            angle = random.randint(-7, 7)\n",
    "            img = img.rotate(angle, expand=True, fillcolor=\"white\")\n",
    "            img = self._clamp_image_size(img)\n",
    "\n",
    "        if self.random_distortion:\n",
    "            img = self.perspective_distortion(img)\n",
    "\n",
    "        if self.random_noise:\n",
    "            img = self.add_noise(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def perspective_distortion(self, img):\n",
    "        img = self._clamp_image_size(img)\n",
    "        w, h = img.size\n",
    "        arr = np.array(img)\n",
    "        shift = min(w, h) * 0.1\n",
    "\n",
    "        pts1 = np.float32([[0,0],[w,0],[0,h],[w,h]])\n",
    "        pts2 = np.float32([\n",
    "            [random.uniform(-shift, shift), random.uniform(-shift, shift)],\n",
    "            [w + random.uniform(-shift, shift), random.uniform(-shift, shift)],\n",
    "            [random.uniform(-shift, shift), h + random.uniform(-shift, shift)],\n",
    "            [w + random.uniform(-shift, shift), h + random.uniform(-shift, shift)],\n",
    "        ])\n",
    "        matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "        warped = cv2.warpPerspective(arr, matrix, (w,h), borderMode=cv2.BORDER_CONSTANT, borderValue=(255,255,255))\n",
    "        return Image.fromarray(warped)\n",
    "\n",
    "    def add_noise(self, img):\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        if random.random() < 0.5:\n",
    "            arr += np.random.normal(0, 10, arr.shape)\n",
    "        if random.random() < 0.5:\n",
    "            amount = 0.02\n",
    "            num_salt = int(arr.size * amount * 0.5)\n",
    "            num_pepper = int(arr.size * amount * 0.5)\n",
    "            coords = [np.random.randint(0, i - 1, num_salt) for i in arr.shape]\n",
    "            arr[tuple(coords)] = 255\n",
    "            coords = [np.random.randint(0, i - 1, num_pepper) for i in arr.shape]\n",
    "            arr[tuple(coords)] = 0\n",
    "        arr = np.clip(arr, 0, 255)\n",
    "        return Image.fromarray(arr.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30069a0",
   "metadata": {},
   "source": [
    "## Step 3: Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47901147",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GENERATING SYNTHETIC DATASET (5000 WORD SAMPLES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "generator = SyntheticHarfBuzzOCRDatasetGenerator(\n",
    "    strings=training_words,\n",
    "    fonts_dir=\"fonts\",\n",
    "    output_dir=\"data/word_images\",\n",
    "    font_size_range=(40, 56),\n",
    "    random_blur=True,\n",
    "    random_noise=True,\n",
    "    random_rotate=True,\n",
    "    random_distortion=True,\n",
    "    background_mode=\"random\",\n",
    "    max_image_size=1024\n",
    ")\n",
    "\n",
    "generator.generate_dataset()\n",
    "\n",
    "print(\"\\n✓ DATASET GENERATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify\n",
    "output_dir = \"data/word_images\"\n",
    "image_files = [f for f in os.listdir(output_dir) if f.endswith(\".png\")]\n",
    "print(f\"✓ Generated {len(image_files)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbd88c",
   "metadata": {},
   "source": [
    "## Step 4: Create Charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique characters\n",
    "charset = set()\n",
    "for word in training_words:\n",
    "    charset.update(word)\n",
    "charset = sorted(list(charset))\n",
    "\n",
    "with open(\"charset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(charset))\n",
    "\n",
    "print(f\"✓ Charset: {len(charset)} unique characters\")\n",
    "print(f\"✓ num_classes = {len(charset) + 1} (including CTC blank)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd7fcc",
   "metadata": {},
   "source": [
    "## Step 5: Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CRNNFeatureExtractor(nn.Module):\n",
    "    \"\"\"CNN backbone for CRNN.\"\"\"\n",
    "    def __init__(self, img_channels=1):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 512, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),\n",
    "            nn.Conv2d(512, 512, 2, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_output = self.cnn(x)\n",
    "        b, c, h, w = conv_output.size()\n",
    "        conv_output = conv_output.mean(2)\n",
    "        return conv_output.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    \"\"\"BiLSTM layer.\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        self.embedding = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        recurrent, _ = self.rnn(x)\n",
    "        return self.embedding(recurrent)\n",
    "\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    \"\"\"CRNN + BiLSTM + CTC.\"\"\"\n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256):\n",
    "        super().__init__()\n",
    "        self.cnn = CRNNFeatureExtractor(img_channels)\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)\n",
    "        )\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.cnn(x)\n",
    "        return self.rnn(features)\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        preds_log = preds.log_softmax(2)\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "print(\"✓ Model classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba47a9",
   "metadata": {},
   "source": [
    "## Step 6: Create Config & Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Create config\n",
    "config_content = f\"\"\"# CRNN OCR Configuration\n",
    "num_classes: {len(charset) + 1}\n",
    "num_channels: 1\n",
    "hidden_size: 256\n",
    "\n",
    "img_height: 32\n",
    "img_width: 256\n",
    "\n",
    "batch_size: 64\n",
    "epochs: 50\n",
    "learning_rate: 0.001\n",
    "weight_decay: 1e-5\n",
    "scheduler_step: 15\n",
    "scheduler_gamma: 0.5\n",
    "\n",
    "train_samples: 5000\n",
    "samples_per_word: 1\n",
    "fonts_dir: \"fonts\"\n",
    "output_dir: \"data/word_images\"\n",
    "charset_path: \"charset.txt\"\n",
    "\"\"\"\n",
    "\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"✓ config.yaml created\")\n",
    "print(f\"✓ Charset size: {len(charset)} → {len(charset) + 1} classes\")\n",
    "print(\"\\nTo start training, run: python scripts/train_word_ocr.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962a1f0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Pipeline complete:\n",
    "- ✅ Extracted 5000 unique Nepali words\n",
    "- ✅ Generated synthetic images (HarfBuzz + augmentations)\n",
    "- ✅ Created charset.txt\n",
    "- ✅ Defined OCR model (CRNN + BiLSTM + CTC)\n",
    "- ✅ Ready for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
