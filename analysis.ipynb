{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbfb030d",
   "metadata": {},
   "source": [
    "# OCR-Devanagari-CRNN — Dataset Analysis & Training Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete pipeline for building an Optical Character Recognition (OCR) system specifically trained for Devanagari script using a Convolutional Recurrent Neural Network (CRNN) architecture combined with LSTM layers and CTC (Connectionist Temporal Classification) loss.\n",
    "\n",
    "## Key Components:\n",
    "1. **Data Source**: HuggingFace dataset \"Sakonii/nepalitext-language-model-dataset\" containing real Nepali text\n",
    "2. **Synthetic Data Generation**: Uses HarfBuzz for proper Devanagari shaping and rendering with augmentations\n",
    "3. **Model Architecture**: CRNN (CNN feature extractor + Bidirectional LSTM + CTC decoder)\n",
    "4. **Training Ready**: Configuration and pipeline setup for training the OCR model\n",
    "\n",
    "## Why This Approach?\n",
    "- **HarfBuzz**: Ensures proper rendering of complex Devanagari ligatures and diacritics\n",
    "- **Synthetic Data**: Allows controlled generation of training samples with various fonts and augmentations\n",
    "- **CRNN+LSTM**: Powerful combination for sequence-to-sequence learning in OCR tasks\n",
    "- **CTC Loss**: Optimal for sequence alignment when character positions are unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5482c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation Commands (Run if dependencies not installed)\n",
    "# !pip install --upgrade pip setuptools\n",
    "# !pip install -r requirements.txt\n",
    "# \n",
    "# Expected dependencies:\n",
    "# - datasets (HuggingFace datasets library)\n",
    "# - PIL/Pillow (image processing)\n",
    "# - numpy (numerical operations)\n",
    "# - matplotlib (visualization)\n",
    "# - torch (deep learning framework)\n",
    "# - freetype-py (font rendering)\n",
    "# - uharfbuzz (text shaping for complex scripts)\n",
    "# - opencv-python (computer vision operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9368611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "IMPORTS & DEPENDENCIES\n",
    "=======================\n",
    "This section imports all required libraries for the OCR pipeline.\n",
    "\n",
    "Library purposes:\n",
    "- datasets: Load HuggingFace datasets (Nepali text corpus)\n",
    "- PIL/Image: Image creation, manipulation, and format handling\n",
    "- numpy: Numerical arrays and mathematical operations\n",
    "- matplotlib: Data visualization and plotting\n",
    "- random: Randomization for data shuffling and augmentation\n",
    "- re: Regular expressions for text extraction and cleaning\n",
    "- os/glob: File system operations and path handling\n",
    "- torch: PyTorch deep learning framework\n",
    "- torch.nn: Neural network layers and models\n",
    "- freetype: Low-level font rendering engine\n",
    "- uharfbuzz: Text shaping for complex scripts (Devanagari)\n",
    "- cv2: OpenCV for image transformations (perspective, distortion)\n",
    "- Counter: Efficient counting of character frequencies\n",
    "\"\"\"\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import freetype\n",
    "import uharfbuzz as hb\n",
    "import cv2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20287711",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LOAD NEPALI TEXT DATASET\n",
    "==========================\n",
    "Load the HuggingFace dataset containing real Nepali text samples.\n",
    "\n",
    "Dataset Details:\n",
    "- Source: \"Sakonii/nepalitext-language-model-dataset\"\n",
    "- Contains: Real Nepali text in Devanagari script\n",
    "- Purpose: Extract authentic vocabulary for OCR training\n",
    "- Train split: Contains thousands of text samples\n",
    "\n",
    "The dataset is publicly available and contains clean, real-world Nepali text\n",
    "that we'll use to extract authentic words for synthetic image generation.\n",
    "\"\"\"\n",
    "# Load the NepaliText dataset\n",
    "dataset = load_dataset(\"Sakonii/nepalitext-language-model-dataset\")\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "print(f\"Loaded dataset with {len(train_texts)} training samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f225a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CHARACTER ANALYSIS & FREQUENCY ANALYSIS\n",
    "==========================================\n",
    "Analyze and visualize the frequency distribution of characters in the dataset.\n",
    "\n",
    "This section:\n",
    "1. Defines a cleaning function to handle special characters\n",
    "2. Counts character frequencies across all texts\n",
    "3. Visualizes the top 50 most common characters\n",
    "\n",
    "Technical Details:\n",
    "\n",
    "DEVANAGARI UNICODE RANGE: U+0900 to U+097F (2304-2431 in decimal)\n",
    "This range includes:\n",
    "- Consonants (वर्ण): क, ख, ग, घ, etc.\n",
    "- Vowels (स्वर): अ, आ, इ, ई, उ, ऊ, etc.\n",
    "- Diacritics (मात्रा): ा, ि, ी, ु, ू, ृ, etc.\n",
    "- Nukt consonants: ड़, ढ़, etc.\n",
    "\n",
    "Characters Filtered Out:\n",
    "- Newlines, tabs, carriage returns\n",
    "- Zero-width characters (8203-8207): Used for text flow control\n",
    "- Non-printable ASCII characters\n",
    "- Any character outside Devanagari range (unless printable ASCII for punctuation)\n",
    "\n",
    "Why This Cleaning?\n",
    "- OCR models perform better with consistent character sets\n",
    "- Zero-width characters don't contribute to visual appearance\n",
    "- Reduces confusion between visually similar patterns\n",
    "\"\"\"\n",
    "# Clean character function - removes unwanted characters\n",
    "def clean_char(c):\n",
    "    \"\"\"\n",
    "    Remove non-essential characters that don't contribute to visual OCR training.\n",
    "    \n",
    "    Args:\n",
    "        c (str): Single character to clean\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned character (empty string if filtered out)\n",
    "    \"\"\"\n",
    "    # Remove whitespace characters that don't render\n",
    "    if c in [\"\\n\", \"\\t\", \"\\r\"]:\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove zero-width characters (used for bidirectional text control)\n",
    "    # Unicode range: U+200B to U+200F\n",
    "    if ord(c) in [8203, 8204, 8205, 8206, 8207]:\n",
    "        return \"\"\n",
    "    \n",
    "    # Keep all Devanagari characters (U+0900 to U+097F)\n",
    "    if 2304 <= ord(c) <= 2431:\n",
    "        return c\n",
    "    \n",
    "    # Keep other printable characters (spaces, punctuation, etc.)\n",
    "    if c.isprintable():\n",
    "        return c\n",
    "    \n",
    "    # Filter out everything else\n",
    "    return \"\"\n",
    "\n",
    "# Count character frequencies across entire dataset\n",
    "char_freq = Counter()\n",
    "for text in train_texts:\n",
    "    if isinstance(text, str):\n",
    "        # Clean and update frequency counter\n",
    "        cleaned = \"\".join(clean_char(c) for c in text)\n",
    "        char_freq.update(cleaned)\n",
    "\n",
    "print(f\"✓ Unique cleaned characters: {len(char_freq)}\")\n",
    "\n",
    "# Extract top 50 most frequent characters for visualization\n",
    "top_50 = char_freq.most_common(50)\n",
    "chars, freqs = zip(*top_50)\n",
    "\n",
    "# Visualize character frequency distribution\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.bar(chars, freqs)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.title(\"Top 50 Characters (Cleaned)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Characters\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCharacter frequency analysis complete. Top characters are visualized above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818175a",
   "metadata": {},
   "source": [
    "## Step 1: Extract 5000 Nepali Words\n",
    "\n",
    "### Objective\n",
    "Extract authentic Devanagari words from the dataset to use as training samples for synthetic image generation.\n",
    "\n",
    "### Methodology\n",
    "- Use regex pattern to identify contiguous sequences of Devanagari characters\n",
    "- Filter by word length (2-30 characters) to exclude single characters and unrealistic sequences\n",
    "- Create a vocabulary of unique words for synthetic training data\n",
    "\n",
    "### Why Synthetic Data?\n",
    "- **Controlled Augmentation**: We can vary fonts, sizes, rotations, and distortions\n",
    "- **Unlimited Samples**: Generate as many training examples as needed\n",
    "- **Authentic Text**: Uses real Nepali words, not randomly generated characters\n",
    "- **Reproducibility**: Same vocabulary can be used for fair model comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bab0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXTRACT DEVANAGARI WORDS FROM DATASET\n",
    "=======================================\n",
    "Extract authentic Nepali words from the real text corpus.\n",
    "\n",
    "Process:\n",
    "1. Define regex pattern to match Devanagari character sequences\n",
    "2. Iterate through all dataset texts\n",
    "3. Extract matching word sequences\n",
    "4. Filter by length constraints\n",
    "5. Remove duplicates using a set\n",
    "6. Randomly shuffle and select 5000 words for training\n",
    "\n",
    "Regex Pattern: r'[\\u0900-\\u097F]+'\n",
    "- \\u0900-\\u097F: Unicode range for Devanagari script\n",
    "- +: One or more consecutive Devanagari characters\n",
    "\n",
    "Word Length Constraints:\n",
    "- Minimum: 2 characters (avoid single-character words)\n",
    "- Maximum: 30 characters (avoid unrealistic sequences)\n",
    "\n",
    "Why Deduplication?\n",
    "- Prevents bias towards frequently occurring words\n",
    "- Ensures diverse vocabulary representation\n",
    "- Reduces redundant synthetic image generation\n",
    "\"\"\"\n",
    "\n",
    "def extract_nepali_words(text):\n",
    "    \"\"\"\n",
    "    Extract all Devanagari word sequences from a text string.\n",
    "    \n",
    "    Uses regex to find contiguous sequences of Devanagari characters.\n",
    "    Filters by length to get reasonable word-like sequences.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text that may contain Devanagari and other characters\n",
    "        \n",
    "    Returns:\n",
    "        list: Extracted Devanagari words meeting length criteria\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    # Match one or more consecutive Devanagari characters (U+0900 to U+097F)\n",
    "    matches = re.findall(r\"[\\u0900-\\u097F]+\", text)\n",
    "    \n",
    "    # Filter by length: keep only words between 2 and 30 characters\n",
    "    return [w for w in matches if 2 <= len(w) <= 30]\n",
    "\n",
    "# Extract all unique words from dataset\n",
    "all_words = set()\n",
    "print(\"Extracting Nepali words from dataset...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, text in enumerate(train_texts):\n",
    "    words = extract_nepali_words(text)\n",
    "    all_words.update(words)\n",
    "    \n",
    "    # Print progress at regular intervals\n",
    "    if (i + 1) % 10000 == 0:\n",
    "        print(f\"  Processed {i + 1:6d} texts  |  Found {len(all_words):6d} unique words\")\n",
    "\n",
    "# Prepare training word list\n",
    "all_words = list(all_words)\n",
    "random.shuffle(all_words)  # Randomize order to avoid order bias\n",
    "training_words = all_words[:5000]  # Select first 5000\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"✓ Total unique words extracted: {len(all_words):6d}\")\n",
    "print(f\"✓ Selected for training:         {len(training_words):6d} words\")\n",
    "print(f\"✓ Sample words: {training_words[:10]}\")\n",
    "print(\"\\nThese words will be used to generate synthetic training images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846268b",
   "metadata": {},
   "source": [
    "## Step 2: Synthetic Dataset Generator\n",
    "\n",
    "### Overview\n",
    "Create synthetic training images with proper Devanagari text rendering using HarfBuzz and various augmentation techniques.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "#### Text Rendering with HarfBuzz\n",
    "- **Complex Script Shaping**: Properly renders Devanagari ligatures and diacritics\n",
    "- **Glyph Positioning**: Accurate placement of shaped glyphs on the canvas\n",
    "- **Font Support**: Works with any TTF font that supports Devanagari\n",
    "\n",
    "#### Data Augmentations\n",
    "- **Blur**: Simulates camera focus variations (Gaussian blur)\n",
    "- **Noise**: Adds Gaussian noise and salt-and-pepper noise\n",
    "- **Rotation**: Random slight rotations (-7° to +7°)\n",
    "- **Perspective Distortion**: Simulates viewing angle variations\n",
    "- **Background Variation**: Random backgrounds to prevent overfitting to white/gray\n",
    "\n",
    "#### Why These Augmentations?\n",
    "- **Blur**: Handles out-of-focus camera captures\n",
    "- **Noise**: Increases robustness to sensor noise and compression artifacts\n",
    "- **Rotation**: Camera tilt and document orientation variations\n",
    "- **Perspective**: Non-perpendicular document scanning\n",
    "- **Background**: Prevents model from using background as discriminative feature\n",
    "\n",
    "### Technical Implementation\n",
    "Uses PIL for image manipulation, HarfBuzz for text shaping, OpenCV for perspective transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee9a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SYNTHETIC OCR DATASET GENERATOR CLASS\n",
    "======================================\n",
    "Generate synthetic training images with proper Devanagari rendering and augmentations.\n",
    "\n",
    "Architecture:\n",
    "1. Load TTF fonts from fonts directory\n",
    "2. For each word:\n",
    "   a. Render text using HarfBuzz shaping engine\n",
    "   b. Apply random augmentations (blur, noise, rotation, distortion)\n",
    "   c. Save image and corresponding label\n",
    "   d. Track progress\n",
    "\n",
    "Key Method: render_text_image()\n",
    "- Uses HarfBuzz for proper Devanagari glyph shaping\n",
    "- Handles complex ligatures and diacritics\n",
    "- Applies FreeType for glyph rendering\n",
    "- Composes glyphs on canvas with proper positioning\n",
    "\n",
    "Augmentation Chain:\n",
    "1. Gaussian blur (50% probability)\n",
    "2. Rotation (-7° to +7°, 100% probability)\n",
    "3. Perspective distortion (100% probability)\n",
    "4. Gaussian + salt-and-pepper noise (100% probability)\n",
    "\"\"\"\n",
    "\n",
    "class SyntheticHarfBuzzOCRDatasetGenerator:\n",
    "    \"\"\"\n",
    "    Generate synthetic OCR training dataset with proper Devanagari text rendering.\n",
    "    \n",
    "    This generator uses:\n",
    "    - HarfBuzz for complex script shaping\n",
    "    - FreeType for glyph rendering\n",
    "    - PIL for image composition\n",
    "    - OpenCV for geometric transformations\n",
    "    - NumPy for noise generation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        strings,\n",
    "        fonts_dir=\"fonts\",\n",
    "        output_dir=\"data/word_images\",\n",
    "        font_size_range=(40, 56),\n",
    "        random_blur=True,\n",
    "        random_noise=True,\n",
    "        random_rotate=True,\n",
    "        random_distortion=True,\n",
    "        background_mode=\"random\",\n",
    "        max_image_size=1024\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the synthetic dataset generator.\n",
    "        \n",
    "        Args:\n",
    "            strings (list): List of text strings to render\n",
    "            fonts_dir (str): Directory containing TTF font files\n",
    "            output_dir (str): Directory to save generated images\n",
    "            font_size_range (tuple): Min and max font sizes in points\n",
    "            random_blur (bool): Apply Gaussian blur augmentation\n",
    "            random_noise (bool): Apply Gaussian and salt-pepper noise\n",
    "            random_rotate (bool): Apply random rotation (-7 to 7 degrees)\n",
    "            random_distortion (bool): Apply perspective distortion\n",
    "            background_mode (str): \"white\", \"lightgray\", or \"random\"\n",
    "            max_image_size (int): Maximum image dimension to prevent memory issues\n",
    "        \"\"\"\n",
    "        self.strings = strings\n",
    "        \n",
    "        # Load all available TTF fonts\n",
    "        self.fonts = glob.glob(os.path.join(fonts_dir, \"**/*.ttf\"), recursive=True)\n",
    "        if not self.fonts:\n",
    "            raise ValueError(f\"No fonts found in {fonts_dir}. Please download Devanagari fonts.\")\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "        # Augmentation parameters\n",
    "        self.font_size_range = font_size_range\n",
    "        self.random_blur = random_blur\n",
    "        self.random_noise = random_noise\n",
    "        self.random_rotate = random_rotate\n",
    "        self.random_distortion = random_distortion\n",
    "        self.background_mode = background_mode\n",
    "        self.MAX_SIZE = max_image_size\n",
    "\n",
    "    def _clamp_image_size(self, img):\n",
    "        \"\"\"\n",
    "        Ensure image doesn't exceed maximum size.\n",
    "        \n",
    "        This prevents memory issues with very large generated images.\n",
    "        Uses high-quality LANCZOS resampling for downscaling.\n",
    "        \n",
    "        Args:\n",
    "            img (PIL.Image): Input image\n",
    "            \n",
    "        Returns:\n",
    "            PIL.Image: Image with clamped dimensions\n",
    "        \"\"\"\n",
    "        w, h = img.size\n",
    "        if w > self.MAX_SIZE or h > self.MAX_SIZE:\n",
    "            img.thumbnail((self.MAX_SIZE, self.MAX_SIZE), Image.LANCZOS)\n",
    "        return img\n",
    "\n",
    "    def generate_dataset(self):\n",
    "        \"\"\"\n",
    "        Generate the complete synthetic dataset.\n",
    "        \n",
    "        For each input string:\n",
    "        1. Render it as an image using render_text_image()\n",
    "        2. Save image as PNG (indexed by 5-digit counter)\n",
    "        3. Save label as text file with corresponding text\n",
    "        4. Print progress every 500 images\n",
    "        \n",
    "        Output Structure:\n",
    "            output_dir/\n",
    "            ├── 00001.png\n",
    "            ├── 00001.txt\n",
    "            ├── 00002.png\n",
    "            ├── 00002.txt\n",
    "            └── ...\n",
    "        \"\"\"\n",
    "        for idx, text in enumerate(self.strings, start=1):\n",
    "            img = self.render_text_image(text)\n",
    "            image_path = os.path.join(self.output_dir, f\"{idx:05d}.png\")\n",
    "            label_path = os.path.join(self.output_dir, f\"{idx:05d}.txt\")\n",
    "            \n",
    "            img.save(image_path)\n",
    "            with open(label_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(text)\n",
    "            \n",
    "            if idx % 500 == 0:\n",
    "                print(f\"  [{idx:5d}/{len(self.strings)}] Generated images\")\n",
    "\n",
    "    def render_text_image(self, text, padding=20):\n",
    "        \"\"\"\n",
    "        Render a single text string as an image with augmentations.\n",
    "        \n",
    "        Process:\n",
    "        1. Select random font and size\n",
    "        2. Shape text using HarfBuzz (handles complex Devanagari)\n",
    "        3. Calculate required image dimensions\n",
    "        4. Create background image\n",
    "        5. Render glyphs using FreeType\n",
    "        6. Apply augmentations:\n",
    "           - Blur (50% probability)\n",
    "           - Rotation (100%)\n",
    "           - Perspective distortion (100%)\n",
    "           - Noise (100%)\n",
    "        \n",
    "        Args:\n",
    "            text (str): Devanagari text to render\n",
    "            padding (int): Pixel padding around text\n",
    "            \n",
    "        Returns:\n",
    "            PIL.Image: Rendered and augmented image\n",
    "        \"\"\"\n",
    "        # Random font and size selection\n",
    "        font_path = random.choice(self.fonts)\n",
    "        font_size = random.randint(*self.font_size_range)\n",
    "        \n",
    "        # Initialize FreeType face\n",
    "        face = freetype.Face(font_path)\n",
    "        face.set_char_size(font_size * 64)  # FreeType uses 1/64 point units\n",
    "\n",
    "        # === HARFBUZZ TEXT SHAPING ===\n",
    "        # HarfBuzz properly shapes complex Devanagari text, handling:\n",
    "        # - Ligatures (e.g., क्ष, त्र)\n",
    "        # - Diacritics (e.g., ा, ि)\n",
    "        # - Conjuncts (combined consonants)\n",
    "        \n",
    "        hb_blob = hb.Blob.from_file_path(font_path)\n",
    "        hb_face = hb.Face(hb_blob, 0)\n",
    "        hb_font = hb.Font(hb_face)\n",
    "        hb_font.scale = (face.size.ascender, face.size.ascender)\n",
    "\n",
    "        buf = hb.Buffer()\n",
    "        buf.add_str(text)\n",
    "        buf.guess_segment_properties()  # Auto-detect script and language\n",
    "        hb.shape(hb_font, buf)\n",
    "\n",
    "        infos = buf.glyph_infos  # Glyph indices and properties\n",
    "        positions = buf.glyph_positions  # Positioning info (advance, offset)\n",
    "\n",
    "        # Calculate required image dimensions\n",
    "        width = sum(pos.x_advance for pos in positions) // 64 + 2*padding\n",
    "        height = font_size + 2*padding\n",
    "\n",
    "        # === CREATE BACKGROUND ===\n",
    "        if self.background_mode == \"white\":\n",
    "            img = Image.new(\"RGB\", (width, height), \"white\")\n",
    "        elif self.background_mode == \"lightgray\":\n",
    "            img = Image.new(\"RGB\", (width, height), \"lightgray\")\n",
    "        else:\n",
    "            # Random background with slight variation (200-255 intensity)\n",
    "            arr = np.random.randint(200, 255, (height, width, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(arr)\n",
    "\n",
    "        # Starting position for glyph rendering\n",
    "        x, y = padding, padding + font_size\n",
    "\n",
    "        # === RENDER GLYPHS ===\n",
    "        for info, pos in zip(infos, positions):\n",
    "            glyph_index = info.codepoint\n",
    "            face.load_glyph(glyph_index, freetype.FT_LOAD_RENDER)\n",
    "            bitmap = face.glyph.bitmap\n",
    "            top = face.glyph.bitmap_top\n",
    "            left = face.glyph.bitmap_left\n",
    "\n",
    "            if bitmap.width > 0 and bitmap.rows > 0:\n",
    "                # Convert FreeType bitmap to PIL image\n",
    "                glyph_img = Image.frombytes(\n",
    "                    \"L\", \n",
    "                    (bitmap.width, bitmap.rows), \n",
    "                    bytes(bitmap.buffer)\n",
    "                )\n",
    "                # Create colored glyph (black text on transparent)\n",
    "                colored_glyph = Image.new(\"RGB\", glyph_img.size, \"black\")\n",
    "                # Paste using alpha channel from glyph_img\n",
    "                img.paste(colored_glyph, (int(x + left), int(y - top)), glyph_img)\n",
    "\n",
    "            # Move cursor based on HarfBuzz positioning\n",
    "            x += pos.x_advance / 64  # 1/64 point to pixel conversion\n",
    "            y -= pos.y_advance / 64\n",
    "\n",
    "        img = self._clamp_image_size(img)\n",
    "\n",
    "        # === APPLY AUGMENTATIONS ===\n",
    "        \n",
    "        # 1. Gaussian Blur (50% probability)\n",
    "        if self.random_blur and random.random() < 0.5:\n",
    "            from PIL import ImageFilter\n",
    "            img = img.filter(ImageFilter.GaussianBlur(radius=random.uniform(0.5, 1.5)))\n",
    "\n",
    "        # 2. Random Rotation (-7 to +7 degrees)\n",
    "        if self.random_rotate:\n",
    "            angle = random.randint(-7, 7)\n",
    "            img = img.rotate(angle, expand=True, fillcolor=\"white\")\n",
    "            img = self._clamp_image_size(img)\n",
    "\n",
    "        # 3. Perspective Distortion\n",
    "        if self.random_distortion:\n",
    "            img = self.perspective_distortion(img)\n",
    "\n",
    "        # 4. Noise Addition\n",
    "        if self.random_noise:\n",
    "            img = self.add_noise(img)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def perspective_distortion(self, img):\n",
    "        \"\"\"\n",
    "        Apply random perspective distortion to simulate camera angle variations.\n",
    "        \n",
    "        This transformation is useful for:\n",
    "        - Simulating non-perpendicular document scanning\n",
    "        - Handling camera tilt in real OCR scenarios\n",
    "        - Increasing model robustness to viewing angles\n",
    "        \n",
    "        Process:\n",
    "        1. Define source corners as image boundaries\n",
    "        2. Randomly perturb destination corners (±10% of image size)\n",
    "        3. Compute perspective transformation matrix\n",
    "        4. Apply transformation using OpenCV warpPerspective\n",
    "        \n",
    "        Args:\n",
    "            img (PIL.Image): Input image\n",
    "            \n",
    "        Returns:\n",
    "            PIL.Image: Perspective-distorted image\n",
    "        \"\"\"\n",
    "        img = self._clamp_image_size(img)\n",
    "        w, h = img.size\n",
    "        arr = np.array(img)\n",
    "        \n",
    "        # Maximum distortion amount (±10% of image size)\n",
    "        shift = min(w, h) * 0.1\n",
    "\n",
    "        # Source points (image corners)\n",
    "        pts1 = np.float32([[0, 0], [w, 0], [0, h], [w, h]])\n",
    "        \n",
    "        # Destination points (randomly perturbed corners)\n",
    "        pts2 = np.float32([\n",
    "            [random.uniform(-shift, shift), random.uniform(-shift, shift)],\n",
    "            [w + random.uniform(-shift, shift), random.uniform(-shift, shift)],\n",
    "            [random.uniform(-shift, shift), h + random.uniform(-shift, shift)],\n",
    "            [w + random.uniform(-shift, shift), h + random.uniform(-shift, shift)],\n",
    "        ])\n",
    "        \n",
    "        # Compute perspective transformation matrix\n",
    "        matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "        \n",
    "        # Apply perspective transformation (white border for out-of-bounds regions)\n",
    "        warped = cv2.warpPerspective(\n",
    "            arr, \n",
    "            matrix, \n",
    "            (w, h), \n",
    "            borderMode=cv2.BORDER_CONSTANT, \n",
    "            borderValue=(255, 255, 255)\n",
    "        )\n",
    "        return Image.fromarray(warped)\n",
    "\n",
    "    def add_noise(self, img):\n",
    "        \"\"\"\n",
    "        Add realistic noise to the image.\n",
    "        \n",
    "        This includes:\n",
    "        1. Gaussian noise (zero-mean, σ=10)\n",
    "           - Simulates sensor noise and compression artifacts\n",
    "        \n",
    "        2. Salt-and-pepper noise (2% of pixels)\n",
    "           - 1% turned to white (salt)\n",
    "           - 1% turned to black (pepper)\n",
    "           - Simulates sensor spikes and transmission errors\n",
    "        \n",
    "        Args:\n",
    "            img (PIL.Image): Input image\n",
    "            \n",
    "        Returns:\n",
    "            PIL.Image: Noisy image\n",
    "        \"\"\"\n",
    "        arr = np.array(img).astype(np.float32)\n",
    "        \n",
    "        # Gaussian noise (50% probability)\n",
    "        if random.random() < 0.5:\n",
    "            # Add Gaussian noise with std dev = 10\n",
    "            arr += np.random.normal(0, 10, arr.shape)\n",
    "        \n",
    "        # Salt-and-pepper noise (50% probability)\n",
    "        if random.random() < 0.5:\n",
    "            amount = 0.02  # 2% of pixels affected\n",
    "            num_salt = int(arr.size * amount * 0.5)\n",
    "            num_pepper = int(arr.size * amount * 0.5)\n",
    "            \n",
    "            # Add salt (white) noise\n",
    "            coords = [np.random.randint(0, i - 1, num_salt) for i in arr.shape]\n",
    "            arr[tuple(coords)] = 255\n",
    "            \n",
    "            # Add pepper (black) noise\n",
    "            coords = [np.random.randint(0, i - 1, num_pepper) for i in arr.shape]\n",
    "            arr[tuple(coords)] = 0\n",
    "        \n",
    "        # Clamp values to valid range [0, 255]\n",
    "        arr = np.clip(arr, 0, 255)\n",
    "        return Image.fromarray(arr.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30069a0",
   "metadata": {},
   "source": [
    "## Step 3: Generate Dataset\n",
    "\n",
    "### Execution\n",
    "This step runs the dataset generator to create synthetic training images from the 5000 Nepali words extracted in Step 1.\n",
    "\n",
    "### Process\n",
    "- Initialize generator with augmentation parameters\n",
    "- Generate images in parallel (if available)\n",
    "- Save each image with corresponding label file\n",
    "- Total output: 5000 images + 5000 label files\n",
    "\n",
    "### Expected Duration\n",
    "Depends on system performance, typically 5-30 minutes for 5000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47901147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GENERATE SYNTHETIC TRAINING DATASET\n",
    "====================================\n",
    "Execute the synthetic dataset generator to create training images.\n",
    "\n",
    "Configuration:\n",
    "- Font size range: 40-56 points (provides variation)\n",
    "- All augmentations enabled:\n",
    "  * Blur: Handles out-of-focus variations\n",
    "  * Noise: Adds robustness to sensor noise\n",
    "  * Rotation: Handles document tilt\n",
    "  * Distortion: Handles perspective variations\n",
    "- Background: Random (prevents background-based overfitting)\n",
    "- Image size limit: 1024 pixels (memory management)\n",
    "\n",
    "Output Structure:\n",
    "    data/word_images/\n",
    "    ├── 00001.png  (synthetic image)\n",
    "    ├── 00001.txt  (label: \"काठमाडौ\" or similar)\n",
    "    ├── 00002.png\n",
    "    ├── 00002.txt\n",
    "    ├── ...\n",
    "    └── 05000.png / 05000.txt\n",
    "\n",
    "Total Files Created: 10,000 (5,000 image-label pairs)\n",
    "Average Image Size: ~50-150 KB per PNG\n",
    "\n",
    "This dataset is ready for training the CRNN+LSTM model.\n",
    "\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"GENERATING SYNTHETIC DATASET (5000 WORD SAMPLES)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize generator with full augmentation pipeline\n",
    "generator = SyntheticHarfBuzzOCRDatasetGenerator(\n",
    "    strings=training_words,\n",
    "    fonts_dir=\"fonts\",\n",
    "    output_dir=\"data/word_images\",\n",
    "    font_size_range=(40, 56),\n",
    "    random_blur=True,\n",
    "    random_noise=True,\n",
    "    random_rotate=True,\n",
    "    random_distortion=True,\n",
    "    background_mode=\"random\",\n",
    "    max_image_size=1024\n",
    ")\n",
    "\n",
    "# Generate all images and labels\n",
    "generator.generate_dataset()\n",
    "\n",
    "print(\"\\n✓ DATASET GENERATION COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verification: Count generated files\n",
    "output_dir = \"data/word_images\"\n",
    "image_files = [f for f in os.listdir(output_dir) if f.endswith(\".png\")]\n",
    "label_files = [f for f in os.listdir(output_dir) if f.endswith(\".txt\")]\n",
    "\n",
    "print(f\"✓ Generated {len(image_files)} images\")\n",
    "print(f\"✓ Generated {len(label_files)} labels\")\n",
    "print(f\"\\nDataset location: {output_dir}\")\n",
    "print(f\"Ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbd88c",
   "metadata": {},
   "source": [
    "## Step 4: Create Charset\n",
    "\n",
    "### Purpose\n",
    "Extract all unique characters from the training vocabulary and create a charset file that will be used by the model.\n",
    "\n",
    "### Why Charset?\n",
    "- **Class Indexing**: Each character gets a unique index (0 to num_classes-1)\n",
    "- **Output Layer**: Model's output layer must have size = num_classes + 1 (extra for CTC blank)\n",
    "- **Decoding**: Used to convert predicted indices back to characters\n",
    "\n",
    "### Charset File Format\n",
    "- Single line containing all unique characters in order\n",
    "- Encoding: UTF-8 (important for Devanagari)\n",
    "- Used during training and inference for character-to-index mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c7d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXTRACT AND SAVE CHARACTER SET\n",
    "================================\n",
    "Create a charset from all unique characters in the training vocabulary.\n",
    "\n",
    "Process:\n",
    "1. Iterate through all training words\n",
    "2. Collect unique characters\n",
    "3. Sort alphabetically for consistent ordering\n",
    "4. Save to charset.txt (one line, all characters)\n",
    "\n",
    "Charset Details:\n",
    "- num_classes = len(charset) + 1\n",
    "  * Additional class for CTC blank token (index 0)\n",
    "  * Character indices: 1 to len(charset)\n",
    "  \n",
    "Why +1 for CTC?\n",
    "- CTC (Connectionist Temporal Classification) requires a blank token\n",
    "- This token represents the absence of character (for variable-length alignment)\n",
    "- Reserved as index 0 in the output layer\n",
    "\n",
    "Character Ordering:\n",
    "- Sorted alphabetically for consistency\n",
    "- Enables reproducible index-to-character mapping\n",
    "- Simplifies debugging and cross-model comparison\n",
    "\"\"\"\n",
    "\n",
    "# Extract unique characters from all training words\n",
    "charset = set()\n",
    "for word in training_words:\n",
    "    charset.update(word)  # Add all characters in the word\n",
    "\n",
    "# Sort for consistent, reproducible ordering\n",
    "charset = sorted(list(charset))\n",
    "\n",
    "# Save charset to file\n",
    "with open(\"charset.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\".join(charset))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CHARACTER SET EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"✓ Charset: {len(charset)} unique characters\")\n",
    "print(f\"✓ num_classes = {len(charset) + 1} (including CTC blank token at index 0)\")\n",
    "print(f\"\\nCharset file saved to: charset.txt\")\n",
    "print(f\"\\nCharacter breakdown:\")\n",
    "print(f\"  - Index 0: CTC blank token (reserved for temporal alignment)\")\n",
    "print(f\"  - Index 1-{len(charset)}: Character indices\")\n",
    "print(f\"\\nExample characters: {charset[:10]}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffd7fcc",
   "metadata": {},
   "source": [
    "## Step 5: Model Architecture\n",
    "\n",
    "### Overview\n",
    "Define the CRNN (Convolutional Recurrent Neural Network) model architecture for Devanagari OCR.\n",
    "\n",
    "### Architecture Stack\n",
    "1. **CNN Feature Extractor** (CRNNFeatureExtractor)\n",
    "   - Extracts visual features from image\n",
    "   - Output: Sequence of feature vectors\n",
    "\n",
    "2. **Bidirectional LSTM** (BidirectionalLSTM) × 2\n",
    "   - Captures sequential dependencies\n",
    "   - First layer: hidden_size → hidden_size\n",
    "   - Second layer: hidden_size → num_classes\n",
    "\n",
    "3. **CTC Loss** (Connectionist Temporal Classification)\n",
    "   - Handles variable-length character sequences\n",
    "   - No need for explicit character alignment\n",
    "\n",
    "### Why CRNN+LSTM?\n",
    "- **CNNs**: Excellent for spatial feature extraction\n",
    "- **RNNs**: Model sequential/temporal dependencies\n",
    "- **Bidirectional**: Uses context from both directions\n",
    "- **LSTM**: Captures long-range dependencies better than vanilla RNN\n",
    "- **CTC**: Aligns variable-length sequences without explicit alignment\n",
    "\n",
    "### Model Flow\n",
    "Image → CNN (spatial features) → BiLSTM × 2 (sequence modeling) → Output (character probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afafedd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NEURAL NETWORK MODEL ARCHITECTURE\n",
    "===================================\n",
    "Define CRNN + BiLSTM + CTC model for Devanagari OCR.\n",
    "\n",
    "This implementation follows the CRNN architecture from:\n",
    "\"An End-to-End Trainable Neural Network for Image-based Sequence Recognition\"\n",
    "\n",
    "Architecture Layers:\n",
    "├── CNN Feature Extractor\n",
    "│   ├── Conv2d(1, 64, 3×3)    - Initial feature extraction\n",
    "│   ├── Conv2d(64, 128, 3×3)  - Mid-level features\n",
    "│   ├── Conv2d(128, 256, 3×3) - Higher-level patterns\n",
    "│   ├── Conv2d(256, 256, 3×3) - Deeper patterns\n",
    "│   ├── Conv2d(256, 512, 3×3) - Complex feature combinations\n",
    "│   ├── Conv2d(512, 512, 3×3) - Final CNN layer\n",
    "│   └── Conv2d(512, 512, 2×2) - Dimension reduction\n",
    "│\n",
    "├── BiLSTM Layer 1\n",
    "│   └── 512 → 256 hidden units (bidirectional)\n",
    "│\n",
    "├── BiLSTM Layer 2\n",
    "│   └── 256 → num_classes output\n",
    "│\n",
    "└── CTC Loss Function\n",
    "    └── For alignment-free sequence learning\n",
    "\"\"\"\n",
    "\n",
    "class CRNNFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN backbone for feature extraction.\n",
    "    \n",
    "    This convolutional encoder progressively extracts features:\n",
    "    - Early layers: Edge and small pattern detection\n",
    "    - Middle layers: Local features and textures\n",
    "    - Deep layers: Semantic features and character patterns\n",
    "    \n",
    "    Architecture Details:\n",
    "    - Kernel size: 3×3 (good for Devanagari character features)\n",
    "    - Pooling: 2×2 (downsamples spatial dimensions)\n",
    "    - BatchNorm: Stabilizes training and enables higher learning rates\n",
    "    - Final pooling: (2,1) in last stages (preserve horizontal resolution)\n",
    "    \n",
    "    Why (2,1) pooling?\n",
    "    - Devanagari characters have distinct vertical (height) patterns\n",
    "    - Horizontal resolution is critical for sequence learning\n",
    "    - Reduces height but preserves width for LSTM processing\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, img_channels=1):\n",
    "        \"\"\"\n",
    "        Initialize CNN feature extractor.\n",
    "        \n",
    "        Args:\n",
    "            img_channels (int): Input channels (1 for grayscale, 3 for RGB)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Block 1: 1 → 64 channels\n",
    "            nn.Conv2d(img_channels, 64, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # H/2, W/2\n",
    "            \n",
    "            # Block 2: 64 → 128 channels\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # H/4, W/4\n",
    "            \n",
    "            # Block 3: 128 → 256 channels\n",
    "            nn.Conv2d(128, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),  # Normalize before next layer\n",
    "            \n",
    "            # Block 4: 256 → 256 channels (deeper feature extraction)\n",
    "            nn.Conv2d(256, 256, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # H/2, W same (preserve horizontal)\n",
    "            \n",
    "            # Block 5: 256 → 512 channels\n",
    "            nn.Conv2d(256, 512, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(512),\n",
    "            \n",
    "            # Block 6: 512 → 512 channels (final deep layer)\n",
    "            nn.Conv2d(512, 512, 3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # H/2, W same\n",
    "            \n",
    "            # Block 7: Dimension reduction (2×2 kernel)\n",
    "            nn.Conv2d(512, 512, 2, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through CNN.\n",
    "        \n",
    "        Input shape: (batch, channels, height, width)\n",
    "        Output shape: (width, batch, channels)\n",
    "        \n",
    "        The output is reshaped for LSTM:\n",
    "        - Collapses height dimension (spatial averaging)\n",
    "        - Produces sequence along width dimension\n",
    "        - Width position becomes temporal dimension for LSTM\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input images (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Sequence of feature vectors (W, B, C)\n",
    "        \"\"\"\n",
    "        conv_output = self.cnn(x)  # (B, 512, H', W')\n",
    "        b, c, h, w = conv_output.size()\n",
    "        \n",
    "        # Average across height dimension (treat height as spatial, not temporal)\n",
    "        conv_output = conv_output.mean(2)  # (B, 512, W')\n",
    "        \n",
    "        # Rearrange for LSTM: (W', B, 512)\n",
    "        # Width becomes temporal dimension (character sequence)\n",
    "        return conv_output.permute(2, 0, 1)\n",
    "\n",
    "\n",
    "class BidirectionalLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional LSTM layer with output projection.\n",
    "    \n",
    "    Purpose: Capture sequential dependencies in character sequences.\n",
    "    \n",
    "    Why Bidirectional?\n",
    "    - Forward LSTM: Uses context from left (previous characters)\n",
    "    - Backward LSTM: Uses context from right (future characters)\n",
    "    - Combined: Richer representation using full context\n",
    "    \n",
    "    Process:\n",
    "    1. LSTM processes sequence (forward + backward)\n",
    "    2. Concatenate forward and backward hidden states\n",
    "    3. Project to output dimension using linear layer\n",
    "    \n",
    "    Hidden State Concatenation:\n",
    "    - LSTM output shape: (seq_len, batch, 2*hidden_size)\n",
    "    - Linear layer maps: 2*hidden_size → output_size\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize BiLSTM layer.\n",
    "        \n",
    "        Args:\n",
    "            input_size (int): Dimension of input features\n",
    "            hidden_size (int): Number of LSTM hidden units (each direction)\n",
    "            output_size (int): Output dimension after projection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Bidirectional LSTM: outputs 2*hidden_size (forward + backward concatenated)\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers=1, bidirectional=True)\n",
    "        \n",
    "        # Project BiLSTM output to desired dimension\n",
    "        # Input: 2*hidden_size (bidirectional concatenation)\n",
    "        # Output: output_size\n",
    "        self.embedding = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through BiLSTM.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input sequence (seq_len, batch, input_size)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Projected output (seq_len, batch, output_size)\n",
    "        \"\"\"\n",
    "        # BiLSTM forward\n",
    "        recurrent, _ = self.rnn(x)  # (seq_len, batch, 2*hidden_size)\n",
    "        \n",
    "        # Project to output dimension\n",
    "        return self.embedding(recurrent)  # (seq_len, batch, output_size)\n",
    "\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete OCR model: CNN + BiLSTM + CTC.\n",
    "    \n",
    "    Architecture Flow:\n",
    "    Input Image\n",
    "        ↓\n",
    "    CNN Feature Extractor (CRNNFeatureExtractor)\n",
    "        ↓ Feature Sequence (width, batch, 512)\n",
    "    BiLSTM Layer 1 (512 → 256)\n",
    "        ↓ (width, batch, 256)\n",
    "    BiLSTM Layer 2 (256 → num_classes)\n",
    "        ↓ (width, batch, num_classes)\n",
    "    CTC Loss → Character Predictions\n",
    "    \n",
    "    Why This Architecture?\n",
    "    - CNN: Excellent for image-to-feature conversion\n",
    "    - LSTM: Models character sequence dependencies\n",
    "    - CTC: Handles alignment automatically (no explicit character positions needed)\n",
    "    \n",
    "    Training:\n",
    "    - Forward pass outputs logits (not normalized)\n",
    "    - Converted to log probabilities for CTC loss\n",
    "    - CTC loss aligns predictions with ground truth\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, img_channels=1, hidden_size=256):\n",
    "        \"\"\"\n",
    "        Initialize OCR model.\n",
    "        \n",
    "        Args:\n",
    "            num_classes (int): Number of character classes (including CTC blank)\n",
    "            img_channels (int): Input image channels (1 for grayscale)\n",
    "            hidden_size (int): LSTM hidden layer dimension\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.cnn = CRNNFeatureExtractor(img_channels)\n",
    "        \n",
    "        # Stacked BiLSTM layers for deeper sequence modeling\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(512, hidden_size, hidden_size),  # 512 → 256\n",
    "            BidirectionalLSTM(hidden_size, hidden_size, num_classes)  # 256 → num_classes\n",
    "        )\n",
    "        \n",
    "        # CTC Loss for alignment-free training\n",
    "        # blank=0 corresponds to CTC blank token\n",
    "        # zero_infinity=True: handles inf loss from very incorrect predictions\n",
    "        self.ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass: image to character predictions.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input images (batch, channels, height, width)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Character logits (seq_len, batch, num_classes)\n",
    "        \"\"\"\n",
    "        features = self.cnn(x)  # (width, batch, 512)\n",
    "        return self.rnn(features)  # (width, batch, num_classes)\n",
    "\n",
    "    def compute_ctc_loss(self, preds, targets, pred_lengths, target_lengths):\n",
    "        \"\"\"\n",
    "        Compute CTC loss for training.\n",
    "        \n",
    "        CTC (Connectionist Temporal Classification) Loss:\n",
    "        - Handles variable-length sequences\n",
    "        - Automatically finds best alignment between predictions and targets\n",
    "        - No explicit character position labels needed\n",
    "        \n",
    "        Args:\n",
    "            preds (torch.Tensor): Model predictions (seq_len, batch, num_classes)\n",
    "            targets (torch.Tensor): Ground truth character indices (batch, max_target_len)\n",
    "            pred_lengths (torch.Tensor): Actual prediction lengths (batch,)\n",
    "            target_lengths (torch.Tensor): Actual target lengths (batch,)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: CTC loss value (scalar)\n",
    "        \"\"\"\n",
    "        # Convert logits to log-softmax (CTC expects log probabilities)\n",
    "        preds_log = preds.log_softmax(2)\n",
    "        \n",
    "        # Compute CTC loss with automatic alignment\n",
    "        return self.ctc_loss(preds_log, targets, pred_lengths, target_lengths)\n",
    "\n",
    "print(\"✓ Model architecture classes defined successfully\")\n",
    "print(\"\\nModel Summary:\")\n",
    "print(\"├── CRNNFeatureExtractor: CNN backbone (1-channel → 512-channel features)\")\n",
    "print(\"├── BidirectionalLSTM: Bidirectional LSTM with output projection\")\n",
    "print(\"├── OCRModel: Complete pipeline with CTC loss\")\n",
    "print(\"\\nModel is ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba47a9",
   "metadata": {},
   "source": [
    "## Step 6: Create Configuration & Training Setup\n",
    "\n",
    "### Configuration Parameters\n",
    "Create YAML configuration file with all training hyperparameters.\n",
    "\n",
    "### Key Parameters\n",
    "- **Model**: num_classes, hidden_size, image dimensions\n",
    "- **Training**: batch_size, epochs, learning_rate, optimizer settings\n",
    "- **Data**: paths to fonts, images, charset\n",
    "- **Augmentation**: font size range, distortion settings\n",
    "\n",
    "### Next Steps\n",
    "After this notebook:\n",
    "1. Run `python scripts/train_word_ocr.py` to start training\n",
    "2. Monitor validation metrics\n",
    "3. Save best model checkpoint\n",
    "4. Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2c132",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CREATE TRAINING CONFIGURATION FILE\n",
    "====================================\n",
    "Generate YAML configuration with all hyperparameters and paths.\n",
    "\n",
    "Configuration Components:\n",
    "\n",
    "1. MODEL ARCHITECTURE\n",
    "   - num_classes: Character classes + CTC blank (important: +1)\n",
    "   - hidden_size: LSTM hidden units (256 is standard, can try 128/512)\n",
    "   - num_channels: 1 (grayscale), 3 (RGB)\n",
    "\n",
    "2. IMAGE DIMENSIONS\n",
    "   - img_height: 32 pixels (standard for OCR)\n",
    "   - img_width: 256 pixels (character sequence length)\n",
    "   - Aspect ratio: 8:1 (encourages wide, short images for text)\n",
    "\n",
    "3. TRAINING HYPERPARAMETERS\n",
    "   - batch_size: 64 (balance between memory and gradient variance)\n",
    "   - epochs: 50 (may need adjustment based on convergence)\n",
    "   - learning_rate: 0.001 (Adam default, start conservative)\n",
    "   - weight_decay: 1e-5 (L2 regularization for overfitting prevention)\n",
    "   \n",
    "4. LEARNING RATE SCHEDULER\n",
    "   - scheduler_step: 15 epochs\n",
    "   - scheduler_gamma: 0.5 (reduce LR by 50% every 15 epochs)\n",
    "   - Helps convergence in later epochs and prevents divergence\n",
    "\n",
    "5. DATA PATHS\n",
    "   - Points to generated dataset and charset\n",
    "\n",
    "Why These Hyperparameters?\n",
    "- batch_size=64: Good compromise (not too small=noisy gradients, not too large=memory)\n",
    "- lr=0.001: Standard Adam learning rate\n",
    "- epochs=50: Typical for this dataset size with early stopping\n",
    "- scheduler_step=15: Reduce learning rate after 15 epochs of training\n",
    "- weight_decay=1e-5: Gentle regularization (not too strong)\n",
    "\n",
    "Customization Tips:\n",
    "- Increase batch_size if GPU memory allows (improves gradient stability)\n",
    "- Decrease learning_rate if training is unstable\n",
    "- Increase epochs if validation loss still decreasing at epoch 50\n",
    "\"\"\"\n",
    "\n",
    "config_content = f\"\"\"# CRNN OCR Configuration for Devanagari\n",
    "# ======================================\n",
    "# This configuration file specifies all training hyperparameters\n",
    "\n",
    "# MODEL ARCHITECTURE\n",
    "# ------------------\n",
    "# Number of output classes = unique_characters + 1 (for CTC blank token)\n",
    "# The +1 is critical for CTC loss computation\n",
    "num_classes: {len(charset) + 1}\n",
    "\n",
    "# Number of input channels (1=grayscale, 3=RGB)\n",
    "# Grayscale is standard for OCR as it's more robust to lighting variations\n",
    "num_channels: 1\n",
    "\n",
    "# LSTM hidden layer dimensions\n",
    "# Typical values: 128, 256, 512\n",
    "# Larger = more model capacity but slower training and more parameters\n",
    "hidden_size: 256\n",
    "\n",
    "# IMAGE DIMENSIONS\n",
    "# ----------------\n",
    "# Input image height (in pixels)\n",
    "# 32 is standard for OCR tasks\n",
    "img_height: 32\n",
    "\n",
    "# Input image width (in pixels)\n",
    "# Should accommodate maximum word length expected\n",
    "# 256 pixels ≈ ~40-50 characters (depends on font size)\n",
    "img_width: 256\n",
    "\n",
    "# TRAINING PARAMETERS\n",
    "# -------------------\n",
    "# Batch size for training\n",
    "# Larger batch = more stable gradients, more GPU memory\n",
    "# Typical range: 32-128 depending on GPU\n",
    "batch_size: 64\n",
    "\n",
    "# Number of training epochs\n",
    "# One epoch = one complete pass through dataset\n",
    "# Typical range: 30-100 (with early stopping)\n",
    "epochs: 50\n",
    "\n",
    "# Initial learning rate for Adam optimizer\n",
    "# Typical range: 0.0001 - 0.01\n",
    "# 0.001 is a good starting point\n",
    "learning_rate: 0.001\n",
    "\n",
    "# Weight decay (L2 regularization)\n",
    "# Prevents overfitting by penalizing large weights\n",
    "# Typical range: 1e-4 to 1e-6\n",
    "weight_decay: 1e-5\n",
    "\n",
    "# LEARNING RATE SCHEDULER\n",
    "# -----------------------\n",
    "# Reduce learning rate after this many epochs without improvement\n",
    "# Helps fine-tune model in later training stages\n",
    "scheduler_step: 15\n",
    "\n",
    "# Learning rate decay factor\n",
    "# new_lr = old_lr * scheduler_gamma\n",
    "# 0.5 means 50% reduction (LR is halved)\n",
    "scheduler_gamma: 0.5\n",
    "\n",
    "# DATA PATHS\n",
    "# ----------\n",
    "# Number of training samples\n",
    "train_samples: 5000\n",
    "\n",
    "# Images per word (1 means one synthetic image per word)\n",
    "samples_per_word: 1\n",
    "\n",
    "# Directory containing TTF fonts for rendering\n",
    "fonts_dir: \"fonts\"\n",
    "\n",
    "# Directory containing generated word images\n",
    "output_dir: \"data/word_images\"\n",
    "\n",
    "# Path to charset file (one line of all characters)\n",
    "charset_path: \"charset.txt\"\n",
    "\n",
    "# AUGMENTATION PARAMETERS\n",
    "# -----------------------\n",
    "# Font size range for synthetic image generation (in points)\n",
    "# Smaller = more variation, but might be too small to read\n",
    "# Larger = less variation, might be too large for image size\n",
    "font_size_range: [40, 56]\n",
    "\n",
    "# Whether to apply data augmentations during training\n",
    "apply_augmentation: true\n",
    "\"\"\"\n",
    "\n",
    "# Save configuration file\n",
    "with open(\"config.yaml\", \"w\") as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION FILE CREATED\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n✓ config.yaml created successfully\")\n",
    "print(f\"\\nKey Configuration Summary:\")\n",
    "print(f\"  Model:\")\n",
    "print(f\"    - Num Classes: {len(charset) + 1} (including CTC blank)\")\n",
    "print(f\"    - Hidden Size: 256\")\n",
    "print(f\"    - Input Channels: 1 (grayscale)\")\n",
    "print(f\"\\n  Image Dimensions:\")\n",
    "print(f\"    - Height: 32 pixels\")\n",
    "print(f\"    - Width: 256 pixels\")\n",
    "print(f\"\\n  Training:\")\n",
    "print(f\"    - Batch Size: 64\")\n",
    "print(f\"    - Epochs: 50\")\n",
    "print(f\"    - Learning Rate: 0.001\")\n",
    "print(f\"    - Training Samples: 5,000\")\n",
    "print(f\"\\n  Dataset:\")\n",
    "print(f\"    - Image Dir: data/word_images/\")\n",
    "print(f\"    - Charset Size: {len(charset)} characters\")\n",
    "print(f\"    - Charset File: charset.txt\")\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Ensure you have the following ready:\")\n",
    "print(\"   - fonts/ directory with Devanagari TTF fonts\")\n",
    "print(\"   - data/word_images/ (generated in Step 3)\")\n",
    "print(\"   - charset.txt (generated in Step 4)\")\n",
    "print(\"\\n2. Create scripts/train_word_ocr.py with training loop using:\")\n",
    "print(\"   - OCRModel defined above\")\n",
    "print(\"   - config.yaml parameters\")\n",
    "print(\"   - DataLoader for batch training\")\n",
    "print(\"   - Adam optimizer\")\n",
    "print(\"   - CTC loss computation\")\n",
    "print(\"\\n3. Run training:\")\n",
    "print(\"   $ python scripts/train_word_ocr.py\")\n",
    "print(\"\\n4. Monitor:\")\n",
    "print(\"   - Training and validation CTC loss\")\n",
    "print(\"   - Character error rate (CER)\")\n",
    "print(\"   - Model checkpoints\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962a1f0",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### Pipeline Completion\n",
    "✅ **Step 1**: Extracted 5,000 unique Nepali words from authentic dataset\n",
    "✅ **Step 2**: Defined synthetic dataset generator with HarfBuzz shaping\n",
    "✅ **Step 3**: Generated 5,000 synthetic training images with augmentations\n",
    "✅ **Step 4**: Created character set from vocabulary (charset.txt)\n",
    "✅ **Step 5**: Designed CRNN+BiLSTM+CTC model architecture\n",
    "✅ **Step 6**: Generated training configuration (config.yaml)\n",
    "\n",
    "### Files Created\n",
    "- `data/word_images/` - 5,000 pairs of images (.png) and labels (.txt)\n",
    "- `charset.txt` - All unique characters, one line\n",
    "- `config.yaml` - Training configuration and hyperparameters\n",
    "\n",
    "### Ready for Training\n",
    "The pipeline is now ready for the training script (`scripts/train_word_ocr.py`) which should:\n",
    "1. Load images from `data/word_images/`\n",
    "2. Create DataLoader with batch processing\n",
    "3. Initialize OCRModel with num_classes\n",
    "4. Train using CTC loss\n",
    "5. Save model checkpoints\n",
    "6. Evaluate on validation set\n",
    "\n",
    "### Key Metrics to Track\n",
    "- **CTC Loss**: Should decrease over epochs\n",
    "- **Character Error Rate (CER)**: Percentage of incorrect character predictions\n",
    "- **Word Accuracy**: Percentage of fully correct predictions\n",
    "- **Validation Loss**: Should not increase (indicates overfitting)\n",
    "\n",
    "### Future Enhancements\n",
    "- Increase dataset size (10,000+ images)\n",
    "- Try different architectures (ResNet backbone, Transformer)\n",
    "- Add more fonts and augmentations\n",
    "- Fine-tune on real OCR data\n",
    "- Implement inference pipeline\n",
    "- Convert to ONNX for deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
